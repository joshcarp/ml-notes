# Papers

A collection of papers with summaries and quick access links.

## Paper Notes

```{list-table}
:header-rows: 1

* - Title
  - Tags
  - Full Notes 
* - [The Super Weight in Large Language Models](https://doi.org/10.48550/arXiv.2411.07191)
  - Model internals
  - 
* - [Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation](https://doi.org/10.48550/arXiv.2410.02725)
  - 
  - 
* - [ReFT: Representation Finetuning for Language Models](https://doi.org/10.48550/arXiv.2404.03592)
  - Fine-tuning, Model representations
  - 
* - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://dl.acm.org/doi/abs/10.5555/2627435.2670313)
  - Model performance, Optimization, Model architecture
  - 
* - [Observation-based unit test generation at Meta](https://doi.org/10.48550/arXiv.2402.06111)
  - Automated testing, Software engineering
  - 
* - [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://doi.org/10.48550/arXiv.2403.20327)
  - Embeddings, Model distillation
  - 
* - [Language Models of Code are Few-Shot Commonsense Learners](https://doi.org/10.48550/arXiv.2210.07128)
  - Code models, Transfer learning
  - 
* - [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](https://doi.org/10.48550/arXiv.2407.10969)
  - Efficiency, Model performance
  - 
* - [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://doi.org/10.48550/arXiv.2207.01780)
  - Code generation, Reinforcement learning
  - 
* - [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](https://doi.org/10.48550/arXiv.2405.20541)
  - Data pruning, Perplexity
  - [Details](perplexity-based-data-pruning)
* - [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://doi.org/10.48550/arXiv.2402.17764)
  - Hardware optimization, Model compression
  - 
* - [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://doi.org/10.48550/arXiv.2305.17493)
  - Model collapse, Training data
  - 
* - [Chain of thought prompting](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
  - Prompting strategies
  - 
* - [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://doi.org/10.48550/arXiv.2404.07143)
  - Attention mechanisms, Context window
  - 
* - [TextGrad](https://doi.org/10.48550/arXiv.2406.07496)
  - Agent systems, Text optimization
  - 
* - [Scalable MatMul-free Language Modeling](https://doi.org/10.48550/arXiv.2406.02528)
  - Attention mechanisms, Model efficiency
  - 
* - [Will humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?](https://doi.org/10.48550/arXiv.1712.00676)
  - AI in software development, Future of coding
  - 
* - [Scalable Extraction of Training Data from (Production) Language Models](https://doi.org/10.48550/arXiv.2311.17035)
  - Data accumulation, Model performance, Curated datasets
  - 
* - [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://doi.org/10.48550/arXiv.2305.07759)
  - Dataset creation, Small language models
  - 
* - [GAIA: A Benchmark for General AI Assistants](https://doi.org/10.48550/arXiv.2311.12983)
  - AI assistants, Benchmarking
  - 
* - [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://www.anthropic.com/news/mapping-mind-language-model)
  - Feature extraction, Interpretability
  - 
* - [Mixture-of-Agents Enhances Large Language Model Capabilities](https://doi.org/10.48550/arXiv.2406.04692)
  - Model performance, Multi-agent systems
  - 
* - [Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems](https://doi.org/10.48550/arXiv.2311.05884)
  - Transformers, Model architecture, Model performance
  - 
* - [Large Language Models Understand and Can Be Enhanced by Emotional Stimuli](https://doi.org/10.48550/arXiv.2307.11760)
  - emotional stimuli, Model behavior
  - 
* - [Automated Unit Test Improvement using Large Language Models at Meta](https://doi.org/10.48550/arXiv.2402.09171)
  - Automated testing, Software engineering
  - 
* - [Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data](https://doi.org/10.48550/arXiv.2404.01413)
  - Data accumulation, Model collapse prevention
  - 
* - [A\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks](https://doi.org/10.48550/arXiv.2102.04518)
  - Reinforcement learning, Search algorithms
  - 
* - [Large Language Models: A Survey](https://doi.org/10.48550/arXiv.2402.06196)
  - LLM capabilities, Survey
  - 
* - [A Language Model's Guide Through Latent Space](https://doi.org/10.48550/arXiv.2402.14433)
  - Interpretability, Latent space
  - 
* - [Extracting Latent Steering Vectors from Pretrained Language Models](https://doi.org/10.48550/arXiv.2205.05124)
  - Interpretability, Latent space
  - 
* - [Many-shot jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking)
  - Jailbreaking, Model safety
  - 
* - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://doi.org/10.48550/arXiv.1910.10683)
  - Transfer learning
  - 
* - [Activation Addition: Steering Language Models Without Optimization](https://doi.org/10.48550/arXiv.2308.10248)
  - Activation manipulation, Model steering
  - 
* - [Evaluating Large Language Models Trained on Code](https://doi.org/10.48550/arXiv.2107.03374)
  - Code generation, Model evaluation
  - 
* - [To Believe or Not to Believe Your LLM](https://doi.org/10.48550/arXiv.2406.02543)
  - Hallucination detection, Uncertainty quantification
  - 
* - [Mixture of Agents](https://doi.org/10.48550/arXiv.2406.04692)
  - Multi-agent systems, Prompting
  - 
* - [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](https://doi.org/10.48550/arXiv.2404.14619)
  - Efficiency, Model architecture
  - 
* - [Deep Reinforcement Learning from Human Preferences](https://doi.org/10.48550/arXiv.1706.03741)
  - human feedback, Reinforcement learning
  - 
* - [Federated Large Language Model: A Position Paper](https://doi.org/10.48550/arXiv.2307.08925)
  - Distributed training, Federated learning
  - 
* - [MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning](https://doi.org/10.48550/arXiv.2212.02508)
  - Music representation, Self-supervised learning
  - 
* - [LLaMA: Open and Efficient Foundation Language Models](https://doi.org/10.48550/arXiv.2302.13971)
  - Model architecture, Open-source LLMs
  - 
* - [Phi1: Textbooks Are All You Need](https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/)
  - Curated datasets, Model efficiency
  - 
* - [Layer Normalization](https://doi.org/10.48550/arXiv.1607.06450)
  - Model internals, Optimization
  - 
* - [Attention Is All You Need](https://doi.org/10.48550/arXiv.1706.03762)
  - OG papers, Transformers
  - 
* - [Explore the Limits of Omni-modal Pretraining at Scale](https://doi.org/10.48550/arXiv.2406.09412)
  - Multi-modal models, Pretraining
  - 
* - [Gaussian Error Linear Units (GELUs)](https://doi.org/10.48550/arXiv.1606.08415)
  - Activation functions, Model internals
  - 
* - [A Fast, Performant, Secure Distributed Training Framework For Large Language Model](https://doi.org/10.48550/arXiv.2401.09796)
  - Distributed training, Security
  - 
* - [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://doi.org/10.48550/arXiv.1910.01108)
  - Efficiency, Model distillation
  - 
* - [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  - OG papers, Pre-training
  - 
* - [Training language models to follow instructions with human feedback](https://doi.org/10.48550/arXiv.2203.02155)
  - Instruction following, Reinforcement learning
  - 
* - [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://doi.org/10.48550/arXiv.2307.09288)
  - Fine-tuning, Open-source LLMs
  - 
* - [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://doi.org/10.48550/arXiv.2104.09864)
  - Embeddings, Model architecture
  - 
* - [Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00748-9)
  - Biological Brains
  - 
* - [Understanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library](https://onlinelibrary.wiley.com/doi/10.1111/tops.12737)
  - Biological Brains
  - 
