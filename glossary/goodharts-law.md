
# Goodhart's Law

## Definition


## Tags


## Additional Notes
When a measure becomes a target, it ceases to be a good measure. This often comes up in context of judging a model based on reported metrics because the act of coming up with targets ends up making the targets a bad predictor of real world performance. This can be due to many reasons but a common one is that the testing data ends up being in the training set.

