{"kind":"Article","sha256":"64697a1afe006585cafe4eeb1430891a4103872c2b52c86d8ebc7d00c3232ad9","slug":"glossary.lora","location":"/glossary/lora.md","dependencies":[],"frontmatter":{"title":"LoRA (Low-Rank Adaptation)","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"lora.md","url":"/lora-d5161c61fd54032cb693d695b113f660.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oaKraEZPER"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"HcYsy8gbuE"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"U0t7QJW54T"}],"key":"dD8aWUjvep"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"U0H1191dYS"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"uZVrmil9je"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"BbS0qPkjOl"}],"key":"aTP3ZALCb2"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"j6iQyMAyS9"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"OKtgSXXggL"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"clKql5OMO0"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.2106.09685","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu ","key":"SFc314Mar7"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"qFrRMqJ1oo"}],"key":"FYzkdFvEUd"},{"type":"text","value":" (2021)","key":"rJKi9oIkuf"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2106.09685","identifier":"https://doi.org/10.48550/arXiv.2106.09685","enumerator":"1","key":"qaHwZHLBEV"}],"key":"bcqUiwdB6J"}],"key":"SwokWOPD78"}],"key":"NP3RlHc9NS"}],"key":"UxUzGhAbBi"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2106.09685"],"data":{"https://doi.org/10.48550/arxiv.2106.09685":{"label":"https://doi.org/10.48550/arxiv.2106.09685","enumerator":"1","doi":"10.48550/ARXIV.2106.09685","html":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). <i>LoRA: Low-Rank Adaptation of Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2106.09685\">10.48550/ARXIV.2106.09685</a>","url":"https://doi.org/10.48550/ARXIV.2106.09685"}}}}}