{"kind":"Article","sha256":"64697a1afe006585cafe4eeb1430891a4103872c2b52c86d8ebc7d00c3232ad9","slug":"glossary.lora","location":"/glossary/lora.md","dependencies":[],"frontmatter":{"title":"LoRA (Low-Rank Adaptation)","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"lora.md","url":"/lora-d5161c61fd54032cb693d695b113f660.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"orNCatvkdY"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"PAJ07YWbra"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"SHU1KR66KP"}],"key":"CtqVfP3Iwh"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"RaRt4aTTVL"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"geKCNUe4Mn"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Edi7Ky8fTj"}],"key":"vGFCWQ4LcU"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"fYC9B3SlmO"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"RBxNvru51u"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"wFDpwjRr9b"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.2106.09685","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu ","key":"IL7FAMuTMq"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"hr514ZqHHe"}],"key":"RjhiECBOAT"},{"type":"text","value":" (2021)","key":"wvEPg5T5Nu"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2106.09685","identifier":"https://doi.org/10.48550/arXiv.2106.09685","enumerator":"1","key":"X7mOfxrlVv"}],"key":"n8MlluDtGg"}],"key":"WzSUHxSpEg"}],"key":"bkkBxe1hGl"}],"key":"ncCLPfKkSJ"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2106.09685"],"data":{"https://doi.org/10.48550/arxiv.2106.09685":{"label":"https://doi.org/10.48550/arxiv.2106.09685","enumerator":"1","doi":"10.48550/ARXIV.2106.09685","html":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). <i>LoRA: Low-Rank Adaptation of Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2106.09685\">10.48550/ARXIV.2106.09685</a>","url":"https://doi.org/10.48550/ARXIV.2106.09685"}}}}}