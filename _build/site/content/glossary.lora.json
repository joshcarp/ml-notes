{"kind":"Article","sha256":"64697a1afe006585cafe4eeb1430891a4103872c2b52c86d8ebc7d00c3232ad9","slug":"glossary.lora","location":"/glossary/lora.md","dependencies":[],"frontmatter":{"title":"LoRA (Low-Rank Adaptation)","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"lora.md","url":"/lora-6af80bef6b9d9b08819a4bdeddc73e46.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QRmzeUs7er"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"CCv64MOLeq"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"yZSSQbVKdF"}],"key":"KoatDY21fA"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"VjlFUEp59Q"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"dKGHd2XHlQ"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ug9EksUlGt"}],"key":"ofGuXUESzE"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"AnbO0oZLpE"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"gFekc8rQCX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"x1JjVrdcqc"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.2106.09685","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu ","key":"idcSYMcO1m"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"HXgKObMJsn"}],"key":"Npx61yStzO"},{"type":"text","value":" (2021)","key":"BYtMHf6PO9"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2106.09685","identifier":"https://doi.org/10.48550/arXiv.2106.09685","enumerator":"1","key":"fJUGItQOHu"}],"key":"l7pnVv0yNr"}],"key":"IHdiZ2yC5E"}],"key":"H8OZ9N7uM5"}],"key":"tPDiTIjTah"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2106.09685"],"data":{"https://doi.org/10.48550/arxiv.2106.09685":{"label":"https://doi.org/10.48550/arxiv.2106.09685","enumerator":"1","doi":"10.48550/ARXIV.2106.09685","html":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). <i>LoRA: Low-Rank Adaptation of Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2106.09685\">10.48550/ARXIV.2106.09685</a>","url":"https://doi.org/10.48550/ARXIV.2106.09685"}}}}}