{"kind":"Article","sha256":"64697a1afe006585cafe4eeb1430891a4103872c2b52c86d8ebc7d00c3232ad9","slug":"glossary.lora","location":"/glossary/lora.md","dependencies":[],"frontmatter":{"title":"LoRA (Low-Rank Adaptation)","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"lora.md","url":"/lora-d5161c61fd54032cb693d695b113f660.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Me7BOJROT1"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"PNaQwKis04"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"IN96TaveS6"}],"key":"RsMykCLW7W"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"XfoBLEG1cD"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"QKfwxJ9C0J"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"C4M605zv2W"}],"key":"bFnKAiMngY"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"np98CNJ2Cr"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"nfOpFn78p3"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"moppmxPhwh"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.2106.09685","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu ","key":"e9akMLzVMm"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"Tywsy1Nq7l"}],"key":"msAEZLkTGI"},{"type":"text","value":" (2021)","key":"HL416u64cu"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2106.09685","identifier":"https://doi.org/10.48550/arXiv.2106.09685","enumerator":"1","key":"JlaZNW6CqL"}],"key":"wuqJDMc12I"}],"key":"h0TpRJj3hD"}],"key":"rIht9iFdjA"}],"key":"oho15OU5WX"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2106.09685"],"data":{"https://doi.org/10.48550/arxiv.2106.09685":{"label":"https://doi.org/10.48550/arxiv.2106.09685","enumerator":"1","doi":"10.48550/ARXIV.2106.09685","html":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). <i>LoRA: Low-Rank Adaptation of Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2106.09685\">10.48550/ARXIV.2106.09685</a>","url":"https://doi.org/10.48550/ARXIV.2106.09685"}}}}}