{"kind":"Article","sha256":"2c3d2ea9ba00854de1a22bb58838925a86f71e37a798d354698aa4bc96cdc548","slug":"glossary.reinforcement-learning-1","location":"/glossary/reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"Reinforcement Learning","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"reinforcement-learning.md","url":"/reinforcement-learni-3963aecb2b88e8a639d6707ccb1e7c8b.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"PP73HagGx1"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"MtuIk5lSHU"},{"type":"heading","depth":2,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"vVjKokXHog"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"seql1tilSl"},{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Reinforcement learning, Training","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"FjkAT8tx0f"}],"key":"XXceYcYLyM"},{"type":"heading","depth":2,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Additional Notes","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"vWKIgcn1EO"}],"identifier":"additional-notes","label":"Additional Notes","html_id":"additional-notes","implicit":true,"key":"waNcxNc8jJ"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"gANcRoWP9t"}],"key":"WCfmyUukyh"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Think of it like teaching a dog new tricks:","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"qQAJEN46jk"}],"key":"vBEHuq0ZIy"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":15,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"The dog (agent) performs actions","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"NuGlpBZhNd"}],"key":"T6lOWfgK9h"},{"type":"listItem","spread":true,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"You give treats or praise (rewards) for good behavior","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"sjpYkZZxFR"}],"key":"XQGjhuqdiT"},{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"The dog learns which actions lead to treats","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"cK5GaBhBd7"}],"key":"ElnvXOo8w6"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Over time, it figures out the best sequence of actions to get rewards\nKey Components:","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"L1Qmk4q53X"}],"key":"GBIn9mtmN1"}],"key":"HWfklFjIr1"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"The Learning Process:","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Rcs8ud5oVf"}],"key":"ulmRXqrho0"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Common Approaches:","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"zyM7BKc3ec"}],"key":"KvsV3Xm0kL"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Q-Learning: Learns the value of actions in different states","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"PXA2vJdKlq"}],"key":"GGljJ4XbWG"}],"key":"U94mSBu6Dd"},{"type":"listItem","spread":true,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"Policy Gradient: Directly learns the best policy","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"H7PRUfq2QI"}],"key":"G5feQjolls"}],"key":"ZzOF483gNY"},{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Deep RL: Combines deep neural networks with RL","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"Iu8YrMyrzl"}],"key":"f3DThGpN8P"}],"key":"v5yqtSbr2D"},{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Model-Based RL: Learns a model of the environment\nReal-World Applications:","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"KGyMBvtTS8"}],"key":"ndmAemcA8H"}],"key":"LG9mirb81R"},{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Game playing (AlphaGo, OpenAI Five)","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"LSYGKFq908"}],"key":"xR4GzxWB4o"}],"key":"BYVoZ5TPAl"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Robotics and robot control","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"WGEUmhvdo8"}],"key":"HZ6A661Qnq"}],"key":"nvLVItnbti"},{"type":"listItem","spread":true,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Resource management","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"C4mQoSjOfn"}],"key":"NDfEBPwg8Y"}],"key":"XD3IJ5Lhg7"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Recommendation systems","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"B9DqQI9ZA0"}],"key":"Z1lGtUFmrO"}],"key":"ifT8i7r9GA"},{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Autonomous vehicles","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"GuJNzvmONE"}],"key":"nze5B9retM"}],"key":"h13bMu4u1x"},{"type":"listItem","spread":true,"position":{"start":{"line":36,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":36,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"Trading strategies\nKey Challenges:","position":{"start":{"line":36,"column":1},"end":{"line":36,"column":1}},"key":"lUvbhKMwFz"}],"key":"ax1uU24882"}],"key":"R6FDehthOJ"},{"type":"listItem","spread":true,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Exploration vs. Exploitation trade-off","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"UUJzyvJSw3"}],"key":"mYENLDLD5X"}],"key":"u5FfAeFUH9"},{"type":"listItem","spread":true,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Delayed rewards (credit assignment problem)","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"skHXpyVELh"}],"key":"EWctBXC3Oy"}],"key":"gwUf4OEpvx"},{"type":"listItem","spread":true,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"Large state/action spaces","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"ZjY5nZ15IT"}],"key":"qr6CLpMs9T"}],"key":"V1UJQaL9N1"},{"type":"listItem","spread":true,"position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"Sample efficiency","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"va3QlPsRRQ"}],"key":"tGfOzZ6bf4"}],"key":"sP4vdJsklt"},{"type":"listItem","spread":true,"position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":43,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"text","value":"Stability during training\nThe power of RL lies in its ability to learn through trial and error, discovering solutions that might not be obvious to human programmers. Unlike supervised learning, which requires labeled examples, RL can learn from raw experience in the environment.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"R0KOC3tRSJ"}],"key":"shCIISnAZe"}],"key":"tcmL0Y7upm"}],"key":"cOeh2fPX3H"}],"key":"gJa73dZRR0"}],"key":"Kp1RNkkw31"},"references":{"cite":{"order":[],"data":{}}}}