{"kind":"Article","sha256":"d8cc420123c577e83548eda6bfc4c9a52220ae882fc0e28131961d6c50e95563","slug":"glossary.residual-stream","location":"/glossary/residual_stream.md","dependencies":[],"frontmatter":{"title":"Residual Stream","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"residual_stream.md","url":"/residual_stream-d46d66af4cb95ed047139cd2d755533b.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Xvkd9eDH7V"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"NTWqa1Olpz"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The residual stream is a core concept in transformer architectures, referring to the main pathway through which information flows across layers. It maintains a persistent representation that each sublayer (self-attention and feed-forward network) modifies via residual connections. The residual stream allows for better gradient flow during training and helps maintain information from earlier layers throughout the network depth. Each sublayer processes the stream and adds its output back to it, rather than completely transforming it.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"XkkqqLpiQf"}],"key":"IEcfRFNSrG"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"n8LyrFCb00"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"DiRgI5XIeJ"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Transformers, Architecture, Deep Learning, Residual Connections","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"DcAIrd45Kr"}],"key":"TnBxmjNVYK"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"QiFVlQOhbH"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"PrVhEP3Zev"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"YKJo1l2G9A"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.1706.03762","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Vaswani ","key":"rWb59WVPfA"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"R1pP8OyH1J"}],"key":"OFfI1AXiID"},{"type":"text","value":" (2017)","key":"o6kW248GpL"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1706.03762","identifier":"https://doi.org/10.48550/arXiv.1706.03762","enumerator":"1","key":"qywvhBxhQG"}],"key":"AidUMuWWXn"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"ynejlGMM3M"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.1512.03385","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"He ","key":"zgGKR7o8bU"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"oy2WV2enLf"}],"key":"lsEHgq69Ue"},{"type":"text","value":" (2015)","key":"IVq2V1EeUp"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1512.03385","identifier":"https://doi.org/10.48550/arXiv.1512.03385","enumerator":"2","key":"LZEb28jXhZ"}],"key":"H67Bdfa5kb"}],"key":"j45IKT2tnq"}],"key":"R6GuzHWgzN"}],"key":"qq4uK58CrE"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.1706.03762","https://doi.org/10.48550/arxiv.1512.03385"],"data":{"https://doi.org/10.48550/arxiv.1706.03762":{"label":"https://doi.org/10.48550/arxiv.1706.03762","enumerator":"1","doi":"10.48550/ARXIV.1706.03762","html":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). <i>Attention Is All You Need</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1706.03762\">10.48550/ARXIV.1706.03762</a>","url":"https://doi.org/10.48550/ARXIV.1706.03762"},"https://doi.org/10.48550/arxiv.1512.03385":{"label":"https://doi.org/10.48550/arxiv.1512.03385","enumerator":"2","doi":"10.48550/ARXIV.1512.03385","html":"He, K., Zhang, X., Ren, S., & Sun, J. (2015). <i>Deep Residual Learning for Image Recognition</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1512.03385\">10.48550/ARXIV.1512.03385</a>","url":"https://doi.org/10.48550/ARXIV.1512.03385"}}}}}