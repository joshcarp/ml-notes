{"kind":"Article","sha256":"de6f13421b9e894a3495f69d13e261da2b558605fa45aba906e17ebef11a1aea","slug":"paper-notes.index","location":"/paper-notes/index.md","dependencies":[],"frontmatter":{"title":"Papers","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"index.md","url":"/index-ab00802d9242182da19d48d67abc124d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"A collection of papers with summaries and quick access links.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nQ6mIyIq99"}],"key":"ujP6627xld"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Paper Notes","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kOdpL96QPL"}],"identifier":"paper-notes","label":"Paper Notes","html_id":"paper-notes","implicit":true,"key":"nKyFSfyo20"},{"type":"container","kind":"table","children":[{"type":"table","children":[{"type":"tableRow","children":[{"type":"tableCell","header":true,"children":[{"type":"text","value":"Title","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"htijGiXQtM"}],"key":"WaKgsiwEiE"},{"type":"tableCell","header":true,"children":[{"type":"text","value":"Tags","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"I8wPTigrWZ"}],"key":"HsbKrsIXKQ"},{"type":"tableCell","header":true,"children":[{"type":"text","value":"Full Notes","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hPgFt3lLpF"}],"key":"ByhCvODlnY"}],"key":"IUiWpmfChw"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2411.07191","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"The Super Weight in Large Language Models","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"biV2xTqhAM"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2411.07191","identifier":"https://doi.org/10.48550/arXiv.2411.07191","enumerator":"1","key":"pYe5MuX1a1"}],"key":"IomVRSOMVk"},{"type":"tableCell","children":[{"type":"text","value":"Model internals","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"NPJ82QywOi"}],"key":"RrRS8Ei6Vb"},{"type":"tableCell","children":[],"key":"oLmO9QIOfh"}],"key":"V5ASico21g"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2410.02725","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Vc2USYkKhk"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2410.02725","identifier":"https://doi.org/10.48550/arXiv.2410.02725","enumerator":"2","key":"QaBoPFL5Mf"}],"key":"qBuS5cLzOp"},{"type":"tableCell","children":[],"key":"Lt8ruche0u"},{"type":"tableCell","children":[],"key":"JahW9RUKd7"}],"key":"K5qUrK1kjO"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.03592","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"ReFT: Representation Finetuning for Language Models","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"Ibm9rp0bJs"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.03592","identifier":"https://doi.org/10.48550/arXiv.2404.03592","enumerator":"3","key":"F1PWidY3qx"}],"key":"n3adLV4LML"},{"type":"tableCell","children":[{"type":"text","value":"Fine-tuning, Model representations","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"lG0oAsdWkd"}],"key":"h6upjliHSl"},{"type":"tableCell","children":[],"key":"U8T8XLDKIy"}],"key":"DlEkASIPPK"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://doi.org/10.5555/2627435.2670313","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Dropout: A Simple Way to Prevent Neural Networks from Overfitting","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"LsXJ5rPzU5"}],"urlSource":"https://dl.acm.org/doi/abs/10.5555/2627435.2670313","data":{"doi":"10.5555/2627435.2670313"},"internal":false,"protocol":"doi","key":"PDlVA7jOjA"}],"key":"V7iYdFuVD5"},{"type":"tableCell","children":[{"type":"text","value":"Model performance, Optimization, Model architecture","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"t73SCRrDsn"}],"key":"oSspiWR1tn"},{"type":"tableCell","children":[],"key":"X420IeFV7I"}],"key":"gty1aeH5yC"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.06111","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Observation-based unit test generation at Meta","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"jGnqvYMYA9"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.06111","identifier":"https://doi.org/10.48550/arXiv.2402.06111","enumerator":"4","key":"EB1cRSyRGN"}],"key":"IqjJPrF8gh"},{"type":"tableCell","children":[{"type":"text","value":"Automated testing, Software engineering","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"mtYRiBfduB"}],"key":"n6NciHA1xS"},{"type":"tableCell","children":[],"key":"RSk2wZra5R"}],"key":"qrI7eLirYB"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2403.20327","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"DkpqFZ9HF5"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2403.20327","identifier":"https://doi.org/10.48550/arXiv.2403.20327","enumerator":"5","key":"MGobZreQCg"}],"key":"fStQOO0D4R"},{"type":"tableCell","children":[{"type":"text","value":"Embeddings, Model distillation","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"ARotXPSZiN"}],"key":"HXGiDSaVxX"},{"type":"tableCell","children":[],"key":"yzM4oJ4wSB"}],"key":"ehDYqRxgcY"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2210.07128","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Language Models of Code are Few-Shot Commonsense Learners","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"QYCldG77u2"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2210.07128","identifier":"https://doi.org/10.48550/arXiv.2210.07128","enumerator":"6","key":"Jy7nINuCX4"}],"key":"U2w0ZmgWuz"},{"type":"tableCell","children":[{"type":"text","value":"Code models, Transfer learning","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"n5L4jj7Lh3"}],"key":"Q9zBmiqtml"},{"type":"tableCell","children":[],"key":"BXkZWJzfK2"}],"key":"Mjyg3OsMLw"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2407.10969","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"OxL1hgTA4p"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2407.10969","identifier":"https://doi.org/10.48550/arXiv.2407.10969","enumerator":"7","key":"tH3lcFmbOF"}],"key":"iZnV6JTV5m"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model performance","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"En08b3RYpM"}],"key":"zlmXdbQ217"},{"type":"tableCell","children":[],"key":"SHp7SMFbto"}],"key":"xKDD5WGx8v"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2207.01780","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"mAttvDHX3U"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2207.01780","identifier":"https://doi.org/10.48550/arXiv.2207.01780","enumerator":"8","key":"UEyXeAnO3i"}],"key":"kraeTSrkCh"},{"type":"tableCell","children":[{"type":"text","value":"Code generation, Reinforcement learning","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"aKqBiegIm7"}],"key":"DQEMYuxg2a"},{"type":"tableCell","children":[],"key":"ALLErc5hYf"}],"key":"zmOavwzOxz"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2405.20541","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"Ovah8S6WTE"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2405.20541","identifier":"https://doi.org/10.48550/arXiv.2405.20541","enumerator":"9","key":"XYqK1RQqmE"}],"key":"YOvGIx6fsL"},{"type":"tableCell","children":[{"type":"text","value":"Data pruning, Perplexity","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"oAF1GmgTeQ"}],"key":"jopQWaYMns"},{"type":"tableCell","children":[{"type":"link","url":"/paper-notes/perplexity-based-data-pruning","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"Details","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"O9oRprK37D"}],"urlSource":"perplexity-based-data-pruning","dataUrl":"/paper-notes.perplexity-based-data-pruning.json","internal":true,"protocol":"file","key":"EloaEBRJqv"}],"key":"C2IT5iIhZp"}],"key":"Dm0xB4bASf"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.17764","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"hscCzMvqIv"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.17764","identifier":"https://doi.org/10.48550/arXiv.2402.17764","enumerator":"10","key":"PeE7URAqvV"}],"key":"uzhZP0PCsN"},{"type":"tableCell","children":[{"type":"text","value":"Hardware optimization, Model compression","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"V3cJP4LEfO"}],"key":"SKPlfiqV11"},{"type":"tableCell","children":[],"key":"a6XWb6xbOz"}],"key":"oAkZJa7uiD"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2305.17493","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"The Curse of Recursion: Training on Generated Data Makes Models Forget","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"KMDlySRsg2"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2305.17493","identifier":"https://doi.org/10.48550/arXiv.2305.17493","enumerator":"11","key":"Y8t8ecRXWx"}],"key":"AooWbvepTj"},{"type":"tableCell","children":[{"type":"text","value":"Model collapse, Training data","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"EwJFq16B8U"}],"key":"B7wUpvprhn"},{"type":"tableCell","children":[],"key":"xeTJJOeyxy"}],"key":"dvLV9hBFtN"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"Chain of thought prompting","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"RUKrW57LHW"}],"urlSource":"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html","key":"xXBxmdQl3w"}],"key":"DHFRB7G54U"},{"type":"tableCell","children":[{"type":"text","value":"Prompting strategies","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"AJhtNgAl3s"}],"key":"ZxUanABaUS"},{"type":"tableCell","children":[],"key":"QxfYKVnkyS"}],"key":"WfLwiClQd7"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.07143","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"NlJV6hLtlT"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.07143","identifier":"https://doi.org/10.48550/arXiv.2404.07143","enumerator":"12","key":"AH9RwxAGId"}],"key":"qLL1s3loyq"},{"type":"tableCell","children":[{"type":"text","value":"Attention mechanisms, Context window","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"Ipa6ui92p6"}],"key":"hu9VOtftel"},{"type":"tableCell","children":[],"key":"xkdJeoRVXP"}],"key":"QSHlOgxvOY"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.07496","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"TextGrad","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"OIKYvUSHVu"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.07496","identifier":"https://doi.org/10.48550/arXiv.2406.07496","enumerator":"13","key":"V9clD5k4pq"}],"key":"qS7Eju8BDg"},{"type":"tableCell","children":[{"type":"text","value":"Agent systems, Text optimization","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"uLfx4375Nu"}],"key":"owbwzPa0wn"},{"type":"tableCell","children":[],"key":"EtQWbC5SjP"}],"key":"R6rlmtODp2"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.02528","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"Scalable MatMul-free Language Modeling","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"JPYpt8xDN9"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.02528","identifier":"https://doi.org/10.48550/arXiv.2406.02528","enumerator":"14","key":"MMhuSupKQ4"}],"key":"CcfP1WwSja"},{"type":"tableCell","children":[{"type":"text","value":"Attention mechanisms, Model efficiency","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"ywXtgwdXeh"}],"key":"HPPw1eGyUs"},{"type":"tableCell","children":[],"key":"tcj99CukzP"}],"key":"yRKdX4M5ni"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1712.00676","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"Will humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"JJ251V1lzs"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1712.00676","identifier":"https://doi.org/10.48550/arXiv.1712.00676","enumerator":"15","key":"NzBChQHhKx"}],"key":"AEzm7U2Jab"},{"type":"tableCell","children":[{"type":"text","value":"AI in software development, Future of coding","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"pZlYezrfUa"}],"key":"cP56fYb4ee"},{"type":"tableCell","children":[],"key":"lL2AONftSo"}],"key":"QbxZsR6R00"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.17035","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Scalable Extraction of Training Data from (Production) Language Models","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"hMuhX0K2Bd"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.17035","identifier":"https://doi.org/10.48550/arXiv.2311.17035","enumerator":"16","key":"kw3KF50AEQ"}],"key":"oZppOAFIKy"},{"type":"tableCell","children":[{"type":"text","value":"Data accumulation, Model performance, Curated datasets","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"zhLmE52oZb"}],"key":"kxO8iAi4ik"},{"type":"tableCell","children":[],"key":"j3V4zrvu6g"}],"key":"rWiEdSau0w"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2305.07759","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"nJBW0d3iEt"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2305.07759","identifier":"https://doi.org/10.48550/arXiv.2305.07759","enumerator":"17","key":"jQJKrx1wIj"}],"key":"TLGwvanzkE"},{"type":"tableCell","children":[{"type":"text","value":"Dataset creation, Small language models","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"reI31XJzdR"}],"key":"GEHAThmXVk"},{"type":"tableCell","children":[],"key":"At07fH58UH"}],"key":"d6aEY3uu9m"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.12983","position":{"start":{"line":70,"column":1},"end":{"line":70,"column":1}},"children":[{"type":"text","value":"GAIA: A Benchmark for General AI Assistants","position":{"start":{"line":70,"column":1},"end":{"line":70,"column":1}},"key":"JbblGIF3Mt"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.12983","identifier":"https://doi.org/10.48550/arXiv.2311.12983","enumerator":"18","key":"QHAWoZgLCt"}],"key":"N0TSTuC31I"},{"type":"tableCell","children":[{"type":"text","value":"AI assistants, Benchmarking","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"yXO6lJqEBg"}],"key":"azQUy3j6uj"},{"type":"tableCell","children":[],"key":"YVTINPTNK2"}],"key":"NvuiEK5rgy"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.anthropic.com/news/mapping-mind-language-model","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"key":"YasyHgJps0"}],"urlSource":"https://www.anthropic.com/news/mapping-mind-language-model","key":"KxHCyd6X2N"}],"key":"hr93TyD05D"},{"type":"tableCell","children":[{"type":"text","value":"Feature extraction, Interpretability","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"FJS7SwGaur"}],"key":"CjouOuCkPX"},{"type":"tableCell","children":[],"key":"nbe0Kjfc30"}],"key":"JHgdahvDC9"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.04692","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"Mixture-of-Agents Enhances Large Language Model Capabilities","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"rn4gp9O7Tm"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.04692","identifier":"https://doi.org/10.48550/arXiv.2406.04692","enumerator":"19","key":"xOctmPchNq"}],"key":"KBcTNUlFB0"},{"type":"tableCell","children":[{"type":"text","value":"Model performance, Multi-agent systems","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"key":"u8qpet55aE"}],"key":"S5vExL2cUN"},{"type":"tableCell","children":[],"key":"Rsi0yc4tbM"}],"key":"bOyw7nATu4"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.05884","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"children":[{"type":"text","value":"Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"key":"CdOQ46flYe"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.05884","identifier":"https://doi.org/10.48550/arXiv.2311.05884","enumerator":"20","key":"P4F5Vau0xW"}],"key":"wLz7QpMxd9"},{"type":"tableCell","children":[{"type":"text","value":"Transformers, Model architecture, Model performance","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"gyB0eBbVPw"}],"key":"ewjrT58HmA"},{"type":"tableCell","children":[],"key":"e1yR0vS8mQ"}],"key":"nolotBQIdM"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.11760","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"Large Language Models Understand and Can Be Enhanced by Emotional Stimuli","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"O3Uw50cyrP"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.11760","identifier":"https://doi.org/10.48550/arXiv.2307.11760","enumerator":"21","key":"DqbFlqZKUG"}],"key":"lWaANaNC0a"},{"type":"tableCell","children":[{"type":"text","value":"emotional stimuli, Model behavior","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"R85mrgvkKt"}],"key":"V4BP4vzykt"},{"type":"tableCell","children":[],"key":"KSXmdwPwUN"}],"key":"h0r8cDdBiT"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.09171","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"children":[{"type":"text","value":"Automated Unit Test Improvement using Large Language Models at Meta","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"key":"UknpNqLNXe"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.09171","identifier":"https://doi.org/10.48550/arXiv.2402.09171","enumerator":"22","key":"Nilu0GbDjP"}],"key":"erZp3R2mKH"},{"type":"tableCell","children":[{"type":"text","value":"Automated testing, Software engineering","position":{"start":{"line":86,"column":1},"end":{"line":86,"column":1}},"key":"NZwAFQmHLy"}],"key":"XukPDglzGj"},{"type":"tableCell","children":[],"key":"sR3QxneOTC"}],"key":"AECG0JYhgj"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.01413","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"children":[{"type":"text","value":"Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"key":"Bm3FmePyAx"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.01413","identifier":"https://doi.org/10.48550/arXiv.2404.01413","enumerator":"23","key":"ViLmGMB38p"}],"key":"xTmUV0DnW6"},{"type":"tableCell","children":[{"type":"text","value":"Data accumulation, Model collapse prevention","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"Dx80soaNWE"}],"key":"BpEHa2tGWI"},{"type":"tableCell","children":[],"key":"zGxfJ19dSv"}],"key":"hBJbefeLRl"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2102.04518","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"text","value":"A\\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"key":"LXFLbfGp0T"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2102.04518","identifier":"https://doi.org/10.48550/arXiv.2102.04518","enumerator":"24","key":"W3F40MOgSl"}],"key":"mzGuU9iVBd"},{"type":"tableCell","children":[{"type":"text","value":"Reinforcement learning, Search algorithms","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"key":"yW7w1LmNXZ"}],"key":"dkfrFq2FXh"},{"type":"tableCell","children":[],"key":"nPsNoZfOgW"}],"key":"NmYCPKk4LN"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.06196","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"children":[{"type":"text","value":"Large Language Models: A Survey","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"key":"Z7Dsg2aw6X"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.06196","identifier":"https://doi.org/10.48550/arXiv.2402.06196","enumerator":"25","key":"VjB1Zt9jXm"}],"key":"ptAPtVDCrk"},{"type":"tableCell","children":[{"type":"text","value":"LLM capabilities, Survey","position":{"start":{"line":95,"column":1},"end":{"line":95,"column":1}},"key":"sNnZMQ6uWQ"}],"key":"cMRNpDiN0o"},{"type":"tableCell","children":[],"key":"SR63OvrYk8"}],"key":"N6gMyZBMWf"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.14433","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"A Language Model’s Guide Through Latent Space","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"hYRYPx9yfn"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.14433","identifier":"https://doi.org/10.48550/arXiv.2402.14433","enumerator":"26","key":"oNE6eTQbq0"}],"key":"ICdTtzgO6a"},{"type":"tableCell","children":[{"type":"text","value":"Interpretability, Latent space","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"key":"b0tMWPfhKZ"}],"key":"ZcLLyBjrEg"},{"type":"tableCell","children":[],"key":"PFZp9YKapz"}],"key":"XlN8a4RCj4"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2205.05124","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"text","value":"Extracting Latent Steering Vectors from Pretrained Language Models","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"key":"TADRRYiU9u"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2205.05124","identifier":"https://doi.org/10.48550/arXiv.2205.05124","enumerator":"27","key":"BuDiGXZf2y"}],"key":"WPKhC6pGgQ"},{"type":"tableCell","children":[{"type":"text","value":"Interpretability, Latent space","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"QktVXymnQK"}],"key":"yAYIUv2zCx"},{"type":"tableCell","children":[],"key":"h6vQxRhiCr"}],"key":"KBSqBrhK4N"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.anthropic.com/research/many-shot-jailbreaking","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"Many-shot jailbreaking","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"ChmmOFimBM"}],"urlSource":"https://www.anthropic.com/research/many-shot-jailbreaking","key":"FQMyG8Xvzy"}],"key":"l2ZzhNqIdq"},{"type":"tableCell","children":[{"type":"text","value":"Jailbreaking, Model safety","position":{"start":{"line":104,"column":1},"end":{"line":104,"column":1}},"key":"F8FxF9JtT5"}],"key":"lkMpDCGNIn"},{"type":"tableCell","children":[],"key":"WTB6eketC5"}],"key":"gSkkZTcAu0"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1910.10683","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"children":[{"type":"text","value":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"key":"n9USsjKB8L"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1910.10683","identifier":"https://doi.org/10.48550/arXiv.1910.10683","enumerator":"28","key":"sEAcvfhQkq"}],"key":"nKSRW2daP7"},{"type":"tableCell","children":[{"type":"text","value":"Transfer learning","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"ptaKkR6fCg"}],"key":"dV8yiIqHCh"},{"type":"tableCell","children":[],"key":"Mes7yaI3Ry"}],"key":"rg5DavSD5t"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2308.10248","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"Activation Addition: Steering Language Models Without Optimization","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"VHR1WmlGOu"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2308.10248","identifier":"https://doi.org/10.48550/arXiv.2308.10248","enumerator":"29","key":"gDpcolsdaf"}],"key":"ZFsVFtwgW2"},{"type":"tableCell","children":[{"type":"text","value":"Activation manipulation, Model steering","position":{"start":{"line":110,"column":1},"end":{"line":110,"column":1}},"key":"sCYqSSp84D"}],"key":"x11xFijefZ"},{"type":"tableCell","children":[],"key":"yP8arvea9O"}],"key":"KA2m2kr9zs"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2107.03374","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"children":[{"type":"text","value":"Evaluating Large Language Models Trained on Code","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"key":"GMmYRhLvA8"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2107.03374","identifier":"https://doi.org/10.48550/arXiv.2107.03374","enumerator":"30","key":"LGluGJy0Tc"}],"key":"eAmvElYeRH"},{"type":"tableCell","children":[{"type":"text","value":"Code generation, Model evaluation","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"key":"o0A4bd2fV4"}],"key":"DDSFDSIXs2"},{"type":"tableCell","children":[],"key":"VNNhI4hW0J"}],"key":"SfHGK8ABPs"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.02543","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"To Believe or Not to Believe Your LLM","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"dN9IWROacr"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.02543","identifier":"https://doi.org/10.48550/arXiv.2406.02543","enumerator":"31","key":"kCImR7XmJn"}],"key":"wqJ7oj7vZU"},{"type":"tableCell","children":[{"type":"text","value":"Hallucination detection, Uncertainty quantification","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"fIXB7szvxp"}],"key":"dv0GEq7Zhm"},{"type":"tableCell","children":[],"key":"k0ND3O9j91"}],"key":"DeXP5vcEu2"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.04692","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"children":[{"type":"text","value":"Mixture of Agents","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"key":"xOmuM0uGAl"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.04692","identifier":"https://doi.org/10.48550/arXiv.2406.04692","enumerator":"19","key":"A2gg0CH3GJ"}],"key":"ICW5VZTZzL"},{"type":"tableCell","children":[{"type":"text","value":"Multi-agent systems, Prompting","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"kK9FD2CKk3"}],"key":"HgO7hSXvrb"},{"type":"tableCell","children":[],"key":"jQAQ4ytW7B"}],"key":"xcEqYaYSVH"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.14619","position":{"start":{"line":121,"column":1},"end":{"line":121,"column":1}},"children":[{"type":"text","value":"OpenELM: An Efficient Language Model Family with Open Training and Inference Framework","position":{"start":{"line":121,"column":1},"end":{"line":121,"column":1}},"key":"C6uHYnjPaO"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.14619","identifier":"https://doi.org/10.48550/arXiv.2404.14619","enumerator":"32","key":"TQ6Nmf4Uqc"}],"key":"YEsLi2eYAm"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model architecture","position":{"start":{"line":122,"column":1},"end":{"line":122,"column":1}},"key":"jALJeX24sm"}],"key":"LzGxKG83it"},{"type":"tableCell","children":[],"key":"fcbsrscbM6"}],"key":"MDVwoMZpGq"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1706.03741","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"text","value":"Deep Reinforcement Learning from Human Preferences","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"rxfbTIylhh"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1706.03741","identifier":"https://doi.org/10.48550/arXiv.1706.03741","enumerator":"33","key":"epexyyEy1y"}],"key":"hLnT1XlKJX"},{"type":"tableCell","children":[{"type":"text","value":"human feedback, Reinforcement learning","position":{"start":{"line":125,"column":1},"end":{"line":125,"column":1}},"key":"mXh0v4IcIC"}],"key":"nLKmrKAME4"},{"type":"tableCell","children":[],"key":"BtWgSVK4mH"}],"key":"sdfCyM1sUq"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.08925","position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"children":[{"type":"text","value":"Federated Large Language Model: A Position Paper","position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"key":"BJpNAkk9cA"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.08925","identifier":"https://doi.org/10.48550/arXiv.2307.08925","enumerator":"34","key":"RzC4TZGztz"}],"key":"b7AGBlmd5v"},{"type":"tableCell","children":[{"type":"text","value":"Distributed training, Federated learning","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"key":"z4qJp6nwQr"}],"key":"UmlVFHMNKy"},{"type":"tableCell","children":[],"key":"SFdIQYAlzb"}],"key":"FOZu7Qt357"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2212.02508","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"children":[{"type":"text","value":"MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"key":"JJ8cnJmZBH"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2212.02508","identifier":"https://doi.org/10.48550/arXiv.2212.02508","enumerator":"35","key":"PnrEIDfXyn"}],"key":"iPO8Tg4EYQ"},{"type":"tableCell","children":[{"type":"text","value":"Music representation, Self-supervised learning","position":{"start":{"line":131,"column":1},"end":{"line":131,"column":1}},"key":"maZIfMbJuM"}],"key":"eDMpR3Qj9L"},{"type":"tableCell","children":[],"key":"Bp10CDjzcE"}],"key":"ZcW6vq4bMO"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2302.13971","position":{"start":{"line":133,"column":1},"end":{"line":133,"column":1}},"children":[{"type":"text","value":"LLaMA: Open and Efficient Foundation Language Models","position":{"start":{"line":133,"column":1},"end":{"line":133,"column":1}},"key":"tkOXUch1Cj"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2302.13971","identifier":"https://doi.org/10.48550/arXiv.2302.13971","enumerator":"36","key":"V4W8hRb99S"}],"key":"OYEK6ThcRi"},{"type":"tableCell","children":[{"type":"text","value":"Model architecture, Open-source LLMs","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"q7GVZS2nsN"}],"key":"wPVXctB9E4"},{"type":"tableCell","children":[],"key":"cR3Chbzdtn"}],"key":"Xa59BsjRGF"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"text","value":"Phi1: Textbooks Are All You Need","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"key":"mrI01YesnU"}],"urlSource":"https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/","key":"Gqzrlw2T5M"}],"key":"zcXnEIdw6T"},{"type":"tableCell","children":[{"type":"text","value":"Curated datasets, Model efficiency","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"e7ZagkGicC"}],"key":"e5DJZ4idgt"},{"type":"tableCell","children":[],"key":"USNovr5nqr"}],"key":"horI0Eeii5"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1607.06450","position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"children":[{"type":"text","value":"Layer Normalization","position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"key":"Ry85tX7ecs"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1607.06450","identifier":"https://doi.org/10.48550/arXiv.1607.06450","enumerator":"37","key":"YWpsBBPox1"}],"key":"FlH1z2Y2cP"},{"type":"tableCell","children":[{"type":"text","value":"Model internals, Optimization","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"yI7pJtdX4h"}],"key":"UVGUaVuuQg"},{"type":"tableCell","children":[],"key":"zYwSFUvFq9"}],"key":"oK6a8l8MnE"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1706.03762","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"children":[{"type":"text","value":"Attention Is All You Need","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"key":"u35KKiUZjb"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1706.03762","identifier":"https://doi.org/10.48550/arXiv.1706.03762","enumerator":"38","key":"aIauyGhsap"}],"key":"mFe6g6vmRi"},{"type":"tableCell","children":[{"type":"text","value":"OG papers, Transformers","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"G2p8HuowNK"}],"key":"sP54L7whtp"},{"type":"tableCell","children":[],"key":"te0mICKRvP"}],"key":"vYqVMwDuRo"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.09412","position":{"start":{"line":145,"column":1},"end":{"line":145,"column":1}},"children":[{"type":"text","value":"Explore the Limits of Omni-modal Pretraining at Scale","position":{"start":{"line":145,"column":1},"end":{"line":145,"column":1}},"key":"it08ay8IWJ"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.09412","identifier":"https://doi.org/10.48550/arXiv.2406.09412","enumerator":"39","key":"I90VKxgDh1"}],"key":"yNBxwBC16L"},{"type":"tableCell","children":[{"type":"text","value":"Multi-modal models, Pretraining","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"nkOVA0T3d2"}],"key":"L7WW6L6t86"},{"type":"tableCell","children":[],"key":"yEKNfvbXei"}],"key":"pHkkkW497C"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1606.08415","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"children":[{"type":"text","value":"Gaussian Error Linear Units (GELUs)","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"key":"zUGYQvQoPX"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1606.08415","identifier":"https://doi.org/10.48550/arXiv.1606.08415","enumerator":"40","key":"oq83KNKSno"}],"key":"VX5T5MPLVf"},{"type":"tableCell","children":[{"type":"text","value":"Activation functions, Model internals","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"YO6GZwfbmb"}],"key":"z8td11xXhZ"},{"type":"tableCell","children":[],"key":"rJF1kVovhm"}],"key":"Aamskgfyex"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2401.09796","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"children":[{"type":"text","value":"A Fast, Performant, Secure Distributed Training Framework For Large Language Model","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"key":"ghI2RF8Njf"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2401.09796","identifier":"https://doi.org/10.48550/arXiv.2401.09796","enumerator":"41","key":"fDdTe0QYns"}],"key":"X2jJE7n56f"},{"type":"tableCell","children":[{"type":"text","value":"Distributed training, Security","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"ML4SboUQlX"}],"key":"BBcy1ZwBZX"},{"type":"tableCell","children":[],"key":"HVswNUszLg"}],"key":"dReOUqk38e"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1910.01108","position":{"start":{"line":154,"column":1},"end":{"line":154,"column":1}},"children":[{"type":"text","value":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","position":{"start":{"line":154,"column":1},"end":{"line":154,"column":1}},"key":"C17Lnhh1iv"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1910.01108","identifier":"https://doi.org/10.48550/arXiv.1910.01108","enumerator":"42","key":"UMH4Igbc0r"}],"key":"yRSZLGHBCK"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model distillation","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"ft3brWzjAT"}],"key":"WmUi9CN9d1"},{"type":"tableCell","children":[],"key":"RhnE23PXf0"}],"key":"b5uDOOz9zG"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"children":[{"type":"text","value":"Improving Language Understanding by Generative Pre-Training","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"gagVurkKCi"}],"urlSource":"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf","key":"qF4I2U9kpV"}],"key":"DCBs8uZIzF"},{"type":"tableCell","children":[{"type":"text","value":"OG papers, Pre-training","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"nVfv47nhax"}],"key":"T65Icux0FT"},{"type":"tableCell","children":[],"key":"nqWmYEOxWz"}],"key":"krgB2v1MBc"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2203.02155","position":{"start":{"line":160,"column":1},"end":{"line":160,"column":1}},"children":[{"type":"text","value":"Training language models to follow instructions with human feedback","position":{"start":{"line":160,"column":1},"end":{"line":160,"column":1}},"key":"fMfkx73gts"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2203.02155","identifier":"https://doi.org/10.48550/arXiv.2203.02155","enumerator":"43","key":"kKQYHVeJmw"}],"key":"buCobZ8JZH"},{"type":"tableCell","children":[{"type":"text","value":"Instruction following, Reinforcement learning","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"BNDXSAAs6O"}],"key":"JuGN9Ov42I"},{"type":"tableCell","children":[],"key":"q7iHifLBpM"}],"key":"e2m0VLWSHO"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.09288","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"children":[{"type":"text","value":"Llama 2: Open Foundation and Fine-Tuned Chat Models","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"key":"qTDxKegak7"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.09288","identifier":"https://doi.org/10.48550/arXiv.2307.09288","enumerator":"44","key":"bPVjeue7Bh"}],"key":"PtHaaWEdjN"},{"type":"tableCell","children":[{"type":"text","value":"Fine-tuning, Open-source LLMs","position":{"start":{"line":164,"column":1},"end":{"line":164,"column":1}},"key":"jm2Qj4sejI"}],"key":"XNIZnY1lUm"},{"type":"tableCell","children":[],"key":"nYeMq6kUJT"}],"key":"SYphcvMREa"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2104.09864","position":{"start":{"line":166,"column":1},"end":{"line":166,"column":1}},"children":[{"type":"text","value":"RoFormer: Enhanced Transformer with Rotary Position Embedding","position":{"start":{"line":166,"column":1},"end":{"line":166,"column":1}},"key":"NYETbO1XuF"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2104.09864","identifier":"https://doi.org/10.48550/arXiv.2104.09864","enumerator":"45","key":"rWDanhJzkk"}],"key":"p7foUoUnR8"},{"type":"tableCell","children":[{"type":"text","value":"Embeddings, Model architecture","position":{"start":{"line":167,"column":1},"end":{"line":167,"column":1}},"key":"QPoJPT3nhZ"}],"key":"GYwUVYoc1K"},{"type":"tableCell","children":[],"key":"t0HqdgnBSj"}],"key":"uF9fmbnTC6"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.nature.com/articles/s42256-023-00748-9","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"children":[{"type":"text","value":"Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"PJHjmSCyDt"}],"urlSource":"https://www.nature.com/articles/s42256-023-00748-9","key":"jObCfLip8X"}],"key":"Bt8mjbU0vT"},{"type":"tableCell","children":[{"type":"text","value":"Biological Brains","position":{"start":{"line":170,"column":1},"end":{"line":170,"column":1}},"key":"UYqy5kBdMJ"}],"key":"gOwlSHTTzr"},{"type":"tableCell","children":[],"key":"vdE7IufkSV"}],"key":"ogFm8i9xft"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12737","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"children":[{"type":"text","value":"Understanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"key":"puql7CgbAw"}],"kind":"narrative","label":"Hsiao_2024","identifier":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12737","enumerator":"46","key":"WFRKdQ4xYZ"}],"key":"t9w9KNvjRZ"},{"type":"tableCell","children":[{"type":"text","value":"Biological Brains","position":{"start":{"line":173,"column":1},"end":{"line":173,"column":1}},"key":"UOJQ7FSbkH"}],"key":"by7YhnmcY0"},{"type":"tableCell","children":[],"key":"iNokrQNq5Q"}],"key":"xHbptn8U7X"}],"key":"nnok0zLOKC"}],"enumerator":"1","key":"cO6GDKjB49"}],"key":"knRPCLOibD"}],"key":"Zrpd4jR3OH"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2411.07191","https://doi.org/10.48550/arxiv.2410.02725","https://doi.org/10.48550/arxiv.2404.03592","https://doi.org/10.48550/arxiv.2402.06111","https://doi.org/10.48550/arxiv.2403.20327","https://doi.org/10.48550/arxiv.2210.07128","https://doi.org/10.48550/arxiv.2407.10969","https://doi.org/10.48550/arxiv.2207.01780","https://doi.org/10.48550/arxiv.2405.20541","https://doi.org/10.48550/arxiv.2402.17764","https://doi.org/10.48550/arxiv.2305.17493","https://doi.org/10.48550/arxiv.2404.07143","https://doi.org/10.48550/arxiv.2406.07496","https://doi.org/10.48550/arxiv.2406.02528","https://doi.org/10.48550/arxiv.1712.00676","https://doi.org/10.48550/arxiv.2311.17035","https://doi.org/10.48550/arxiv.2305.07759","https://doi.org/10.48550/arxiv.2311.12983","https://doi.org/10.48550/arxiv.2406.04692","https://doi.org/10.48550/arxiv.2311.05884","https://doi.org/10.48550/arxiv.2307.11760","https://doi.org/10.48550/arxiv.2402.09171","https://doi.org/10.48550/arxiv.2404.01413","https://doi.org/10.48550/arxiv.2102.04518","https://doi.org/10.48550/arxiv.2402.06196","https://doi.org/10.48550/arxiv.2402.14433","https://doi.org/10.48550/arxiv.2205.05124","https://doi.org/10.48550/arxiv.1910.10683","https://doi.org/10.48550/arxiv.2308.10248","https://doi.org/10.48550/arxiv.2107.03374","https://doi.org/10.48550/arxiv.2406.02543","https://doi.org/10.48550/arxiv.2404.14619","https://doi.org/10.48550/arxiv.1706.03741","https://doi.org/10.48550/arxiv.2307.08925","https://doi.org/10.48550/arxiv.2212.02508","https://doi.org/10.48550/arxiv.2302.13971","https://doi.org/10.48550/arxiv.1607.06450","https://doi.org/10.48550/arxiv.1706.03762","https://doi.org/10.48550/arxiv.2406.09412","https://doi.org/10.48550/arxiv.1606.08415","https://doi.org/10.48550/arxiv.2401.09796","https://doi.org/10.48550/arxiv.1910.01108","https://doi.org/10.48550/arxiv.2203.02155","https://doi.org/10.48550/arxiv.2307.09288","https://doi.org/10.48550/arxiv.2104.09864","Hsiao_2024"],"data":{"https://doi.org/10.48550/arxiv.2411.07191":{"label":"https://doi.org/10.48550/arxiv.2411.07191","enumerator":"1","doi":"10.48550/ARXIV.2411.07191","html":"Yu, M., Wang, D., Shan, Q., Reed, C., & Wan, A. (2024). <i>The Super Weight in Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2411.07191\">10.48550/ARXIV.2411.07191</a>","url":"https://doi.org/10.48550/ARXIV.2411.07191"},"https://doi.org/10.48550/arxiv.2410.02725":{"label":"https://doi.org/10.48550/arxiv.2410.02725","enumerator":"2","doi":"10.48550/ARXIV.2410.02725","html":"Manvi, R., Singh, A., & Ermon, S. (2024). <i>Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2410.02725\">10.48550/ARXIV.2410.02725</a>","url":"https://doi.org/10.48550/ARXIV.2410.02725"},"https://doi.org/10.48550/arxiv.2404.03592":{"label":"https://doi.org/10.48550/arxiv.2404.03592","enumerator":"3","doi":"10.48550/ARXIV.2404.03592","html":"Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). <i>ReFT: Representation Finetuning for Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.03592\">10.48550/ARXIV.2404.03592</a>","url":"https://doi.org/10.48550/ARXIV.2404.03592"},"https://doi.org/10.48550/arxiv.2402.06111":{"label":"https://doi.org/10.48550/arxiv.2402.06111","enumerator":"4","doi":"10.48550/ARXIV.2402.06111","html":"Alshahwan, N., Harman, M., Marginean, A., Tal, R., & Wang, E. (2024). <i>Observation-based unit test generation at Meta</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.06111\">10.48550/ARXIV.2402.06111</a>","url":"https://doi.org/10.48550/ARXIV.2402.06111"},"https://doi.org/10.48550/arxiv.2403.20327":{"label":"https://doi.org/10.48550/arxiv.2403.20327","enumerator":"5","doi":"10.48550/ARXIV.2403.20327","html":"Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J. R., Hui, K., Boratko, M., Kapadia, R., Ding, W., Luan, Y., Duddu, S. M. K., Abrego, G. H., Shi, W., Gupta, N., Kusupati, A., Jain, P., Jonnalagadda, S. R., Chang, M.-W., & Naim, I. (2024). <i>Gecko: Versatile Text Embeddings Distilled from Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2403.20327\">10.48550/ARXIV.2403.20327</a>","url":"https://doi.org/10.48550/ARXIV.2403.20327"},"https://doi.org/10.48550/arxiv.2210.07128":{"label":"https://doi.org/10.48550/arxiv.2210.07128","enumerator":"6","doi":"10.48550/ARXIV.2210.07128","html":"Madaan, A., Zhou, S., Alon, U., Yang, Y., & Neubig, G. (2022). <i>Language Models of Code are Few-Shot Commonsense Learners</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2210.07128\">10.48550/ARXIV.2210.07128</a>","url":"https://doi.org/10.48550/ARXIV.2210.07128"},"https://doi.org/10.48550/arxiv.2407.10969":{"label":"https://doi.org/10.48550/arxiv.2407.10969","enumerator":"7","doi":"10.48550/ARXIV.2407.10969","html":"Wang, H., Ma, S., Wang, R., & Wei, F. (2024). <i>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2407.10969\">10.48550/ARXIV.2407.10969</a>","url":"https://doi.org/10.48550/ARXIV.2407.10969"},"https://doi.org/10.48550/arxiv.2207.01780":{"label":"https://doi.org/10.48550/arxiv.2207.01780","enumerator":"8","doi":"10.48550/ARXIV.2207.01780","html":"Le, H., Wang, Y., Gotmare, A. D., Savarese, S., & Hoi, S. C. H. (2022). <i>CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2207.01780\">10.48550/ARXIV.2207.01780</a>","url":"https://doi.org/10.48550/ARXIV.2207.01780"},"https://doi.org/10.48550/arxiv.2405.20541":{"label":"https://doi.org/10.48550/arxiv.2405.20541","enumerator":"9","doi":"10.48550/ARXIV.2405.20541","html":"Ankner, Z., Blakeney, C., Sreenivasan, K., Marion, M., Leavitt, M. L., & Paul, M. (2024). <i>Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2405.20541\">10.48550/ARXIV.2405.20541</a>","url":"https://doi.org/10.48550/ARXIV.2405.20541"},"https://doi.org/10.48550/arxiv.2402.17764":{"label":"https://doi.org/10.48550/arxiv.2402.17764","enumerator":"10","doi":"10.48550/ARXIV.2402.17764","html":"Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., & Wei, F. (2024). <i>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.17764\">10.48550/ARXIV.2402.17764</a>","url":"https://doi.org/10.48550/ARXIV.2402.17764"},"https://doi.org/10.48550/arxiv.2305.17493":{"label":"https://doi.org/10.48550/arxiv.2305.17493","enumerator":"11","doi":"10.48550/ARXIV.2305.17493","html":"Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., & Anderson, R. (2023). <i>The Curse of Recursion: Training on Generated Data Makes Models Forget</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2305.17493\">10.48550/ARXIV.2305.17493</a>","url":"https://doi.org/10.48550/ARXIV.2305.17493"},"https://doi.org/10.48550/arxiv.2404.07143":{"label":"https://doi.org/10.48550/arxiv.2404.07143","enumerator":"12","doi":"10.48550/ARXIV.2404.07143","html":"Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). <i>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.07143\">10.48550/ARXIV.2404.07143</a>","url":"https://doi.org/10.48550/ARXIV.2404.07143"},"https://doi.org/10.48550/arxiv.2406.07496":{"label":"https://doi.org/10.48550/arxiv.2406.07496","enumerator":"13","doi":"10.48550/ARXIV.2406.07496","html":"Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., & Zou, J. (2024). <i>TextGrad: Automatic “Differentiation” via Text</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.07496\">10.48550/ARXIV.2406.07496</a>","url":"https://doi.org/10.48550/ARXIV.2406.07496"},"https://doi.org/10.48550/arxiv.2406.02528":{"label":"https://doi.org/10.48550/arxiv.2406.02528","enumerator":"14","doi":"10.48550/ARXIV.2406.02528","html":"Zhu, R.-J., Zhang, Y., Sifferman, E., Sheaves, T., Wang, Y., Richmond, D., Zhou, P., & Eshraghian, J. K. (2024). <i>Scalable MatMul-free Language Modeling</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.02528\">10.48550/ARXIV.2406.02528</a>","url":"https://doi.org/10.48550/ARXIV.2406.02528"},"https://doi.org/10.48550/arxiv.1712.00676":{"label":"https://doi.org/10.48550/arxiv.1712.00676","enumerator":"15","doi":"10.48550/ARXIV.1712.00676","html":"Billings, J. J., McCaskey, A. J., Vallee, G., & Watson, G. (2017). <i>Will humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?</i> arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1712.00676\">10.48550/ARXIV.1712.00676</a>","url":"https://doi.org/10.48550/ARXIV.1712.00676"},"https://doi.org/10.48550/arxiv.2311.17035":{"label":"https://doi.org/10.48550/arxiv.2311.17035","enumerator":"16","doi":"10.48550/ARXIV.2311.17035","html":"Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tramèr, F., & Lee, K. (2023). <i>Scalable Extraction of Training Data from (Production) Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.17035\">10.48550/ARXIV.2311.17035</a>","url":"https://doi.org/10.48550/ARXIV.2311.17035"},"https://doi.org/10.48550/arxiv.2305.07759":{"label":"https://doi.org/10.48550/arxiv.2305.07759","enumerator":"17","doi":"10.48550/ARXIV.2305.07759","html":"Eldan, R., & Li, Y. (2023). <i>TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</i> arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2305.07759\">10.48550/ARXIV.2305.07759</a>","url":"https://doi.org/10.48550/ARXIV.2305.07759"},"https://doi.org/10.48550/arxiv.2311.12983":{"label":"https://doi.org/10.48550/arxiv.2311.12983","enumerator":"18","doi":"10.48550/ARXIV.2311.12983","html":"Mialon, G., Fourrier, C., Swift, C., Wolf, T., LeCun, Y., & Scialom, T. (2023). <i>GAIA: a benchmark for General AI Assistants</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.12983\">10.48550/ARXIV.2311.12983</a>","url":"https://doi.org/10.48550/ARXIV.2311.12983"},"https://doi.org/10.48550/arxiv.2406.04692":{"label":"https://doi.org/10.48550/arxiv.2406.04692","enumerator":"19","doi":"10.48550/ARXIV.2406.04692","html":"Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., & Zou, J. (2024). <i>Mixture-of-Agents Enhances Large Language Model Capabilities</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.04692\">10.48550/ARXIV.2406.04692</a>","url":"https://doi.org/10.48550/ARXIV.2406.04692"},"https://doi.org/10.48550/arxiv.2311.05884":{"label":"https://doi.org/10.48550/arxiv.2311.05884","enumerator":"20","doi":"10.48550/ARXIV.2311.05884","html":"Gui, H., Wang, R., Yin, K., Jin, L., Kula, M., Xu, T., Hong, L., & Chi, E. H. (2023). <i>Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.05884\">10.48550/ARXIV.2311.05884</a>","url":"https://doi.org/10.48550/ARXIV.2311.05884"},"https://doi.org/10.48550/arxiv.2307.11760":{"label":"https://doi.org/10.48550/arxiv.2307.11760","enumerator":"21","doi":"10.48550/ARXIV.2307.11760","html":"Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., & Xie, X. (2023). <i>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.11760\">10.48550/ARXIV.2307.11760</a>","url":"https://doi.org/10.48550/ARXIV.2307.11760"},"https://doi.org/10.48550/arxiv.2402.09171":{"label":"https://doi.org/10.48550/arxiv.2402.09171","enumerator":"22","doi":"10.48550/ARXIV.2402.09171","html":"Alshahwan, N., Chheda, J., Finegenova, A., Gokkaya, B., Harman, M., Harper, I., Marginean, A., Sengupta, S., & Wang, E. (2024). <i>Automated Unit Test Improvement using Large Language Models at Meta</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.09171\">10.48550/ARXIV.2402.09171</a>","url":"https://doi.org/10.48550/ARXIV.2402.09171"},"https://doi.org/10.48550/arxiv.2404.01413":{"label":"https://doi.org/10.48550/arxiv.2404.01413","enumerator":"23","doi":"10.48550/ARXIV.2404.01413","html":"Gerstgrasser, M., Schaeffer, R., Dey, A., Rafailov, R., Sleight, H., Hughes, J., Korbak, T., Agrawal, R., Pai, D., Gromov, A., Roberts, D. A., Yang, D., Donoho, D. L., & Koyejo, S. (2024). <i>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.01413\">10.48550/ARXIV.2404.01413</a>","url":"https://doi.org/10.48550/ARXIV.2404.01413"},"https://doi.org/10.48550/arxiv.2102.04518":{"label":"https://doi.org/10.48550/arxiv.2102.04518","enumerator":"24","doi":"10.48550/ARXIV.2102.04518","html":"Agostinelli, F., Shmakov, A., McAleer, S., Fox, R., & Baldi, P. (2021). <i>A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2102.04518\">10.48550/ARXIV.2102.04518</a>","url":"https://doi.org/10.48550/ARXIV.2102.04518"},"https://doi.org/10.48550/arxiv.2402.06196":{"label":"https://doi.org/10.48550/arxiv.2402.06196","enumerator":"25","doi":"10.48550/ARXIV.2402.06196","html":"Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2024). <i>Large Language Models: A Survey</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.06196\">10.48550/ARXIV.2402.06196</a>","url":"https://doi.org/10.48550/ARXIV.2402.06196"},"https://doi.org/10.48550/arxiv.2402.14433":{"label":"https://doi.org/10.48550/arxiv.2402.14433","enumerator":"26","doi":"10.48550/ARXIV.2402.14433","html":"von Rütte, D., Anagnostidis, S., Bachmann, G., & Hofmann, T. (2024). <i>A Language Model’s Guide Through Latent Space</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.14433\">10.48550/ARXIV.2402.14433</a>","url":"https://doi.org/10.48550/ARXIV.2402.14433"},"https://doi.org/10.48550/arxiv.2205.05124":{"label":"https://doi.org/10.48550/arxiv.2205.05124","enumerator":"27","doi":"10.48550/ARXIV.2205.05124","html":"Subramani, N., Suresh, N., & Peters, M. E. (2022). <i>Extracting Latent Steering Vectors from Pretrained Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2205.05124\">10.48550/ARXIV.2205.05124</a>","url":"https://doi.org/10.48550/ARXIV.2205.05124"},"https://doi.org/10.48550/arxiv.1910.10683":{"label":"https://doi.org/10.48550/arxiv.1910.10683","enumerator":"28","doi":"10.48550/ARXIV.1910.10683","html":"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019). <i>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1910.10683\">10.48550/ARXIV.1910.10683</a>","url":"https://doi.org/10.48550/ARXIV.1910.10683"},"https://doi.org/10.48550/arxiv.2308.10248":{"label":"https://doi.org/10.48550/arxiv.2308.10248","enumerator":"29","doi":"10.48550/ARXIV.2308.10248","html":"Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez, J. J., Mini, U., & MacDiarmid, M. (2023). <i>Steering Language Models With Activation Engineering</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2308.10248\">10.48550/ARXIV.2308.10248</a>","url":"https://doi.org/10.48550/ARXIV.2308.10248"},"https://doi.org/10.48550/arxiv.2107.03374":{"label":"https://doi.org/10.48550/arxiv.2107.03374","enumerator":"30","doi":"10.48550/ARXIV.2107.03374","html":"Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). <i>Evaluating Large Language Models Trained on Code</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2107.03374\">10.48550/ARXIV.2107.03374</a>","url":"https://doi.org/10.48550/ARXIV.2107.03374"},"https://doi.org/10.48550/arxiv.2406.02543":{"label":"https://doi.org/10.48550/arxiv.2406.02543","enumerator":"31","doi":"10.48550/ARXIV.2406.02543","html":"Yadkori, Y. A., Kuzborskij, I., György, A., & Szepesvári, C. (2024). <i>To Believe or Not to Believe Your LLM</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.02543\">10.48550/ARXIV.2406.02543</a>","url":"https://doi.org/10.48550/ARXIV.2406.02543"},"https://doi.org/10.48550/arxiv.2404.14619":{"label":"https://doi.org/10.48550/arxiv.2404.14619","enumerator":"32","doi":"10.48550/ARXIV.2404.14619","html":"Mehta, S., Sekhavat, M. H., Cao, Q., Horton, M., Jin, Y., Sun, C., Mirzadeh, I., Najibi, M., Belenko, D., Zatloukal, P., & Rastegari, M. (2024). <i>OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.14619\">10.48550/ARXIV.2404.14619</a>","url":"https://doi.org/10.48550/ARXIV.2404.14619"},"https://doi.org/10.48550/arxiv.1706.03741":{"label":"https://doi.org/10.48550/arxiv.1706.03741","enumerator":"33","doi":"10.48550/ARXIV.1706.03741","html":"Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). <i>Deep reinforcement learning from human preferences</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1706.03741\">10.48550/ARXIV.1706.03741</a>","url":"https://doi.org/10.48550/ARXIV.1706.03741"},"https://doi.org/10.48550/arxiv.2307.08925":{"label":"https://doi.org/10.48550/arxiv.2307.08925","enumerator":"34","doi":"10.48550/ARXIV.2307.08925","html":"Chen, C., Feng, X., Li, Y., Lyu, L., Zhou, J., Zheng, X., & Yin, J. (2023). <i>Integration of Large Language Models and Federated Learning</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.08925\">10.48550/ARXIV.2307.08925</a>","url":"https://doi.org/10.48550/ARXIV.2307.08925"},"https://doi.org/10.48550/arxiv.2212.02508":{"label":"https://doi.org/10.48550/arxiv.2212.02508","enumerator":"35","doi":"10.48550/ARXIV.2212.02508","html":"Li, Y., Yuan, R., Zhang, G., Ma, Y., Lin, C., Chen, X., Ragni, A., Yin, H., Hu, Z., He, H., Benetos, E., Gyenge, N., Liu, R., & Fu, J. (2022). <i>MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2212.02508\">10.48550/ARXIV.2212.02508</a>","url":"https://doi.org/10.48550/ARXIV.2212.02508"},"https://doi.org/10.48550/arxiv.2302.13971":{"label":"https://doi.org/10.48550/arxiv.2302.13971","enumerator":"36","doi":"10.48550/ARXIV.2302.13971","html":"Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). <i>LLaMA: Open and Efficient Foundation Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2302.13971\">10.48550/ARXIV.2302.13971</a>","url":"https://doi.org/10.48550/ARXIV.2302.13971"},"https://doi.org/10.48550/arxiv.1607.06450":{"label":"https://doi.org/10.48550/arxiv.1607.06450","enumerator":"37","doi":"10.48550/ARXIV.1607.06450","html":"Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). <i>Layer Normalization</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1607.06450\">10.48550/ARXIV.1607.06450</a>","url":"https://doi.org/10.48550/ARXIV.1607.06450"},"https://doi.org/10.48550/arxiv.1706.03762":{"label":"https://doi.org/10.48550/arxiv.1706.03762","enumerator":"38","doi":"10.48550/ARXIV.1706.03762","html":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). <i>Attention Is All You Need</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1706.03762\">10.48550/ARXIV.1706.03762</a>","url":"https://doi.org/10.48550/ARXIV.1706.03762"},"https://doi.org/10.48550/arxiv.2406.09412":{"label":"https://doi.org/10.48550/arxiv.2406.09412","enumerator":"39","doi":"10.48550/ARXIV.2406.09412","html":"Zhang, Y., Li, H., Liu, J., & Yue, X. (2024). <i>Explore the Limits of Omni-modal Pretraining at Scale</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.09412\">10.48550/ARXIV.2406.09412</a>","url":"https://doi.org/10.48550/ARXIV.2406.09412"},"https://doi.org/10.48550/arxiv.1606.08415":{"label":"https://doi.org/10.48550/arxiv.1606.08415","enumerator":"40","doi":"10.48550/ARXIV.1606.08415","html":"Hendrycks, D., & Gimpel, K. (2016). <i>Gaussian Error Linear Units (GELUs)</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1606.08415\">10.48550/ARXIV.1606.08415</a>","url":"https://doi.org/10.48550/ARXIV.1606.08415"},"https://doi.org/10.48550/arxiv.2401.09796":{"label":"https://doi.org/10.48550/arxiv.2401.09796","enumerator":"41","doi":"10.48550/ARXIV.2401.09796","html":"Huang, W., Wang, Y., Cheng, A., Zhou, A., Yu, C., & Wang, L. (2024). <i>A Fast, Performant, Secure Distributed Training Framework For Large Language Model</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2401.09796\">10.48550/ARXIV.2401.09796</a>","url":"https://doi.org/10.48550/ARXIV.2401.09796"},"https://doi.org/10.48550/arxiv.1910.01108":{"label":"https://doi.org/10.48550/arxiv.1910.01108","enumerator":"42","doi":"10.48550/ARXIV.1910.01108","html":"Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). <i>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1910.01108\">10.48550/ARXIV.1910.01108</a>","url":"https://doi.org/10.48550/ARXIV.1910.01108"},"https://doi.org/10.48550/arxiv.2203.02155":{"label":"https://doi.org/10.48550/arxiv.2203.02155","enumerator":"43","doi":"10.48550/ARXIV.2203.02155","html":"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). <i>Training language models to follow instructions with human feedback</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2203.02155\">10.48550/ARXIV.2203.02155</a>","url":"https://doi.org/10.48550/ARXIV.2203.02155"},"https://doi.org/10.48550/arxiv.2307.09288":{"label":"https://doi.org/10.48550/arxiv.2307.09288","enumerator":"44","doi":"10.48550/ARXIV.2307.09288","html":"Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., … Scialom, T. (2023). <i>Llama 2: Open Foundation and Fine-Tuned Chat Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.09288\">10.48550/ARXIV.2307.09288</a>","url":"https://doi.org/10.48550/ARXIV.2307.09288"},"https://doi.org/10.48550/arxiv.2104.09864":{"label":"https://doi.org/10.48550/arxiv.2104.09864","enumerator":"45","doi":"10.48550/ARXIV.2104.09864","html":"Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). <i>RoFormer: Enhanced Transformer with Rotary Position Embedding</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2104.09864\">10.48550/ARXIV.2104.09864</a>","url":"https://doi.org/10.48550/ARXIV.2104.09864"},"Hsiao_2024":{"label":"Hsiao_2024","enumerator":"46","doi":"10.1111/tops.12737","html":"Hsiao, J. H. (2024). Understanding Human Cognition Through Computational Modeling. <i>Topics in Cognitive Science</i>, <i>16</i>(3), 349–376. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1111/tops.12737\">10.1111/tops.12737</a>","url":"https://doi.org/10.1111/tops.12737"}}}}}