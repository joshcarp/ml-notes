{"version":"1","records":[{"hierarchy":{"lvl1":"Dropout"},"type":"lvl1","url":"/glossary/dropout-1","position":0},{"hierarchy":{"lvl1":"Dropout"},"content":"","type":"content","url":"/glossary/dropout-1","position":1},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"type":"lvl2","url":"/glossary/dropout-1#definition","position":2},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/dropout-1#definition","position":3},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"type":"lvl2","url":"/glossary/dropout-1#tags","position":4},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"content":"Optimization, Training","type":"content","url":"/glossary/dropout-1#tags","position":5},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"type":"lvl2","url":"/glossary/dropout-1#references","position":6},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"content":"","type":"content","url":"/glossary/dropout-1#references","position":7},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/dropout-1#additional-notes","position":8},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"content":"Dropout is a regularization technique used in neural networks to prevent overfitting. Here’s how it works:\n\nDuring training, dropout randomly “turns off” (sets to zero) a certain percentage of neurons in a layer with each training batch. Typically, you might drop out 20-50% of neurons. When a neuron is dropped out, it doesn’t participate in forward propagation or backpropagation for that specific training batch.\n\nThink of it like forcing the network to learn with an incomplete brain each time. This has several beneficial effects:\n\nDuring inference (when actually using the model), dropout is turned off and all neurons are active. To compensate for having more active neurons than during training, the weights are typically scaled proportionally.\n\nA helpful analogy is to think of dropout like studying for an exam where you know some of your study materials will be unavailable during the test. This forces you to develop a more robust understanding rather than relying on any single source too heavily.\n\nInterestingly, this technique was partly inspired by sexual reproduction in nature, where genes are randomly combined from two parents, forcing useful genes to work well in many different combinations rather than relying on specific gene combinations.","type":"content","url":"/glossary/dropout-1#additional-notes","position":9},{"hierarchy":{"lvl1":"Dropout"},"type":"lvl1","url":"/glossary/dropout-1","position":0},{"hierarchy":{"lvl1":"Dropout"},"content":"","type":"content","url":"/glossary/dropout-1","position":1},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"type":"lvl2","url":"/glossary/dropout-1#definition","position":2},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/dropout-1#definition","position":3},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"type":"lvl2","url":"/glossary/dropout-1#tags","position":4},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"content":"Optimization, Training","type":"content","url":"/glossary/dropout-1#tags","position":5},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"type":"lvl2","url":"/glossary/dropout-1#references","position":6},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"content":"","type":"content","url":"/glossary/dropout-1#references","position":7},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/dropout-1#additional-notes","position":8},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"content":"Dropout is a regularization technique used in neural networks to prevent overfitting. Here’s how it works:\n\nDuring training, dropout randomly “turns off” (sets to zero) a certain percentage of neurons in a layer with each training batch. Typically, you might drop out 20-50% of neurons. When a neuron is dropped out, it doesn’t participate in forward propagation or backpropagation for that specific training batch.\n\nThink of it like forcing the network to learn with an incomplete brain each time. This has several beneficial effects:\n\nDuring inference (when actually using the model), dropout is turned off and all neurons are active. To compensate for having more active neurons than during training, the weights are typically scaled proportionally.\n\nA helpful analogy is to think of dropout like studying for an exam where you know some of your study materials will be unavailable during the test. This forces you to develop a more robust understanding rather than relying on any single source too heavily.\n\nInterestingly, this technique was partly inspired by sexual reproduction in nature, where genes are randomly combined from two parents, forcing useful genes to work well in many different combinations rather than relying on specific gene combinations.","type":"content","url":"/glossary/dropout-1#additional-notes","position":9},{"hierarchy":{"lvl1":"Feed-Forward Network"},"type":"lvl1","url":"/glossary/feed-forward-network","position":0},{"hierarchy":{"lvl1":"Feed-Forward Network"},"content":"","type":"content","url":"/glossary/feed-forward-network","position":1},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Definition"},"type":"lvl2","url":"/glossary/feed-forward-network#definition","position":2},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Definition"},"content":"A Feed-Forward Network (FFN) is a fundamental neural network architecture where information flows in one direction, from input to output layers, through one or more hidden layers, without any cycles or loops. Each neuron in a layer is connected to all neurons in the subsequent layer, but there are no connections between neurons in the same layer or backwards connections. FFNs are also known as Multi-Layer Perceptrons (MLPs) and form basic building blocks in more complex architectures like transformers.","type":"content","url":"/glossary/feed-forward-network#definition","position":3},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Tags"},"type":"lvl2","url":"/glossary/feed-forward-network#tags","position":4},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Tags"},"content":"Neural Networks, Architecture, Basic Concepts, MLP","type":"content","url":"/glossary/feed-forward-network#tags","position":5},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"References"},"type":"lvl2","url":"/glossary/feed-forward-network#references","position":6},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"References"},"content":"Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536. \n\nRumelhart et al. (1986)","type":"content","url":"/glossary/feed-forward-network#references","position":7},{"hierarchy":{"lvl1":"Fine-Tuning"},"type":"lvl1","url":"/glossary/fine-tuning-1","position":0},{"hierarchy":{"lvl1":"Fine-Tuning"},"content":"","type":"content","url":"/glossary/fine-tuning-1","position":1},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/fine-tuning-1#definition","position":2},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/fine-tuning-1#definition","position":3},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/fine-tuning-1#tags","position":4},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"content":"Fine-tuning","type":"content","url":"/glossary/fine-tuning-1#tags","position":5},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/fine-tuning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"content":"Mode seeking occurs in the “Fine-tuning” phase - Once the model has a distribution to work on, the model needs to be “fine-tuned”. Here the model will learn which parts of the distribution are useful and slightly diverge from the original distribution to make more “useful” behaviours more likely. It’s important to keep the distribution matching the original distribution to a certain extent. This can lead to model collapse where the model overfits on the fine-tuning examples and isn’t able to handle inputs that haven’t been fine-tuned because the rest of the distributions have been destroyed, negating the effect of pre-training.","type":"content","url":"/glossary/fine-tuning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Fine-Tuning"},"type":"lvl1","url":"/glossary/fine-tuning-1","position":0},{"hierarchy":{"lvl1":"Fine-Tuning"},"content":"","type":"content","url":"/glossary/fine-tuning-1","position":1},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/fine-tuning-1#definition","position":2},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/fine-tuning-1#definition","position":3},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/fine-tuning-1#tags","position":4},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"content":"Fine-tuning","type":"content","url":"/glossary/fine-tuning-1#tags","position":5},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/fine-tuning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"content":"Mode seeking occurs in the “Fine-tuning” phase - Once the model has a distribution to work on, the model needs to be “fine-tuned”. Here the model will learn which parts of the distribution are useful and slightly diverge from the original distribution to make more “useful” behaviours more likely. It’s important to keep the distribution matching the original distribution to a certain extent. This can lead to model collapse where the model overfits on the fine-tuning examples and isn’t able to handle inputs that haven’t been fine-tuned because the rest of the distributions have been destroyed, negating the effect of pre-training.","type":"content","url":"/glossary/fine-tuning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Glossary"},"type":"lvl1","url":"/glossary","position":0},{"hierarchy":{"lvl1":"Glossary"},"content":"A collection of terms and definitions used throughout the documentation.\n\nTerm\n\nTags\n\nFull Notes\n\nSparse/Dense Reward\n\nTraining\n\nDetails\n\nFine-Tuning\n\nFine-tuning\n\nDetails\n\nPre-Training\n\nPre-training\n\nDetails\n\nReinforcement Learning\n\nReinforcement learning, Training\n\nDetails\n\nDropout\n\nOptimization, Training\n\nDetails\n\nTransformer\n\nModel Architecture, Training\n\nDetails\n\nLoRA\n\nOptimization, Training, Fine-tuning, Parameter-efficient training\n\nDetails\n\nModel Collapse\n\nTraining, Failure modes, Optimization\n\nDetails\n\nFeed-Forward Network\n\nNeural Networks, Architecture, Basic Concepts\n\nDetails\n\nResidual Stream\n\nTransformers, Architecture, Deep Learning\n\nDetails\n\nRAG\n\nNatural Language Processing, Information Retrieval, Text Generation\n\nDetails\n\nPoly-semanticity\n\nInterpretability, Neural Networks, Semantics, Model Analysis\n\nDetails","type":"content","url":"/glossary","position":1},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)"},"type":"lvl1","url":"/glossary/lora","position":0},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)"},"content":"","type":"content","url":"/glossary/lora","position":1},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Definition"},"type":"lvl2","url":"/glossary/lora#definition","position":2},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Definition"},"content":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","type":"content","url":"/glossary/lora#definition","position":3},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Tags"},"type":"lvl2","url":"/glossary/lora#tags","position":4},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Tags"},"content":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","type":"content","url":"/glossary/lora#tags","position":5},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"References"},"type":"lvl2","url":"/glossary/lora#references","position":6},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"References"},"content":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. \n\nHu et al. (2021)","type":"content","url":"/glossary/lora#references","position":7},{"hierarchy":{"lvl1":"Model Collapse"},"type":"lvl1","url":"/glossary/model-collapse","position":0},{"hierarchy":{"lvl1":"Model Collapse"},"content":"","type":"content","url":"/glossary/model-collapse","position":1},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Definition"},"type":"lvl2","url":"/glossary/model-collapse#definition","position":2},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Definition"},"content":"Model collapse refers to a failure mode in training where a model converges to a degenerate state, producing limited or uniform outputs regardless of different inputs. This phenomenon is particularly common in generative models like GANs, where the generator might produce only a small subset of possible outputs, failing to capture the full diversity of the training distribution. In language models, it can manifest as repetitive or generic responses regardless of input prompts.","type":"content","url":"/glossary/model-collapse#definition","position":3},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Tags"},"type":"lvl2","url":"/glossary/model-collapse#tags","position":4},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Tags"},"content":"Training, Failure modes, Optimization, GANs, Model behavior","type":"content","url":"/glossary/model-collapse#tags","position":5},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"References"},"type":"lvl2","url":"/glossary/model-collapse#references","position":6},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"References"},"content":"Arjovsky, M., & Bottou, L. (2017). Towards Principled Methods for Training Generative Adversarial Networks. \n\nArjovsky & Bottou (2017)\n\nSrivastava, A., Valkov, L., Russell, C., Gutmann, M. U., & Sutton, C. (2017). VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning. \n\nSrivastava et al. (2017)","type":"content","url":"/glossary/model-collapse#references","position":7},{"hierarchy":{"lvl1":"Poly-semanticity"},"type":"lvl1","url":"/glossary/poly-semanticity","position":0},{"hierarchy":{"lvl1":"Poly-semanticity"},"content":"","type":"content","url":"/glossary/poly-semanticity","position":1},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Definition"},"type":"lvl2","url":"/glossary/poly-semanticity#definition","position":2},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Definition"},"content":"Poly-semanticity refers to the phenomenon in neural networks where individual neurons or features encode multiple distinct concepts or meanings simultaneously. This property is particularly observed in large language models and deep neural networks, where single components (neurons, attention heads, or feature dimensions) respond to or represent multiple semantic concepts, making interpretation and analysis of these networks more complex. Understanding poly-semanticity is crucial for model interpretability and optimization.","type":"content","url":"/glossary/poly-semanticity#definition","position":3},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Tags"},"type":"lvl2","url":"/glossary/poly-semanticity#tags","position":4},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Tags"},"content":"Interpretability, Neural Networks, Semantics, Model Analysis, Representation Learning","type":"content","url":"/glossary/poly-semanticity#tags","position":5},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"References"},"type":"lvl2","url":"/glossary/poly-semanticity#references","position":6},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"References"},"content":"Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Ndousse, K., Jones, C., DasSarma, N., Hernandez, D., Drain, D., Ganguli, D., Chen, Z., Hatfield-Dodds, Z., Kernion, J., Nova, T., Lovitt, L., Sellitto, M., Kundu, S., ... Kaplan, J. (2022). Transformer Circuits Thread. \n\nhttps://​transformer​-circuits​.pub​/\n\nCammarata, N., Goh, G., Carter, S., Petrov, M., Schubert, L., Gao, C., ... & Olah, C. (2020). Thread: Circuits. Distill. \n\nCammarata et al. (2020)","type":"content","url":"/glossary/poly-semanticity#references","position":7},{"hierarchy":{"lvl1":"Pre-Training"},"type":"lvl1","url":"/glossary/pre-training-1","position":0},{"hierarchy":{"lvl1":"Pre-Training"},"content":"","type":"content","url":"/glossary/pre-training-1","position":1},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"type":"lvl2","url":"/glossary/pre-training-1#definition","position":2},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/pre-training-1#definition","position":3},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"type":"lvl2","url":"/glossary/pre-training-1#tags","position":4},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"content":"Pre-training","type":"content","url":"/glossary/pre-training-1#tags","position":5},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/pre-training-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"content":"This is the “PT” in “GPT”, and it was one of the breakthroughs in the og GPT paper\n\nDistribution matching occurs in this phase; This is where the model is trained on bulk internet data. The model learns how to predict the next token by minimising the loss between P(predicted-token|context) and P(actual-token|context).\n\nA concrete example of there this occurs is in the\n\nThis concept was introduced in the\n\nPre-training effectively cuts down on this upper limit by making it more likely that the model will predict coherent sentences, which means that it will converge on correct behaviour orders of magnitude faster.","type":"content","url":"/glossary/pre-training-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Pre-Training"},"type":"lvl1","url":"/glossary/pre-training-1","position":0},{"hierarchy":{"lvl1":"Pre-Training"},"content":"","type":"content","url":"/glossary/pre-training-1","position":1},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"type":"lvl2","url":"/glossary/pre-training-1#definition","position":2},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/pre-training-1#definition","position":3},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"type":"lvl2","url":"/glossary/pre-training-1#tags","position":4},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"content":"Pre-training","type":"content","url":"/glossary/pre-training-1#tags","position":5},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/pre-training-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"content":"This is the “PT” in “GPT”, and it was one of the breakthroughs in the og GPT paper\n\nDistribution matching occurs in this phase; This is where the model is trained on bulk internet data. The model learns how to predict the next token by minimising the loss between P(predicted-token|context) and P(actual-token|context).\n\nA concrete example of there this occurs is in the\n\nThis concept was introduced in the\n\nPre-training effectively cuts down on this upper limit by making it more likely that the model will predict coherent sentences, which means that it will converge on correct behaviour orders of magnitude faster.","type":"content","url":"/glossary/pre-training-1#additional-notes","position":7},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)"},"type":"lvl1","url":"/glossary/rag","position":0},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)"},"content":"","type":"content","url":"/glossary/rag","position":1},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Definition"},"type":"lvl2","url":"/glossary/rag#definition","position":2},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Definition"},"content":"RAG is a hybrid framework that combines retrieval-based and generation-based approaches for text generation. It enhances language model outputs by first retrieving relevant documents or passages from a knowledge base, then conditioning the generation process on both the input query and the retrieved information. This approach helps ground the model’s responses in specific, relevant information while maintaining the flexibility of generative models.","type":"content","url":"/glossary/rag#definition","position":3},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Tags"},"type":"lvl2","url":"/glossary/rag#tags","position":4},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Tags"},"content":"Natural Language Processing, Information Retrieval, Text Generation, Knowledge Base","type":"content","url":"/glossary/rag#tags","position":5},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"References"},"type":"lvl2","url":"/glossary/rag#references","position":6},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"References"},"content":"Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., & Kiela, D. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. \n\nLewis et al. (2020)","type":"content","url":"/glossary/rag#references","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning"},"type":"lvl1","url":"/glossary/reinforcement-learning-1","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#definition","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1#definition","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#tags","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"content":"Reinforcement learning, Training","type":"content","url":"/glossary/reinforcement-learning-1#tags","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"content":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.\n\nThink of it like teaching a dog new tricks:\n\nThe dog (agent) performs actions\n\nYou give treats or praise (rewards) for good behavior\n\nThe dog learns which actions lead to treats\n\nOver time, it figures out the best sequence of actions to get rewards\nKey Components:\n\nThe Learning Process:\n\nCommon Approaches:\n\nQ-Learning: Learns the value of actions in different states\n\nPolicy Gradient: Directly learns the best policy\n\nDeep RL: Combines deep neural networks with RL\n\nModel-Based RL: Learns a model of the environment\nReal-World Applications:\n\nGame playing (AlphaGo, OpenAI Five)\n\nRobotics and robot control\n\nResource management\n\nRecommendation systems\n\nAutonomous vehicles\n\nTrading strategies\nKey Challenges:\n\nExploration vs. Exploitation trade-off\n\nDelayed rewards (credit assignment problem)\n\nLarge state/action spaces\n\nSample efficiency\n\nStability during training\nThe power of RL lies in its ability to learn through trial and error, discovering solutions that might not be obvious to human programmers. Unlike supervised learning, which requires labeled examples, RL can learn from raw experience in the environment.","type":"content","url":"/glossary/reinforcement-learning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning"},"type":"lvl1","url":"/glossary/reinforcement-learning-1","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#definition","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1#definition","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#tags","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"content":"Reinforcement learning, Training","type":"content","url":"/glossary/reinforcement-learning-1#tags","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"content":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.\n\nThink of it like teaching a dog new tricks:\n\nThe dog (agent) performs actions\n\nYou give treats or praise (rewards) for good behavior\n\nThe dog learns which actions lead to treats\n\nOver time, it figures out the best sequence of actions to get rewards\nKey Components:\n\nThe Learning Process:\n\nCommon Approaches:\n\nQ-Learning: Learns the value of actions in different states\n\nPolicy Gradient: Directly learns the best policy\n\nDeep RL: Combines deep neural networks with RL\n\nModel-Based RL: Learns a model of the environment\nReal-World Applications:\n\nGame playing (AlphaGo, OpenAI Five)\n\nRobotics and robot control\n\nResource management\n\nRecommendation systems\n\nAutonomous vehicles\n\nTrading strategies\nKey Challenges:\n\nExploration vs. Exploitation trade-off\n\nDelayed rewards (credit assignment problem)\n\nLarge state/action spaces\n\nSample efficiency\n\nStability during training\nThe power of RL lies in its ability to learn through trial and error, discovering solutions that might not be obvious to human programmers. Unlike supervised learning, which requires labeled examples, RL can learn from raw experience in the environment.","type":"content","url":"/glossary/reinforcement-learning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Residual Stream"},"type":"lvl1","url":"/glossary/residual-stream","position":0},{"hierarchy":{"lvl1":"Residual Stream"},"content":"","type":"content","url":"/glossary/residual-stream","position":1},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Definition"},"type":"lvl2","url":"/glossary/residual-stream#definition","position":2},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Definition"},"content":"The residual stream is a core concept in transformer architectures, referring to the main pathway through which information flows across layers. It maintains a persistent representation that each sublayer (self-attention and feed-forward network) modifies via residual connections. The residual stream allows for better gradient flow during training and helps maintain information from earlier layers throughout the network depth. Each sublayer processes the stream and adds its output back to it, rather than completely transforming it.","type":"content","url":"/glossary/residual-stream#definition","position":3},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Tags"},"type":"lvl2","url":"/glossary/residual-stream#tags","position":4},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Tags"},"content":"Transformers, Architecture, Deep Learning, Residual Connections","type":"content","url":"/glossary/residual-stream#tags","position":5},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"References"},"type":"lvl2","url":"/glossary/residual-stream#references","position":6},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"References"},"content":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. \n\nVaswani et al. (2017)\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. \n\nHe et al. (2015)","type":"content","url":"/glossary/residual-stream#references","position":7},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"type":"lvl1","url":"/glossary/sparsedense-reward-1","position":0},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1","position":1},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#definition","position":2},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1#definition","position":3},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#tags","position":4},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"content":"Training","type":"content","url":"/glossary/sparsedense-reward-1#tags","position":5},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"content":"A reward that occurs less frequently when compared to a dense feedback. Post-training is a lot more sparse than pre-training because reward is only given after the action can be judged.","type":"content","url":"/glossary/sparsedense-reward-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"type":"lvl1","url":"/glossary/sparsedense-reward-1","position":0},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1","position":1},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#definition","position":2},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1#definition","position":3},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#tags","position":4},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"content":"Training","type":"content","url":"/glossary/sparsedense-reward-1#tags","position":5},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"content":"A reward that occurs less frequently when compared to a dense feedback. Post-training is a lot more sparse than pre-training because reward is only given after the action can be judged.","type":"content","url":"/glossary/sparsedense-reward-1#additional-notes","position":7},{"hierarchy":{"lvl1":"The Transformer"},"type":"lvl1","url":"/glossary/transformer-1","position":0},{"hierarchy":{"lvl1":"The Transformer"},"content":"📢 TLDR: Links and notes to transformer related research papers.\n\nBelow is a list of all the important research papers to fully understand the transformer architecture as introduced in the “Attention is all you need” paper by Google in 2017.\n\nThis page has a collection of research papers + notes in a directed graph to indicate dependencies between the papers and is to be used as a reference page. Obviously there’s still more to add (RNNs, LSTMs, etc), and they are on my reading list and will be added in time.%%{init: {'theme': 'base', 'themeVariables': { 'nodeTextColor': '#333333', 'mainBkg': '#f0f0f0', 'lineColor': '#F8B229'}}}%%\n\ngraph TD\n\t\ttitle[Research Collections: The Transformer]\n    subgraph \"Neural Networks: Learning\"\n        A[\"<a href='https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf'>Learning representations by back-propagating errors</a>\"]\n        B[\"<a href='https://arxiv.org/abs/1512.03385'>Deep Residual Learning</a>\"]\n        C[\"<a href='https://arxiv.org/abs/1412.6980'>Adam: A Method for Stochastic Optimization</a>\"]\n        D[\"<a href='https://arxiv.org/abs/1711.05101'>Decoupled Weight Decay Regularization</a>\"]\n    end\n\n    subgraph \"Model Components\"\n        E[\"<a href='https://arxiv.org/abs/1606.08415v5'>Gaussian Error Linear Units (GELUs)</a>\"]\n        F[\"<a href='https://arxiv.org/abs/1607.06450'>Layer Normalization</a>\"]\n    end\n\n    subgraph \"The Transformer\"\n\t\t    G[\"<a href='https://arxiv.org/abs/1706.03762v7'>Attention Is All You Need</a>\"]\n        X[\"<a href='https://arxiv.org/abs/1911.02150'>Fast Transformer Decoding: One Write-Head is All You Need</a>\"]\n        H[\"<a href='https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf'>Improving Language Understanding by Generative Pre-Training</a>\"]\n\n    end\n\n    subgraph \"Training LLMs\"\n\t\t\t\tI[\"<a href='https://arxiv.org/abs/1706.03741'>Deep reinforcement learning from human preferences</a>\"]\n        J[\"<a href='https://arxiv.org/abs/2203.02155'>Training language models to follow instructions with human feedback</a>\"]\n    end\n\n    A --> B\n    B --> G\n    C --> D\n    D --> G\n    E --> G\n    F --> G\n    G --> X\n    X --> H\n    H --> J\n    I --> J","type":"content","url":"/glossary/transformer-1","position":1},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"type":"lvl2","url":"/glossary/transformer-1#neural-networks","position":2},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"content":"Neural networks have been around for a while and these are core components of what allows neural networks and transformers to be effective at what they do.\n\n**Learning representations by back-propagating errors -** Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s. Before this paper Multi-layer-perceptrons (MLPs) weren’t very common because they were very, very difficult to train.\n\n**Deep Residual Learning for Image Recognition** - The introduction of residuals (also known as skip connections) allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which allows for gradients to flow backwards through back propagation to earlier components without needing to go through intermediate layers. This greatly speeds up convergence.\n\n**Layer Normalization -** Layernorm happens in each layer to make sure that the values don’t explode and is applied at each layers output activations. It makes the surface more regular so that it’s more symmetrical and easier to optimize over. This means that weight updates can take the same increment step in all directions and not need to worry about overstepping in one dimension but under-stepping in another, and the outputs of the activations will be more regular so that the inputs to the activation functions are all within the same order of magnitude - for example one input to a neuron being 0.00001 and another one being 1,000,000 would cause a lot of problems for floating point rounding and quantization, for example.\n\n**Gaussian Error Linear Units (GELUs) -** Activation function ****that leaves positive values unchanged but maps negative numbers to near zero. Implemented here in \n\nllm.go. Other architectures use different activation functions. For example, OpenElm uses SwiGLU FFN which I don’t exactly understand. Should probably add that to the reading list.","type":"content","url":"/glossary/transformer-1#neural-networks","position":3},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"type":"lvl2","url":"/glossary/transformer-1#optimizers","position":4},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"content":"Optimizers are the functions that control how the weights get changed during training.\n\n**Adam: A Method for Stochastic Optimization -** Introduced the Adam optimiser. Weight updates are important because it causes the training to converge more quickly. Adam has two parameters for each model parameter.\n\n**Decoupled Weight Decay Regularization -** Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.","type":"content","url":"/glossary/transformer-1#optimizers","position":5},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"type":"lvl2","url":"/glossary/transformer-1#the-transformer","position":6},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"content":"**Attention Is All You Need -** The OG introduced the idea of self-attention and the encoder/decoder architecture for language translation tasks (the encoder later got dropped because it was only used for translation). Another breakthrough from this paper was the training; “The Transformer allows for significantly more parallelisation and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.” - This fact here was what let it: overtake RNNs (which weren’t parallelisable), and lead NVIDIA to be worth more than 2.7 Trillion token credits.\n\n**Improving Language Understanding by Generative Pre-Training -** This paper introduced the “GPT” which was a breakthrough at the time. It introduced the idea of using next token prediction as a way to do self-supervised learning, which meant that we can put all of the internet into it and with a simple loss function over the vocabulary adjust the weights via back propagation.\n\n**Fast Transformer Decoding: One Write-Head is All\nYou Need -** People always point to the original Attention is all you need paper or the GPT paper that introduced the decoder only model~~, but this one was the first one that actually used it in practice. It also has very nice implementations of a transformer in python.~~  This previous explanation was incorrect as the GPT paper was released in 2018, a full year before this paper was released. The GPT paper introduced the concept of a decoder only model, but this paper coined the term “decoder only” model (I think)","type":"content","url":"/glossary/transformer-1#the-transformer","position":7},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"type":"lvl2","url":"/glossary/transformer-1#training","position":8},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"content":"These papers introduced methods to align LLMs to human preferences, and therefore allow them to be useful as chatbots/instruct models, etc.\n\n**Deep Reinforcement Learning from Human Preferences -** Reward modelling is introduced in this paper and allows for a small amount of human time to train a model that sits in as a “human proxy”. This allows for the model to train multiple orders of magnitude more human-time efficient than having a human sit there for 1,000 years judging if a simulation looks like it’s walking correctly.\n\n**Training language models to follow instructions with human feedback -** Reinforcement learning used on language models for the first time. This is what allowed for “Pre-Trained” (the P in GPT), to be useful for downstream tasks like being a chat-bot and other things.","type":"content","url":"/glossary/transformer-1#training","position":9},{"hierarchy":{"lvl1":"The Transformer"},"type":"lvl1","url":"/glossary/transformer-1","position":0},{"hierarchy":{"lvl1":"The Transformer"},"content":"📢 TLDR: Links and notes to transformer related research papers.\n\nBelow is a list of all the important research papers to fully understand the transformer architecture as introduced in the “Attention is all you need” paper by Google in 2017.\n\nThis page has a collection of research papers + notes in a directed graph to indicate dependencies between the papers and is to be used as a reference page. Obviously there’s still more to add (RNNs, LSTMs, etc), and they are on my reading list and will be added in time.%%{init: {'theme': 'base', 'themeVariables': { 'nodeTextColor': '#333333', 'mainBkg': '#f0f0f0', 'lineColor': '#F8B229'}}}%%\n\ngraph TD\n\t\ttitle[Research Collections: The Transformer]\n    subgraph \"Neural Networks: Learning\"\n        A[\"<a href='https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf'>Learning representations by back-propagating errors</a>\"]\n        B[\"<a href='https://arxiv.org/abs/1512.03385'>Deep Residual Learning</a>\"]\n        C[\"<a href='https://arxiv.org/abs/1412.6980'>Adam: A Method for Stochastic Optimization</a>\"]\n        D[\"<a href='https://arxiv.org/abs/1711.05101'>Decoupled Weight Decay Regularization</a>\"]\n    end\n\n    subgraph \"Model Components\"\n        E[\"<a href='https://arxiv.org/abs/1606.08415v5'>Gaussian Error Linear Units (GELUs)</a>\"]\n        F[\"<a href='https://arxiv.org/abs/1607.06450'>Layer Normalization</a>\"]\n    end\n\n    subgraph \"The Transformer\"\n\t\t    G[\"<a href='https://arxiv.org/abs/1706.03762v7'>Attention Is All You Need</a>\"]\n        X[\"<a href='https://arxiv.org/abs/1911.02150'>Fast Transformer Decoding: One Write-Head is All You Need</a>\"]\n        H[\"<a href='https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf'>Improving Language Understanding by Generative Pre-Training</a>\"]\n\n    end\n\n    subgraph \"Training LLMs\"\n\t\t\t\tI[\"<a href='https://arxiv.org/abs/1706.03741'>Deep reinforcement learning from human preferences</a>\"]\n        J[\"<a href='https://arxiv.org/abs/2203.02155'>Training language models to follow instructions with human feedback</a>\"]\n    end\n\n    A --> B\n    B --> G\n    C --> D\n    D --> G\n    E --> G\n    F --> G\n    G --> X\n    X --> H\n    H --> J\n    I --> J","type":"content","url":"/glossary/transformer-1","position":1},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"type":"lvl2","url":"/glossary/transformer-1#neural-networks","position":2},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"content":"Neural networks have been around for a while and these are core components of what allows neural networks and transformers to be effective at what they do.\n\n**Learning representations by back-propagating errors -** Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s. Before this paper Multi-layer-perceptrons (MLPs) weren’t very common because they were very, very difficult to train.\n\n**Deep Residual Learning for Image Recognition** - The introduction of residuals (also known as skip connections) allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which allows for gradients to flow backwards through back propagation to earlier components without needing to go through intermediate layers. This greatly speeds up convergence.\n\n**Layer Normalization -** Layernorm happens in each layer to make sure that the values don’t explode and is applied at each layers output activations. It makes the surface more regular so that it’s more symmetrical and easier to optimize over. This means that weight updates can take the same increment step in all directions and not need to worry about overstepping in one dimension but under-stepping in another, and the outputs of the activations will be more regular so that the inputs to the activation functions are all within the same order of magnitude - for example one input to a neuron being 0.00001 and another one being 1,000,000 would cause a lot of problems for floating point rounding and quantization, for example.\n\n**Gaussian Error Linear Units (GELUs) -** Activation function ****that leaves positive values unchanged but maps negative numbers to near zero. Implemented here in \n\nllm.go. Other architectures use different activation functions. For example, OpenElm uses SwiGLU FFN which I don’t exactly understand. Should probably add that to the reading list.","type":"content","url":"/glossary/transformer-1#neural-networks","position":3},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"type":"lvl2","url":"/glossary/transformer-1#optimizers","position":4},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"content":"Optimizers are the functions that control how the weights get changed during training.\n\n**Adam: A Method for Stochastic Optimization -** Introduced the Adam optimiser. Weight updates are important because it causes the training to converge more quickly. Adam has two parameters for each model parameter.\n\n**Decoupled Weight Decay Regularization -** Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.","type":"content","url":"/glossary/transformer-1#optimizers","position":5},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"type":"lvl2","url":"/glossary/transformer-1#the-transformer","position":6},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"content":"**Attention Is All You Need -** The OG introduced the idea of self-attention and the encoder/decoder architecture for language translation tasks (the encoder later got dropped because it was only used for translation). Another breakthrough from this paper was the training; “The Transformer allows for significantly more parallelisation and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.” - This fact here was what let it: overtake RNNs (which weren’t parallelisable), and lead NVIDIA to be worth more than 2.7 Trillion token credits.\n\n**Improving Language Understanding by Generative Pre-Training -** This paper introduced the “GPT” which was a breakthrough at the time. It introduced the idea of using next token prediction as a way to do self-supervised learning, which meant that we can put all of the internet into it and with a simple loss function over the vocabulary adjust the weights via back propagation.\n\n**Fast Transformer Decoding: One Write-Head is All\nYou Need -** People always point to the original Attention is all you need paper or the GPT paper that introduced the decoder only model~~, but this one was the first one that actually used it in practice. It also has very nice implementations of a transformer in python.~~  This previous explanation was incorrect as the GPT paper was released in 2018, a full year before this paper was released. The GPT paper introduced the concept of a decoder only model, but this paper coined the term “decoder only” model (I think)","type":"content","url":"/glossary/transformer-1#the-transformer","position":7},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"type":"lvl2","url":"/glossary/transformer-1#training","position":8},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"content":"These papers introduced methods to align LLMs to human preferences, and therefore allow them to be useful as chatbots/instruct models, etc.\n\n**Deep Reinforcement Learning from Human Preferences -** Reward modelling is introduced in this paper and allows for a small amount of human time to train a model that sits in as a “human proxy”. This allows for the model to train multiple orders of magnitude more human-time efficient than having a human sit there for 1,000 years judging if a simulation looks like it’s walking correctly.\n\n**Training language models to follow instructions with human feedback -** Reinforcement learning used on language models for the first time. This is what allowed for “Pre-Trained” (the P in GPT), to be useful for downstream tasks like being a chat-bot and other things.","type":"content","url":"/glossary/transformer-1#training","position":9},{"hierarchy":{"lvl1":"ML Notes"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"ML Notes"},"content":"Paper-Notes -  For papers I read and the notes I take on them. Including some of the discussions that are had as part of \n\ncognition.to\n\nResearch - For original research and experiments I do myself\n\nProjects - For a list of builder projects I’ve got, less science and more engineering based\n\nGlossary - For a glossary of ML/AI/Intelligence terms.","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"Papers"},"type":"lvl1","url":"/paper-notes","position":0},{"hierarchy":{"lvl1":"Papers"},"content":"A collection of papers with summaries and quick access links.","type":"content","url":"/paper-notes","position":1},{"hierarchy":{"lvl1":"Papers","lvl2":"Paper Notes"},"type":"lvl2","url":"/paper-notes#paper-notes","position":2},{"hierarchy":{"lvl1":"Papers","lvl2":"Paper Notes"},"content":"Title\n\nTags\n\nFull Notes\n\nThe Super Weight in Large Language Models\n\nModel internals\n\n\n\nAdaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\n\n\n\n\n\nReFT: Representation Finetuning for Language Models\n\nFine-tuning, Model representations\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n\nModel performance, Optimization, Model architecture\n\n\n\nObservation-based unit test generation at Meta\n\nAutomated testing, Software engineering\n\n\n\nGecko: Versatile Text Embeddings Distilled from Large Language Models\n\nEmbeddings, Model distillation\n\n\n\nLanguage Models of Code are Few-Shot Commonsense Learners\n\nCode models, Transfer learning\n\n\n\nQ-Sparse: All Large Language Models can be Fully Sparsely-Activated\n\nEfficiency, Model performance\n\n\n\nCodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning\n\nCode generation, Reinforcement learning\n\n\n\nPerplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models\n\nData pruning, Perplexity\n\nDetails\n\nThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\nHardware optimization, Model compression\n\n\n\nThe Curse of Recursion: Training on Generated Data Makes Models Forget\n\nModel collapse, Training data\n\n\n\nChain of thought prompting\n\nPrompting strategies\n\n\n\nLeave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\nAttention mechanisms, Context window\n\n\n\nTextGrad\n\nAgent systems, Text optimization\n\n\n\nScalable MatMul-free Language Modeling\n\nAttention mechanisms, Model efficiency\n\n\n\nWill humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?\n\nAI in software development, Future of coding\n\n\n\nScalable Extraction of Training Data from (Production) Language Models\n\nData accumulation, Model performance, Curated datasets\n\n\n\nTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n\nDataset creation, Small language models\n\n\n\nGAIA: A Benchmark for General AI Assistants\n\nAI assistants, Benchmarking\n\n\n\nScaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet\n\nFeature extraction, Interpretability\n\n\n\nMixture-of-Agents Enhances Large Language Model Capabilities\n\nModel performance, Multi-agent systems\n\n\n\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\n\nTransformers, Model architecture, Model performance\n\n\n\nLarge Language Models Understand and Can Be Enhanced by Emotional Stimuli\n\nemotional stimuli, Model behavior\n\n\n\nAutomated Unit Test Improvement using Large Language Models at Meta\n\nAutomated testing, Software engineering\n\n\n\nIs Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data\n\nData accumulation, Model collapse prevention\n\n\n\nA\\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks\n\nReinforcement learning, Search algorithms\n\n\n\nLarge Language Models: A Survey\n\nLLM capabilities, Survey\n\n\n\nA Language Model’s Guide Through Latent Space\n\nInterpretability, Latent space\n\n\n\nExtracting Latent Steering Vectors from Pretrained Language Models\n\nInterpretability, Latent space\n\n\n\nMany-shot jailbreaking\n\nJailbreaking, Model safety\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n\nTransfer learning\n\n\n\nActivation Addition: Steering Language Models Without Optimization\n\nActivation manipulation, Model steering\n\n\n\nEvaluating Large Language Models Trained on Code\n\nCode generation, Model evaluation\n\n\n\nTo Believe or Not to Believe Your LLM\n\nHallucination detection, Uncertainty quantification\n\n\n\nMixture of Agents\n\nMulti-agent systems, Prompting\n\n\n\nOpenELM: An Efficient Language Model Family with Open Training and Inference Framework\n\nEfficiency, Model architecture\n\n\n\nDeep Reinforcement Learning from Human Preferences\n\nhuman feedback, Reinforcement learning\n\n\n\nFederated Large Language Model: A Position Paper\n\nDistributed training, Federated learning\n\n\n\nMAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning\n\nMusic representation, Self-supervised learning\n\n\n\nLLaMA: Open and Efficient Foundation Language Models\n\nModel architecture, Open-source LLMs\n\n\n\nPhi1: Textbooks Are All You Need\n\nCurated datasets, Model efficiency\n\n\n\nLayer Normalization\n\nModel internals, Optimization\n\n\n\nAttention Is All You Need\n\nOG papers, Transformers\n\n\n\nExplore the Limits of Omni-modal Pretraining at Scale\n\nMulti-modal models, Pretraining\n\n\n\nGaussian Error Linear Units (GELUs)\n\nActivation functions, Model internals\n\n\n\nA Fast, Performant, Secure Distributed Training Framework For Large Language Model\n\nDistributed training, Security\n\n\n\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n\nEfficiency, Model distillation\n\n\n\nImproving Language Understanding by Generative Pre-Training\n\nOG papers, Pre-training\n\n\n\nTraining language models to follow instructions with human feedback\n\nInstruction following, Reinforcement learning\n\n\n\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n\nFine-tuning, Open-source LLMs\n\n\n\nRoFormer: Enhanced Transformer with Rotary Position Embedding\n\nEmbeddings, Model architecture\n\n\n\nSpatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence\n\nBiological Brains\n\n\n\nUnderstanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library\n\nBiological Brains\n\n","type":"content","url":"/paper-notes#paper-notes","position":3},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"},"type":"lvl1","url":"/paper-notes/perplexity-based-data-pruning","position":0},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"},"content":"","type":"content","url":"/paper-notes/perplexity-based-data-pruning","position":1},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Overview"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#overview","position":2},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Overview"},"content":"Introduces a method for pruning datasets based on perplexity measures.","type":"content","url":"/paper-notes/perplexity-based-data-pruning#overview","position":3},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Links"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#links","position":4},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Links"},"content":"Paper\n\nTags: Data pruning, Perplexity","type":"content","url":"/paper-notes/perplexity-based-data-pruning#links","position":5},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Notes"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#notes","position":6},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Notes"},"content":"This is a bunch of stuff that should be put in a note somewhere","type":"content","url":"/paper-notes/perplexity-based-data-pruning#notes","position":7},{"hierarchy":{"lvl1":"CognitionTO Community"},"type":"lvl1","url":"/projects/cognition-to","position":0},{"hierarchy":{"lvl1":"CognitionTO Community"},"content":"Website\n\nDiscord\n\nLu.ma\n\nCognitionTO is a community in Toronto dedicated to learning about the world.\n\nThroughout history, there has been a trend of technology being incomprehensibly efficient at transforming people’s lives. We often hope this transformation is positive, and the reduction of abject poverty, infant mortality, illiteracy, and wartime casualties has been a hallmark of technological progress. Yet, in the wake of this progress, we often find ourselves plagued with new classes of problems that mock us for the simplicity we once assumed of the world: financial instability, political polarization, the loneliness epidemic, and social media addiction end up exploding with the complexity that the world encapsulates today.\n\nThe goal of \n\nCognitionTO is to be a place for discussion, debate, and discovery of these complexities, with the hope that if we all know a little bit more about the world, then maybe we can work together to make it better.\n\nCurrently, \n\nCognitionTO hosts Machine Learning and AI paper readings every Monday, with plans to expand into other fields.","type":"content","url":"/projects/cognition-to","position":1},{"hierarchy":{"lvl1":"Projects"},"type":"lvl1","url":"/projects","position":0},{"hierarchy":{"lvl1":"Projects"},"content":"A collection of my projects.\n\nTitle\n\nTags\n\nDetails\n\nCognitionTO\n\nCommunity\n\nDetails","type":"content","url":"/projects","position":1}]}