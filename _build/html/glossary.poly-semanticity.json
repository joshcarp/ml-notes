{"kind":"Article","sha256":"5d39252ee82fa209d91b00e39134ac707541bfb4444541fddcc5119c74f32f08","slug":"glossary.poly-semanticity","location":"/glossary/poly_semanticity.md","dependencies":[],"frontmatter":{"title":"Poly-semanticity","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"poly_semanticity.md","url":"/build/poly_semanticity-fbb9e81b045cc5d63f6539a78ec940ca.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"NsmhTnMoqE"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"uZcLTD2VXa"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Poly-semanticity refers to the phenomenon in neural networks where individual neurons or features encode multiple distinct concepts or meanings simultaneously. This property is particularly observed in large language models and deep neural networks, where single components (neurons, attention heads, or feature dimensions) respond to or represent multiple semantic concepts, making interpretation and analysis of these networks more complex. Understanding poly-semanticity is crucial for model interpretability and optimization.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"vpQSmx2jBS"}],"key":"WLJJkMpxg8"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"k5XlHfz1Cm"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"bX4tpN1iYE"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Interpretability, Neural Networks, Semantics, Model Analysis, Representation Learning","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"rn7X3CwZst"}],"key":"dRNJTxDlQY"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"qEX4aFP52R"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"R3pcNfAsSl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Ndousse, K., Jones, C., DasSarma, N., Hernandez, D., Drain, D., Ganguli, D., Chen, Z., Hatfield-Dodds, Z., Kernion, J., Nova, T., Lovitt, L., Sellitto, M., Kundu, S., ... Kaplan, J. (2022). Transformer Circuits Thread. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"GyjTeRyv9m"},{"type":"link","url":"https://transformer-circuits.pub/","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"https://​transformer​-circuits​.pub​/","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"NMl5ikqXSM"}],"urlSource":"https://transformer-circuits.pub/","key":"CfnoTR6Vn4"}],"key":"DStskPMAci"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Cammarata, N., Goh, G., Carter, S., Petrov, M., Schubert, L., Gao, C., ... & Olah, C. (2020). Thread: Circuits. Distill. ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"uXACTL4VtD"},{"type":"cite","url":"https://doi.org/10.23915/distill.00024","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Cammarata ","key":"X4qh5VJY4H"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"c3uTMgCehM"}],"key":"eeiENZ7lGX"},{"type":"text","value":" (2020)","key":"zuk7qZmOwA"}],"kind":"narrative","label":"Cammarata_2020","identifier":"https://doi.org/10.23915/distill.00024","enumerator":"1","key":"xB9IXIvcZ8"}],"key":"zOZ4W1whb8"}],"key":"dfWARGcv42"}],"key":"RazA4r9xJi"}],"key":"viEtoEz67R"},"references":{"cite":{"order":["Cammarata_2020"],"data":{"Cammarata_2020":{"label":"Cammarata_2020","enumerator":"1","doi":"10.23915/distill.00024","html":"Cammarata, N., Carter, S., Goh, G., Olah, C., Petrov, M., & Schubert, L. (2020). Thread: Circuits. <i>Distill</i>, <i>5</i>(3). <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.23915/distill.00024\">10.23915/distill.00024</a>","url":"https://doi.org/10.23915/distill.00024"}}}},"footer":{"navigation":{"prev":{"title":"Model Collapse","url":"/glossary/model-collapse","group":"ML Notes"},"next":{"title":"Pre-Training","url":"/glossary/pre-training-1","group":"ML Notes"}}},"domain":"http://localhost:3000"}