{"kind":"Article","sha256":"64697a1afe006585cafe4eeb1430891a4103872c2b52c86d8ebc7d00c3232ad9","slug":"glossary.lora","location":"/glossary/lora.md","dependencies":[],"frontmatter":{"title":"LoRA (Low-Rank Adaptation)","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"lora.md","url":"/build/lora-d33da1ccc43e3e81883621ec2dc2c3af.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Definition","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qHk13P7kDh"}],"identifier":"definition","label":"Definition","html_id":"definition","implicit":true,"key":"sQdnIvrlOs"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"OmWnzSGT9V"}],"key":"F0rhUxiDVs"},{"type":"heading","depth":2,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Tags","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"LzzzEllTW2"}],"identifier":"tags","label":"Tags","html_id":"tags","implicit":true,"key":"DPeHIcdhxw"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"MWRLRUtGUd"}],"key":"d6iIoKTJ8V"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"References","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ewgvbigIbh"}],"identifier":"references","label":"References","html_id":"references","implicit":true,"key":"kW3taLSXGN"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"GLK9WJeHBE"},{"type":"cite","url":"https://doi.org/10.48550/arXiv.2106.09685","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Hu ","key":"rNJbAs802A"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"v72VrYnCzk"}],"key":"zDMJAoP2sB"},{"type":"text","value":" (2021)","key":"uh0L99r1QG"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2106.09685","identifier":"https://doi.org/10.48550/arXiv.2106.09685","enumerator":"1","key":"CwH1ARhYO0"}],"key":"uBxbmRE2DQ"}],"key":"Dmh9SeeyvV"}],"key":"LYZzzA6iyi"}],"key":"JVSJIw2Q0b"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2106.09685"],"data":{"https://doi.org/10.48550/arxiv.2106.09685":{"label":"https://doi.org/10.48550/arxiv.2106.09685","enumerator":"1","doi":"10.48550/ARXIV.2106.09685","html":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). <i>LoRA: Low-Rank Adaptation of Large Language Models</i>. arXiv. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2106.09685\">10.48550/ARXIV.2106.09685</a>","url":"https://doi.org/10.48550/ARXIV.2106.09685"}}}},"footer":{"navigation":{"prev":{"title":"Fine-Tuning","url":"/glossary/fine-tuning-1","group":"ML Notes"},"next":{"title":"Model Collapse","url":"/glossary/model-collapse","group":"ML Notes"}}},"domain":"http://localhost:3000"}