{"version":"1","records":[{"hierarchy":{"lvl1":"Dropout"},"type":"lvl1","url":"/glossary/dropout-1","position":0},{"hierarchy":{"lvl1":"Dropout"},"content":"","type":"content","url":"/glossary/dropout-1","position":1},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"type":"lvl2","url":"/glossary/dropout-1#definition","position":2},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/dropout-1#definition","position":3},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"type":"lvl2","url":"/glossary/dropout-1#tags","position":4},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"content":"Optimization, Training","type":"content","url":"/glossary/dropout-1#tags","position":5},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"type":"lvl2","url":"/glossary/dropout-1#references","position":6},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"content":"","type":"content","url":"/glossary/dropout-1#references","position":7},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/dropout-1#additional-notes","position":8},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"content":"Dropout is a regularization technique used in neural networks to prevent overfitting. Here’s how it works:\n\nDuring training, dropout randomly “turns off” (sets to zero) a certain percentage of neurons in a layer with each training batch. Typically, you might drop out 20-50% of neurons. When a neuron is dropped out, it doesn’t participate in forward propagation or backpropagation for that specific training batch.\n\nThink of it like forcing the network to learn with an incomplete brain each time. This has several beneficial effects:\n\nDuring inference (when actually using the model), dropout is turned off and all neurons are active. To compensate for having more active neurons than during training, the weights are typically scaled proportionally.\n\nA helpful analogy is to think of dropout like studying for an exam where you know some of your study materials will be unavailable during the test. This forces you to develop a more robust understanding rather than relying on any single source too heavily.\n\nInterestingly, this technique was partly inspired by sexual reproduction in nature, where genes are randomly combined from two parents, forcing useful genes to work well in many different combinations rather than relying on specific gene combinations.","type":"content","url":"/glossary/dropout-1#additional-notes","position":9},{"hierarchy":{"lvl1":"Dropout"},"type":"lvl1","url":"/glossary/dropout-1","position":0},{"hierarchy":{"lvl1":"Dropout"},"content":"","type":"content","url":"/glossary/dropout-1","position":1},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"type":"lvl2","url":"/glossary/dropout-1#definition","position":2},{"hierarchy":{"lvl1":"Dropout","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/dropout-1#definition","position":3},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"type":"lvl2","url":"/glossary/dropout-1#tags","position":4},{"hierarchy":{"lvl1":"Dropout","lvl2":"Tags"},"content":"Optimization, Training","type":"content","url":"/glossary/dropout-1#tags","position":5},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"type":"lvl2","url":"/glossary/dropout-1#references","position":6},{"hierarchy":{"lvl1":"Dropout","lvl2":"References"},"content":"","type":"content","url":"/glossary/dropout-1#references","position":7},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/dropout-1#additional-notes","position":8},{"hierarchy":{"lvl1":"Dropout","lvl2":"Additional Notes"},"content":"Dropout is a regularization technique used in neural networks to prevent overfitting. Here’s how it works:\n\nDuring training, dropout randomly “turns off” (sets to zero) a certain percentage of neurons in a layer with each training batch. Typically, you might drop out 20-50% of neurons. When a neuron is dropped out, it doesn’t participate in forward propagation or backpropagation for that specific training batch.\n\nThink of it like forcing the network to learn with an incomplete brain each time. This has several beneficial effects:\n\nDuring inference (when actually using the model), dropout is turned off and all neurons are active. To compensate for having more active neurons than during training, the weights are typically scaled proportionally.\n\nA helpful analogy is to think of dropout like studying for an exam where you know some of your study materials will be unavailable during the test. This forces you to develop a more robust understanding rather than relying on any single source too heavily.\n\nInterestingly, this technique was partly inspired by sexual reproduction in nature, where genes are randomly combined from two parents, forcing useful genes to work well in many different combinations rather than relying on specific gene combinations.","type":"content","url":"/glossary/dropout-1#additional-notes","position":9},{"hierarchy":{"lvl1":"Feed-Forward Network"},"type":"lvl1","url":"/glossary/feed-forward-network","position":0},{"hierarchy":{"lvl1":"Feed-Forward Network"},"content":"","type":"content","url":"/glossary/feed-forward-network","position":1},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Definition"},"type":"lvl2","url":"/glossary/feed-forward-network#definition","position":2},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Definition"},"content":"A Feed-Forward Network (FFN) is a fundamental neural network architecture where information flows in one direction, from input to output layers, through one or more hidden layers, without any cycles or loops. Each neuron in a layer is connected to all neurons in the subsequent layer, but there are no connections between neurons in the same layer or backwards connections. FFNs are also known as Multi-Layer Perceptrons (MLPs) and form basic building blocks in more complex architectures like transformers.","type":"content","url":"/glossary/feed-forward-network#definition","position":3},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Tags"},"type":"lvl2","url":"/glossary/feed-forward-network#tags","position":4},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"Tags"},"content":"Neural Networks, Architecture, Basic Concepts, MLP","type":"content","url":"/glossary/feed-forward-network#tags","position":5},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"References"},"type":"lvl2","url":"/glossary/feed-forward-network#references","position":6},{"hierarchy":{"lvl1":"Feed-Forward Network","lvl2":"References"},"content":"Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536. \n\nRumelhart et al. (1986)","type":"content","url":"/glossary/feed-forward-network#references","position":7},{"hierarchy":{"lvl1":"Fine-Tuning"},"type":"lvl1","url":"/glossary/fine-tuning-1","position":0},{"hierarchy":{"lvl1":"Fine-Tuning"},"content":"","type":"content","url":"/glossary/fine-tuning-1","position":1},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/fine-tuning-1#definition","position":2},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/fine-tuning-1#definition","position":3},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/fine-tuning-1#tags","position":4},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"content":"Fine-tuning","type":"content","url":"/glossary/fine-tuning-1#tags","position":5},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/fine-tuning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"content":"Mode seeking occurs in the “Fine-tuning” phase - Once the model has a distribution to work on, the model needs to be “fine-tuned”. Here the model will learn which parts of the distribution are useful and slightly diverge from the original distribution to make more “useful” behaviours more likely. It’s important to keep the distribution matching the original distribution to a certain extent. This can lead to model collapse where the model overfits on the fine-tuning examples and isn’t able to handle inputs that haven’t been fine-tuned because the rest of the distributions have been destroyed, negating the effect of pre-training.","type":"content","url":"/glossary/fine-tuning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Fine-Tuning"},"type":"lvl1","url":"/glossary/fine-tuning-1","position":0},{"hierarchy":{"lvl1":"Fine-Tuning"},"content":"","type":"content","url":"/glossary/fine-tuning-1","position":1},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/fine-tuning-1#definition","position":2},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/fine-tuning-1#definition","position":3},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/fine-tuning-1#tags","position":4},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Tags"},"content":"Fine-tuning","type":"content","url":"/glossary/fine-tuning-1#tags","position":5},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/fine-tuning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Fine-Tuning","lvl2":"Additional Notes"},"content":"Mode seeking occurs in the “Fine-tuning” phase - Once the model has a distribution to work on, the model needs to be “fine-tuned”. Here the model will learn which parts of the distribution are useful and slightly diverge from the original distribution to make more “useful” behaviours more likely. It’s important to keep the distribution matching the original distribution to a certain extent. This can lead to model collapse where the model overfits on the fine-tuning examples and isn’t able to handle inputs that haven’t been fine-tuned because the rest of the distributions have been destroyed, negating the effect of pre-training.","type":"content","url":"/glossary/fine-tuning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Glossary"},"type":"lvl1","url":"/glossary","position":0},{"hierarchy":{"lvl1":"Glossary"},"content":"A collection of terms and definitions used throughout the documentation.\n\nTerm\n\nTags\n\nFull Notes\n\nSparse/Dense Reward\n\nTraining\n\nDetails\n\nFine-Tuning\n\nFine-tuning\n\nDetails\n\nPre-Training\n\nPre-training\n\nDetails\n\nReinforcement Learning\n\nReinforcement learning, Training\n\nDetails\n\nDropout\n\nOptimization, Training\n\nDetails\n\nTransformer\n\nModel Architecture, Training\n\nDetails\n\nLoRA\n\nOptimization, Training, Fine-tuning, Parameter-efficient training\n\nDetails\n\nModel Collapse\n\nTraining, Failure modes, Optimization\n\nDetails\n\nFeed-Forward Network\n\nNeural Networks, Architecture, Basic Concepts\n\nDetails\n\nResidual Stream\n\nTransformers, Architecture, Deep Learning\n\nDetails\n\nRAG\n\nNatural Language Processing, Information Retrieval, Text Generation\n\nDetails\n\nPoly-semanticity\n\nInterpretability, Neural Networks, Semantics, Model Analysis\n\nDetails","type":"content","url":"/glossary","position":1},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)"},"type":"lvl1","url":"/glossary/lora","position":0},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)"},"content":"","type":"content","url":"/glossary/lora","position":1},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Definition"},"type":"lvl2","url":"/glossary/lora#definition","position":2},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Definition"},"content":"LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by representing weight updates through low-rank decomposition matrices. Instead of updating all weights in a neural network during fine-tuning, LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer, significantly reducing memory requirements while maintaining model performance.","type":"content","url":"/glossary/lora#definition","position":3},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Tags"},"type":"lvl2","url":"/glossary/lora#tags","position":4},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"Tags"},"content":"Optimization, Training, Fine-tuning, Parameter-efficient training, Model adaptation","type":"content","url":"/glossary/lora#tags","position":5},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"References"},"type":"lvl2","url":"/glossary/lora#references","position":6},{"hierarchy":{"lvl1":"LoRA (Low-Rank Adaptation)","lvl2":"References"},"content":"Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. \n\nHu et al. (2021)","type":"content","url":"/glossary/lora#references","position":7},{"hierarchy":{"lvl1":"Model Collapse"},"type":"lvl1","url":"/glossary/model-collapse","position":0},{"hierarchy":{"lvl1":"Model Collapse"},"content":"","type":"content","url":"/glossary/model-collapse","position":1},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Definition"},"type":"lvl2","url":"/glossary/model-collapse#definition","position":2},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Definition"},"content":"Model collapse refers to a failure mode in training where a model converges to a degenerate state, producing limited or uniform outputs regardless of different inputs. This phenomenon is particularly common in generative models like GANs, where the generator might produce only a small subset of possible outputs, failing to capture the full diversity of the training distribution. In language models, it can manifest as repetitive or generic responses regardless of input prompts.","type":"content","url":"/glossary/model-collapse#definition","position":3},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Tags"},"type":"lvl2","url":"/glossary/model-collapse#tags","position":4},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"Tags"},"content":"Training, Failure modes, Optimization, GANs, Model behavior","type":"content","url":"/glossary/model-collapse#tags","position":5},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"References"},"type":"lvl2","url":"/glossary/model-collapse#references","position":6},{"hierarchy":{"lvl1":"Model Collapse","lvl2":"References"},"content":"Arjovsky, M., & Bottou, L. (2017). Towards Principled Methods for Training Generative Adversarial Networks. \n\nArjovsky & Bottou (2017)\n\nSrivastava, A., Valkov, L., Russell, C., Gutmann, M. U., & Sutton, C. (2017). VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning. \n\nSrivastava et al. (2017)","type":"content","url":"/glossary/model-collapse#references","position":7},{"hierarchy":{"lvl1":"Poly-semanticity"},"type":"lvl1","url":"/glossary/poly-semanticity","position":0},{"hierarchy":{"lvl1":"Poly-semanticity"},"content":"","type":"content","url":"/glossary/poly-semanticity","position":1},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Definition"},"type":"lvl2","url":"/glossary/poly-semanticity#definition","position":2},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Definition"},"content":"Poly-semanticity refers to the phenomenon in neural networks where individual neurons or features encode multiple distinct concepts or meanings simultaneously. This property is particularly observed in large language models and deep neural networks, where single components (neurons, attention heads, or feature dimensions) respond to or represent multiple semantic concepts, making interpretation and analysis of these networks more complex. Understanding poly-semanticity is crucial for model interpretability and optimization.","type":"content","url":"/glossary/poly-semanticity#definition","position":3},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Tags"},"type":"lvl2","url":"/glossary/poly-semanticity#tags","position":4},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"Tags"},"content":"Interpretability, Neural Networks, Semantics, Model Analysis, Representation Learning","type":"content","url":"/glossary/poly-semanticity#tags","position":5},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"References"},"type":"lvl2","url":"/glossary/poly-semanticity#references","position":6},{"hierarchy":{"lvl1":"Poly-semanticity","lvl2":"References"},"content":"Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Ndousse, K., Jones, C., DasSarma, N., Hernandez, D., Drain, D., Ganguli, D., Chen, Z., Hatfield-Dodds, Z., Kernion, J., Nova, T., Lovitt, L., Sellitto, M., Kundu, S., ... Kaplan, J. (2022). Transformer Circuits Thread. \n\nhttps://​transformer​-circuits​.pub​/\n\nCammarata, N., Goh, G., Carter, S., Petrov, M., Schubert, L., Gao, C., ... & Olah, C. (2020). Thread: Circuits. Distill. \n\nCammarata et al. (2020)","type":"content","url":"/glossary/poly-semanticity#references","position":7},{"hierarchy":{"lvl1":"Pre-Training"},"type":"lvl1","url":"/glossary/pre-training-1","position":0},{"hierarchy":{"lvl1":"Pre-Training"},"content":"","type":"content","url":"/glossary/pre-training-1","position":1},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"type":"lvl2","url":"/glossary/pre-training-1#definition","position":2},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/pre-training-1#definition","position":3},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"type":"lvl2","url":"/glossary/pre-training-1#tags","position":4},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"content":"Pre-training","type":"content","url":"/glossary/pre-training-1#tags","position":5},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/pre-training-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"content":"This is the “PT” in “GPT”, and it was one of the breakthroughs in the og GPT paper\n\nDistribution matching occurs in this phase; This is where the model is trained on bulk internet data. The model learns how to predict the next token by minimising the loss between P(predicted-token|context) and P(actual-token|context).\n\nA concrete example of there this occurs is in the\n\nThis concept was introduced in the\n\nPre-training effectively cuts down on this upper limit by making it more likely that the model will predict coherent sentences, which means that it will converge on correct behaviour orders of magnitude faster.","type":"content","url":"/glossary/pre-training-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Pre-Training"},"type":"lvl1","url":"/glossary/pre-training-1","position":0},{"hierarchy":{"lvl1":"Pre-Training"},"content":"","type":"content","url":"/glossary/pre-training-1","position":1},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"type":"lvl2","url":"/glossary/pre-training-1#definition","position":2},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/pre-training-1#definition","position":3},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"type":"lvl2","url":"/glossary/pre-training-1#tags","position":4},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Tags"},"content":"Pre-training","type":"content","url":"/glossary/pre-training-1#tags","position":5},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/pre-training-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Pre-Training","lvl2":"Additional Notes"},"content":"This is the “PT” in “GPT”, and it was one of the breakthroughs in the og GPT paper\n\nDistribution matching occurs in this phase; This is where the model is trained on bulk internet data. The model learns how to predict the next token by minimising the loss between P(predicted-token|context) and P(actual-token|context).\n\nA concrete example of there this occurs is in the\n\nThis concept was introduced in the\n\nPre-training effectively cuts down on this upper limit by making it more likely that the model will predict coherent sentences, which means that it will converge on correct behaviour orders of magnitude faster.","type":"content","url":"/glossary/pre-training-1#additional-notes","position":7},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)"},"type":"lvl1","url":"/glossary/rag","position":0},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)"},"content":"","type":"content","url":"/glossary/rag","position":1},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Definition"},"type":"lvl2","url":"/glossary/rag#definition","position":2},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Definition"},"content":"RAG is a hybrid framework that combines retrieval-based and generation-based approaches for text generation. It enhances language model outputs by first retrieving relevant documents or passages from a knowledge base, then conditioning the generation process on both the input query and the retrieved information. This approach helps ground the model’s responses in specific, relevant information while maintaining the flexibility of generative models.","type":"content","url":"/glossary/rag#definition","position":3},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Tags"},"type":"lvl2","url":"/glossary/rag#tags","position":4},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"Tags"},"content":"Natural Language Processing, Information Retrieval, Text Generation, Knowledge Base","type":"content","url":"/glossary/rag#tags","position":5},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"References"},"type":"lvl2","url":"/glossary/rag#references","position":6},{"hierarchy":{"lvl1":"RAG (Retrieval-Augmented Generation)","lvl2":"References"},"content":"Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., & Kiela, D. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. \n\nLewis et al. (2020)","type":"content","url":"/glossary/rag#references","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning"},"type":"lvl1","url":"/glossary/reinforcement-learning-1","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#definition","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1#definition","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#tags","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"content":"Reinforcement learning, Training","type":"content","url":"/glossary/reinforcement-learning-1#tags","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"content":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.\n\nThink of it like teaching a dog new tricks:\n\nThe dog (agent) performs actions\n\nYou give treats or praise (rewards) for good behavior\n\nThe dog learns which actions lead to treats\n\nOver time, it figures out the best sequence of actions to get rewards\nKey Components:\n\nThe Learning Process:\n\nCommon Approaches:\n\nQ-Learning: Learns the value of actions in different states\n\nPolicy Gradient: Directly learns the best policy\n\nDeep RL: Combines deep neural networks with RL\n\nModel-Based RL: Learns a model of the environment\nReal-World Applications:\n\nGame playing (AlphaGo, OpenAI Five)\n\nRobotics and robot control\n\nResource management\n\nRecommendation systems\n\nAutonomous vehicles\n\nTrading strategies\nKey Challenges:\n\nExploration vs. Exploitation trade-off\n\nDelayed rewards (credit assignment problem)\n\nLarge state/action spaces\n\nSample efficiency\n\nStability during training\nThe power of RL lies in its ability to learn through trial and error, discovering solutions that might not be obvious to human programmers. Unlike supervised learning, which requires labeled examples, RL can learn from raw experience in the environment.","type":"content","url":"/glossary/reinforcement-learning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning"},"type":"lvl1","url":"/glossary/reinforcement-learning-1","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#definition","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/reinforcement-learning-1#definition","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#tags","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Tags"},"content":"Reinforcement learning, Training","type":"content","url":"/glossary/reinforcement-learning-1#tags","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/reinforcement-learning-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning","lvl2":"Additional Notes"},"content":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.\n\nThink of it like teaching a dog new tricks:\n\nThe dog (agent) performs actions\n\nYou give treats or praise (rewards) for good behavior\n\nThe dog learns which actions lead to treats\n\nOver time, it figures out the best sequence of actions to get rewards\nKey Components:\n\nThe Learning Process:\n\nCommon Approaches:\n\nQ-Learning: Learns the value of actions in different states\n\nPolicy Gradient: Directly learns the best policy\n\nDeep RL: Combines deep neural networks with RL\n\nModel-Based RL: Learns a model of the environment\nReal-World Applications:\n\nGame playing (AlphaGo, OpenAI Five)\n\nRobotics and robot control\n\nResource management\n\nRecommendation systems\n\nAutonomous vehicles\n\nTrading strategies\nKey Challenges:\n\nExploration vs. Exploitation trade-off\n\nDelayed rewards (credit assignment problem)\n\nLarge state/action spaces\n\nSample efficiency\n\nStability during training\nThe power of RL lies in its ability to learn through trial and error, discovering solutions that might not be obvious to human programmers. Unlike supervised learning, which requires labeled examples, RL can learn from raw experience in the environment.","type":"content","url":"/glossary/reinforcement-learning-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Residual Stream"},"type":"lvl1","url":"/glossary/residual-stream","position":0},{"hierarchy":{"lvl1":"Residual Stream"},"content":"","type":"content","url":"/glossary/residual-stream","position":1},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Definition"},"type":"lvl2","url":"/glossary/residual-stream#definition","position":2},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Definition"},"content":"The residual stream is a core concept in transformer architectures, referring to the main pathway through which information flows across layers. It maintains a persistent representation that each sublayer (self-attention and feed-forward network) modifies via residual connections. The residual stream allows for better gradient flow during training and helps maintain information from earlier layers throughout the network depth. Each sublayer processes the stream and adds its output back to it, rather than completely transforming it.","type":"content","url":"/glossary/residual-stream#definition","position":3},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Tags"},"type":"lvl2","url":"/glossary/residual-stream#tags","position":4},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"Tags"},"content":"Transformers, Architecture, Deep Learning, Residual Connections","type":"content","url":"/glossary/residual-stream#tags","position":5},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"References"},"type":"lvl2","url":"/glossary/residual-stream#references","position":6},{"hierarchy":{"lvl1":"Residual Stream","lvl2":"References"},"content":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. \n\nVaswani et al. (2017)\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. \n\nHe et al. (2015)","type":"content","url":"/glossary/residual-stream#references","position":7},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"type":"lvl1","url":"/glossary/sparsedense-reward-1","position":0},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1","position":1},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#definition","position":2},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1#definition","position":3},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#tags","position":4},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"content":"Training","type":"content","url":"/glossary/sparsedense-reward-1#tags","position":5},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"content":"A reward that occurs less frequently when compared to a dense feedback. Post-training is a lot more sparse than pre-training because reward is only given after the action can be judged.","type":"content","url":"/glossary/sparsedense-reward-1#additional-notes","position":7},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"type":"lvl1","url":"/glossary/sparsedense-reward-1","position":0},{"hierarchy":{"lvl1":"Sparse/Dense Reward"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1","position":1},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#definition","position":2},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Definition"},"content":"","type":"content","url":"/glossary/sparsedense-reward-1#definition","position":3},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#tags","position":4},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Tags"},"content":"Training","type":"content","url":"/glossary/sparsedense-reward-1#tags","position":5},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"type":"lvl2","url":"/glossary/sparsedense-reward-1#additional-notes","position":6},{"hierarchy":{"lvl1":"Sparse/Dense Reward","lvl2":"Additional Notes"},"content":"A reward that occurs less frequently when compared to a dense feedback. Post-training is a lot more sparse than pre-training because reward is only given after the action can be judged.","type":"content","url":"/glossary/sparsedense-reward-1#additional-notes","position":7},{"hierarchy":{"lvl1":"The Transformer"},"type":"lvl1","url":"/glossary/transformer-1","position":0},{"hierarchy":{"lvl1":"The Transformer"},"content":"📢 TLDR: Links and notes to transformer related research papers.\n\nBelow is a list of all the important research papers to fully understand the transformer architecture as introduced in the “Attention is all you need” paper by Google in 2017.\n\nThis page has a collection of research papers + notes in a directed graph to indicate dependencies between the papers and is to be used as a reference page. Obviously there’s still more to add (RNNs, LSTMs, etc), and they are on my reading list and will be added in time.%%{init: {'theme': 'base', 'themeVariables': { 'nodeTextColor': '#333333', 'mainBkg': '#f0f0f0', 'lineColor': '#F8B229'}}}%%\n\ngraph TD\n\t\ttitle[Research Collections: The Transformer]\n    subgraph \"Neural Networks: Learning\"\n        A[\"<a href='https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf'>Learning representations by back-propagating errors</a>\"]\n        B[\"<a href='https://arxiv.org/abs/1512.03385'>Deep Residual Learning</a>\"]\n        C[\"<a href='https://arxiv.org/abs/1412.6980'>Adam: A Method for Stochastic Optimization</a>\"]\n        D[\"<a href='https://arxiv.org/abs/1711.05101'>Decoupled Weight Decay Regularization</a>\"]\n    end\n\n    subgraph \"Model Components\"\n        E[\"<a href='https://arxiv.org/abs/1606.08415v5'>Gaussian Error Linear Units (GELUs)</a>\"]\n        F[\"<a href='https://arxiv.org/abs/1607.06450'>Layer Normalization</a>\"]\n    end\n\n    subgraph \"The Transformer\"\n\t\t    G[\"<a href='https://arxiv.org/abs/1706.03762v7'>Attention Is All You Need</a>\"]\n        X[\"<a href='https://arxiv.org/abs/1911.02150'>Fast Transformer Decoding: One Write-Head is All You Need</a>\"]\n        H[\"<a href='https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf'>Improving Language Understanding by Generative Pre-Training</a>\"]\n\n    end\n\n    subgraph \"Training LLMs\"\n\t\t\t\tI[\"<a href='https://arxiv.org/abs/1706.03741'>Deep reinforcement learning from human preferences</a>\"]\n        J[\"<a href='https://arxiv.org/abs/2203.02155'>Training language models to follow instructions with human feedback</a>\"]\n    end\n\n    A --> B\n    B --> G\n    C --> D\n    D --> G\n    E --> G\n    F --> G\n    G --> X\n    X --> H\n    H --> J\n    I --> J","type":"content","url":"/glossary/transformer-1","position":1},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"type":"lvl2","url":"/glossary/transformer-1#neural-networks","position":2},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"content":"Neural networks have been around for a while and these are core components of what allows neural networks and transformers to be effective at what they do.\n\n**Learning representations by back-propagating errors -** Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s. Before this paper Multi-layer-perceptrons (MLPs) weren’t very common because they were very, very difficult to train.\n\n**Deep Residual Learning for Image Recognition** - The introduction of residuals (also known as skip connections) allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which allows for gradients to flow backwards through back propagation to earlier components without needing to go through intermediate layers. This greatly speeds up convergence.\n\n**Layer Normalization -** Layernorm happens in each layer to make sure that the values don’t explode and is applied at each layers output activations. It makes the surface more regular so that it’s more symmetrical and easier to optimize over. This means that weight updates can take the same increment step in all directions and not need to worry about overstepping in one dimension but under-stepping in another, and the outputs of the activations will be more regular so that the inputs to the activation functions are all within the same order of magnitude - for example one input to a neuron being 0.00001 and another one being 1,000,000 would cause a lot of problems for floating point rounding and quantization, for example.\n\n**Gaussian Error Linear Units (GELUs) -** Activation function ****that leaves positive values unchanged but maps negative numbers to near zero. Implemented here in \n\nllm.go. Other architectures use different activation functions. For example, OpenElm uses SwiGLU FFN which I don’t exactly understand. Should probably add that to the reading list.","type":"content","url":"/glossary/transformer-1#neural-networks","position":3},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"type":"lvl2","url":"/glossary/transformer-1#optimizers","position":4},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"content":"Optimizers are the functions that control how the weights get changed during training.\n\n**Adam: A Method for Stochastic Optimization -** Introduced the Adam optimiser. Weight updates are important because it causes the training to converge more quickly. Adam has two parameters for each model parameter.\n\n**Decoupled Weight Decay Regularization -** Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.","type":"content","url":"/glossary/transformer-1#optimizers","position":5},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"type":"lvl2","url":"/glossary/transformer-1#the-transformer","position":6},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"content":"**Attention Is All You Need -** The OG introduced the idea of self-attention and the encoder/decoder architecture for language translation tasks (the encoder later got dropped because it was only used for translation). Another breakthrough from this paper was the training; “The Transformer allows for significantly more parallelisation and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.” - This fact here was what let it: overtake RNNs (which weren’t parallelisable), and lead NVIDIA to be worth more than 2.7 Trillion token credits.\n\n**Improving Language Understanding by Generative Pre-Training -** This paper introduced the “GPT” which was a breakthrough at the time. It introduced the idea of using next token prediction as a way to do self-supervised learning, which meant that we can put all of the internet into it and with a simple loss function over the vocabulary adjust the weights via back propagation.\n\n**Fast Transformer Decoding: One Write-Head is All\nYou Need -** People always point to the original Attention is all you need paper or the GPT paper that introduced the decoder only model~~, but this one was the first one that actually used it in practice. It also has very nice implementations of a transformer in python.~~  This previous explanation was incorrect as the GPT paper was released in 2018, a full year before this paper was released. The GPT paper introduced the concept of a decoder only model, but this paper coined the term “decoder only” model (I think)","type":"content","url":"/glossary/transformer-1#the-transformer","position":7},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"type":"lvl2","url":"/glossary/transformer-1#training","position":8},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"content":"These papers introduced methods to align LLMs to human preferences, and therefore allow them to be useful as chatbots/instruct models, etc.\n\n**Deep Reinforcement Learning from Human Preferences -** Reward modelling is introduced in this paper and allows for a small amount of human time to train a model that sits in as a “human proxy”. This allows for the model to train multiple orders of magnitude more human-time efficient than having a human sit there for 1,000 years judging if a simulation looks like it’s walking correctly.\n\n**Training language models to follow instructions with human feedback -** Reinforcement learning used on language models for the first time. This is what allowed for “Pre-Trained” (the P in GPT), to be useful for downstream tasks like being a chat-bot and other things.","type":"content","url":"/glossary/transformer-1#training","position":9},{"hierarchy":{"lvl1":"The Transformer"},"type":"lvl1","url":"/glossary/transformer-1","position":0},{"hierarchy":{"lvl1":"The Transformer"},"content":"📢 TLDR: Links and notes to transformer related research papers.\n\nBelow is a list of all the important research papers to fully understand the transformer architecture as introduced in the “Attention is all you need” paper by Google in 2017.\n\nThis page has a collection of research papers + notes in a directed graph to indicate dependencies between the papers and is to be used as a reference page. Obviously there’s still more to add (RNNs, LSTMs, etc), and they are on my reading list and will be added in time.%%{init: {'theme': 'base', 'themeVariables': { 'nodeTextColor': '#333333', 'mainBkg': '#f0f0f0', 'lineColor': '#F8B229'}}}%%\n\ngraph TD\n\t\ttitle[Research Collections: The Transformer]\n    subgraph \"Neural Networks: Learning\"\n        A[\"<a href='https://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf'>Learning representations by back-propagating errors</a>\"]\n        B[\"<a href='https://arxiv.org/abs/1512.03385'>Deep Residual Learning</a>\"]\n        C[\"<a href='https://arxiv.org/abs/1412.6980'>Adam: A Method for Stochastic Optimization</a>\"]\n        D[\"<a href='https://arxiv.org/abs/1711.05101'>Decoupled Weight Decay Regularization</a>\"]\n    end\n\n    subgraph \"Model Components\"\n        E[\"<a href='https://arxiv.org/abs/1606.08415v5'>Gaussian Error Linear Units (GELUs)</a>\"]\n        F[\"<a href='https://arxiv.org/abs/1607.06450'>Layer Normalization</a>\"]\n    end\n\n    subgraph \"The Transformer\"\n\t\t    G[\"<a href='https://arxiv.org/abs/1706.03762v7'>Attention Is All You Need</a>\"]\n        X[\"<a href='https://arxiv.org/abs/1911.02150'>Fast Transformer Decoding: One Write-Head is All You Need</a>\"]\n        H[\"<a href='https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf'>Improving Language Understanding by Generative Pre-Training</a>\"]\n\n    end\n\n    subgraph \"Training LLMs\"\n\t\t\t\tI[\"<a href='https://arxiv.org/abs/1706.03741'>Deep reinforcement learning from human preferences</a>\"]\n        J[\"<a href='https://arxiv.org/abs/2203.02155'>Training language models to follow instructions with human feedback</a>\"]\n    end\n\n    A --> B\n    B --> G\n    C --> D\n    D --> G\n    E --> G\n    F --> G\n    G --> X\n    X --> H\n    H --> J\n    I --> J","type":"content","url":"/glossary/transformer-1","position":1},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"type":"lvl2","url":"/glossary/transformer-1#neural-networks","position":2},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Neural Networks"},"content":"Neural networks have been around for a while and these are core components of what allows neural networks and transformers to be effective at what they do.\n\n**Learning representations by back-propagating errors -** Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s. Before this paper Multi-layer-perceptrons (MLPs) weren’t very common because they were very, very difficult to train.\n\n**Deep Residual Learning for Image Recognition** - The introduction of residuals (also known as skip connections) allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which allows for gradients to flow backwards through back propagation to earlier components without needing to go through intermediate layers. This greatly speeds up convergence.\n\n**Layer Normalization -** Layernorm happens in each layer to make sure that the values don’t explode and is applied at each layers output activations. It makes the surface more regular so that it’s more symmetrical and easier to optimize over. This means that weight updates can take the same increment step in all directions and not need to worry about overstepping in one dimension but under-stepping in another, and the outputs of the activations will be more regular so that the inputs to the activation functions are all within the same order of magnitude - for example one input to a neuron being 0.00001 and another one being 1,000,000 would cause a lot of problems for floating point rounding and quantization, for example.\n\n**Gaussian Error Linear Units (GELUs) -** Activation function ****that leaves positive values unchanged but maps negative numbers to near zero. Implemented here in \n\nllm.go. Other architectures use different activation functions. For example, OpenElm uses SwiGLU FFN which I don’t exactly understand. Should probably add that to the reading list.","type":"content","url":"/glossary/transformer-1#neural-networks","position":3},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"type":"lvl2","url":"/glossary/transformer-1#optimizers","position":4},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Optimizers"},"content":"Optimizers are the functions that control how the weights get changed during training.\n\n**Adam: A Method for Stochastic Optimization -** Introduced the Adam optimiser. Weight updates are important because it causes the training to converge more quickly. Adam has two parameters for each model parameter.\n\n**Decoupled Weight Decay Regularization -** Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.","type":"content","url":"/glossary/transformer-1#optimizers","position":5},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"type":"lvl2","url":"/glossary/transformer-1#the-transformer","position":6},{"hierarchy":{"lvl1":"The Transformer","lvl2":"The Transformer"},"content":"**Attention Is All You Need -** The OG introduced the idea of self-attention and the encoder/decoder architecture for language translation tasks (the encoder later got dropped because it was only used for translation). Another breakthrough from this paper was the training; “The Transformer allows for significantly more parallelisation and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.” - This fact here was what let it: overtake RNNs (which weren’t parallelisable), and lead NVIDIA to be worth more than 2.7 Trillion token credits.\n\n**Improving Language Understanding by Generative Pre-Training -** This paper introduced the “GPT” which was a breakthrough at the time. It introduced the idea of using next token prediction as a way to do self-supervised learning, which meant that we can put all of the internet into it and with a simple loss function over the vocabulary adjust the weights via back propagation.\n\n**Fast Transformer Decoding: One Write-Head is All\nYou Need -** People always point to the original Attention is all you need paper or the GPT paper that introduced the decoder only model~~, but this one was the first one that actually used it in practice. It also has very nice implementations of a transformer in python.~~  This previous explanation was incorrect as the GPT paper was released in 2018, a full year before this paper was released. The GPT paper introduced the concept of a decoder only model, but this paper coined the term “decoder only” model (I think)","type":"content","url":"/glossary/transformer-1#the-transformer","position":7},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"type":"lvl2","url":"/glossary/transformer-1#training","position":8},{"hierarchy":{"lvl1":"The Transformer","lvl2":"Training"},"content":"These papers introduced methods to align LLMs to human preferences, and therefore allow them to be useful as chatbots/instruct models, etc.\n\n**Deep Reinforcement Learning from Human Preferences -** Reward modelling is introduced in this paper and allows for a small amount of human time to train a model that sits in as a “human proxy”. This allows for the model to train multiple orders of magnitude more human-time efficient than having a human sit there for 1,000 years judging if a simulation looks like it’s walking correctly.\n\n**Training language models to follow instructions with human feedback -** Reinforcement learning used on language models for the first time. This is what allowed for “Pre-Trained” (the P in GPT), to be useful for downstream tasks like being a chat-bot and other things.","type":"content","url":"/glossary/transformer-1#training","position":9},{"hierarchy":{"lvl1":"ML Notes"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"ML Notes"},"content":"Paper-Notes -  For papers I read and the notes I take on them. Including some of the discussions that are had as part of \n\ncognition.to\n\nResearch - For original research and experiments I do myself\n\nProjects - For a list of builder projects I’ve got, less science and more engineering based\n\nGlossary - For a glossary of ML/AI/Intelligence terms.","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"Papers"},"type":"lvl1","url":"/paper-notes","position":0},{"hierarchy":{"lvl1":"Papers"},"content":"A collection of papers with summaries and quick access links.","type":"content","url":"/paper-notes","position":1},{"hierarchy":{"lvl1":"Papers","lvl2":"Paper Notes"},"type":"lvl2","url":"/paper-notes#paper-notes","position":2},{"hierarchy":{"lvl1":"Papers","lvl2":"Paper Notes"},"content":"Title\n\nTags\n\nFull Notes\n\nThe Super Weight in Large Language Models\n\nModel internals\n\n\n\nAdaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\n\n\n\n\n\nReFT: Representation Finetuning for Language Models\n\nFine-tuning, Model representations\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n\nModel performance, Optimization, Model architecture\n\n\n\nObservation-based unit test generation at Meta\n\nAutomated testing, Software engineering\n\n\n\nGecko: Versatile Text Embeddings Distilled from Large Language Models\n\nEmbeddings, Model distillation\n\n\n\nLanguage Models of Code are Few-Shot Commonsense Learners\n\nCode models, Transfer learning\n\n\n\nQ-Sparse: All Large Language Models can be Fully Sparsely-Activated\n\nEfficiency, Model performance\n\n\n\nCodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning\n\nCode generation, Reinforcement learning\n\n\n\nPerplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models\n\nData pruning, Perplexity\n\nDetails\n\nThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\nHardware optimization, Model compression\n\n\n\nThe Curse of Recursion: Training on Generated Data Makes Models Forget\n\nModel collapse, Training data\n\n\n\nChain of thought prompting\n\nPrompting strategies\n\n\n\nLeave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\nAttention mechanisms, Context window\n\n\n\nTextGrad\n\nAgent systems, Text optimization\n\n\n\nScalable MatMul-free Language Modeling\n\nAttention mechanisms, Model efficiency\n\n\n\nWill humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?\n\nAI in software development, Future of coding\n\n\n\nScalable Extraction of Training Data from (Production) Language Models\n\nData accumulation, Model performance, Curated datasets\n\n\n\nTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n\nDataset creation, Small language models\n\n\n\nGAIA: A Benchmark for General AI Assistants\n\nAI assistants, Benchmarking\n\n\n\nScaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet\n\nFeature extraction, Interpretability\n\n\n\nMixture-of-Agents Enhances Large Language Model Capabilities\n\nModel performance, Multi-agent systems\n\n\n\nHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\n\nTransformers, Model architecture, Model performance\n\n\n\nLarge Language Models Understand and Can Be Enhanced by Emotional Stimuli\n\nemotional stimuli, Model behavior\n\n\n\nAutomated Unit Test Improvement using Large Language Models at Meta\n\nAutomated testing, Software engineering\n\n\n\nIs Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data\n\nData accumulation, Model collapse prevention\n\n\n\nA\\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks\n\nReinforcement learning, Search algorithms\n\n\n\nLarge Language Models: A Survey\n\nLLM capabilities, Survey\n\n\n\nA Language Model’s Guide Through Latent Space\n\nInterpretability, Latent space\n\n\n\nExtracting Latent Steering Vectors from Pretrained Language Models\n\nInterpretability, Latent space\n\n\n\nMany-shot jailbreaking\n\nJailbreaking, Model safety\n\n\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n\nTransfer learning\n\n\n\nActivation Addition: Steering Language Models Without Optimization\n\nActivation manipulation, Model steering\n\n\n\nEvaluating Large Language Models Trained on Code\n\nCode generation, Model evaluation\n\n\n\nTo Believe or Not to Believe Your LLM\n\nHallucination detection, Uncertainty quantification\n\n\n\nMixture of Agents\n\nMulti-agent systems, Prompting\n\n\n\nOpenELM: An Efficient Language Model Family with Open Training and Inference Framework\n\nEfficiency, Model architecture\n\n\n\nDeep Reinforcement Learning from Human Preferences\n\nhuman feedback, Reinforcement learning\n\n\n\nFederated Large Language Model: A Position Paper\n\nDistributed training, Federated learning\n\n\n\nMAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning\n\nMusic representation, Self-supervised learning\n\n\n\nLLaMA: Open and Efficient Foundation Language Models\n\nModel architecture, Open-source LLMs\n\n\n\nPhi1: Textbooks Are All You Need\n\nCurated datasets, Model efficiency\n\n\n\nLayer Normalization\n\nModel internals, Optimization\n\n\n\nAttention Is All You Need\n\nOG papers, Transformers\n\n\n\nExplore the Limits of Omni-modal Pretraining at Scale\n\nMulti-modal models, Pretraining\n\n\n\nGaussian Error Linear Units (GELUs)\n\nActivation functions, Model internals\n\n\n\nA Fast, Performant, Secure Distributed Training Framework For Large Language Model\n\nDistributed training, Security\n\n\n\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n\nEfficiency, Model distillation\n\n\n\nImproving Language Understanding by Generative Pre-Training\n\nOG papers, Pre-training\n\n\n\nTraining language models to follow instructions with human feedback\n\nInstruction following, Reinforcement learning\n\n\n\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n\nFine-tuning, Open-source LLMs\n\n\n\nRoFormer: Enhanced Transformer with Rotary Position Embedding\n\nEmbeddings, Model architecture\n\n\n\nSpatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence\n\nBiological Brains\n\n\n\nUnderstanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library\n\nBiological Brains\n\n","type":"content","url":"/paper-notes#paper-notes","position":3},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"},"type":"lvl1","url":"/paper-notes/perplexity-based-data-pruning","position":0},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"},"content":"","type":"content","url":"/paper-notes/perplexity-based-data-pruning","position":1},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Overview"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#overview","position":2},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Overview"},"content":"Introduces a method for pruning datasets based on perplexity measures.","type":"content","url":"/paper-notes/perplexity-based-data-pruning#overview","position":3},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Links"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#links","position":4},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Links"},"content":"Paper\n\nTags: Data pruning, Perplexity","type":"content","url":"/paper-notes/perplexity-based-data-pruning#links","position":5},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Notes"},"type":"lvl2","url":"/paper-notes/perplexity-based-data-pruning#notes","position":6},{"hierarchy":{"lvl1":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","lvl2":"Notes"},"content":"This is a bunch of stuff that should be put in a note somewhere","type":"content","url":"/paper-notes/perplexity-based-data-pruning#notes","position":7},{"hierarchy":{"lvl1":"CognitionTO Community"},"type":"lvl1","url":"/projects/cognition-to","position":0},{"hierarchy":{"lvl1":"CognitionTO Community"},"content":"Website\n\nDiscord\n\nLu.ma\n\nCognitionTO is a community in Toronto dedicated to learning about the world.\n\nThroughout history, there has been a trend of technology being incomprehensibly efficient at transforming people’s lives. We often hope this transformation is positive, and the reduction of abject poverty, infant mortality, illiteracy, and wartime casualties has been a hallmark of technological progress. Yet, in the wake of this progress, we often find ourselves plagued with new classes of problems that mock us for the simplicity we once assumed of the world: financial instability, political polarization, the loneliness epidemic, and social media addiction end up exploding with the complexity that the world encapsulates today.\n\nThe goal of \n\nCognitionTO is to be a place for discussion, debate, and discovery of these complexities, with the hope that if we all know a little bit more about the world, then maybe we can work together to make it better.\n\nCurrently, \n\nCognitionTO hosts Machine Learning and AI paper readings every Monday, with plans to expand into other fields.","type":"content","url":"/projects/cognition-to","position":1},{"hierarchy":{"lvl1":"Projects"},"type":"lvl1","url":"/projects","position":0},{"hierarchy":{"lvl1":"Projects"},"content":"A collection of my projects.\n\nTitle\n\nTags\n\nDetails\n\nCognitionTO\n\nCommunity\n\nDetails\n\nThe Interactive Transfomer\n\nTutorial\n\nDetails","type":"content","url":"/projects","position":1},{"hierarchy":{"lvl1":"The Interactive Transformer"},"type":"lvl1","url":"/projects/the-interactive-transformer/interactive-transformer","position":0},{"hierarchy":{"lvl1":"The Interactive Transformer"},"content":"","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer","position":1},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Introduction"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#introduction","position":2},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Introduction"},"content":"Welcome. This is an implementation of a transformer in pure go with no third party libraries. This way, everything from tensor operations to tokenization are all done inside this notebook.\n\nBecause the goal of this project is illustrative, there are no optimisations. Everything runs on a single goroutine and doesn’t have any parallelism. This makes it easy to follow and good as a reference guide.\n\nThis page is also heavily referenced. The goal is to have everything have a reference to its original paper or reference implementation.\n\nNote\n\nThis notebook is a version of my repo \n\ngithub​.com​/joshcarp​/llm​.go which in itself is a fork of \n\ngithub​.com​/karpathy​/llm​.c.\n\n\n\nGPT introduction\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#introduction","position":3},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"References"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#references","position":4},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"References"},"content":"Usually this goes at the bottom, but seeing this entire thing is a reference, it’ll go at the top instead.\n\nIEEE-754 - This is binary floating point. The specification gives good details about what types of errors that can accumulate which would impact training and interence, as well as possible optimisations that can be used, like fused-multiply-add (FMA), which can reduce intermediate errors. CPUs usually use FP32, GPUs FP16.\n\nBLAS - Speed up matrix multiplication on CPUs.\n\nLayer Normalization - Layernorm was introduced in this paper.\n\nDeep Residual Learning for Image Recognition - The introduction of residuals allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which limits how much the neural networks can influence.\n\nGaussian Error Linear Units (GELUs) - Activation function that leaves positive values unchanged but maps negative numbers to near zero. Other architectures use different activation functions. For example, OpenElm uses SwiGLU.\n\nLearning representations by back-propagating errors - Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s.\n\nAdam: A Method for Stochastic Optimization - Introduced the Adam optimiser.\n\nDECOUPLED WEIGHT DECAY REGULARIZATION - Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.\n\nAdam: A Method for Stochastic Optimization - Introduced the Adam optimiser.\n\nDECOUPLED WEIGHT DECAY REGULARIZATION - Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.\n\nFast Transformer Decoding: One Write-Head is All\nYou Need - People always point to the original Attention is all you need paper or the GPT paper that introduced the decoder only model, but this one was the first one that actually used it in practice.\n\nLanguage Models are Unsupervised Multitask Learners - This is the GPT-2 paper\n\nImproving Language Understanding by Generative Pre-Training -** This paper introduced the “GPT” which was a breakthrough at the time. It introduced the idea of using next token prediction as a way to do self-supervised learning, which meant that we can put all of the internet into it and with a simple loss function over the vocabulary adjust the weights via backpropagation.\n\nAttention Is All You Need - The OG introduced the idea of self-attention and the encoder/decoder architecture for language translation tasks (the encoder later got dropped because it was only used for translation). Another breakthrough from this paper was the training; “The Transformer allows for significantly more parallelisation and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.” - This fact here was what let it: overtake RNNs (which weren’t parallelisable), and lead NVIDIA to be worth more than 2.7 Trillion token credits.\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#references","position":5},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Table of contents"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#table-of-contents","position":6},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Table of contents"},"content":"Background\n\nData types and math operations\n\nTensors\n\nMath\n\nMatrix Multiplication\n\nGPT\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#table-of-contents","position":7},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Background"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#background","position":8},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Background"},"content":"Before we can dive into the transformer, we need to cover the basics:\n\nDatatypes\n\nMatricies + Tensors\n\nMatrix multiplication\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#background","position":9},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Datatypes and Math"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#datatypes-and-math","position":10},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Datatypes and Math"},"content":"Because we’re GPU poor, and because it makes the implementation easier, we use float32 for all parameters and calculations.\n\nCPUs can either do calculations in 32 or 64 bits, but the go standard library is opinionated and only supports 64 bit math operations. This wraps all the math functions we need. Whilst all modern architectures have instructions for both float32 and float64 operations, float32 is still faster because it uses 1/2 the bits, so the throughput can be 2x the float64 (citation needed). This is an obvious optimisation for this implementation.\n\nBecause graphics applications aren’t needed to be precise, GPUs often use IEEE 754 half precision which is 16 bits, the training loss from switching from 32 -> 16 bits is negligible. (citation needed)","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#datatypes-and-math","position":11},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Datatypes and Math"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers","position":12},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Datatypes and Math"},"content":"IEEE-754 - I’ve linked the Wikipedia because you need to pay for the standard.\n\nimport \"math\"\n\nfunc Abs(x float32) float32 {\n\tif x > 0 {\n\t\treturn x\n\t}\n\treturn -x\n}\n\nfunc Cosh(x float32) float32 {\n\treturn float32(math.Cosh(float64(x)))\n}\n\nfunc Exp(x float32) float32 {\n\treturn float32(math.Exp(float64(x)))\n}\n\nfunc Inf(sign int) float32 {\n\treturn float32(math.Inf(sign))\n}\n\nfunc Log(x float32) float32 {\n\treturn float32(math.Log(float64(x)))\n}\n\nfunc IsNaN(f float32) bool {\n\treturn math.IsNaN(float64(f))\n}\n\nfunc Pow(x, y float32) float32 {\n\treturn float32(math.Pow(float64(x), float64(y)))\n}\n\nfunc Sqrt(x float32) float32 {\n\treturn float32(math.Sqrt(float64(x)))\n}\n\nfunc Tanh(x float32) float32 {\n\treturn float32(math.Tanh(float64(x)))\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers","position":13},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Tensors"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#tensors","position":14},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Tensors"},"content":"What is a tensor?\nA tensor is a multi-dimensional array. A regular slice is one-dimensional, holding elements in a sequence. A tensor can have multiple dimensions, like a 2D array (grid) or even a 3D array (cube).\n\nTensor libraries like pytorch or tensorflow exist in python. The most widely used tensor library for local inference is \n\nhttps://ggml.ai/ which powers \n\nllama.cpp.\n\n\n\nWhat is a tensor\n\n\ntype tensor struct {\n\tdata []float32\n\tdims []int\n    stride []int\n}\n\nfunc (t tensor) Data() []float32 {\n\treturn t.data\n}\n\nfunc newTensor(data []float32, dims ...int) (tensor, int) {\n\ts := 1\n\tfor _, d := range dims {\n\t\ts *= d\n\t}\n\tif s > len(data) {\n\t\tpanic(\"dimensions larger than supplied data\")\n\t}\n\tss := min(s, len(data))\n\treturn tensor{\n\t\tdata: data[:ss],\n\t\tdims: dims,\n\t}, ss\n}\n\nfunc (t tensor) size() int {\n\tsize := 1\n\tfor _, dim := range t.dims {\n\t\tsize *= dim\n\t}\n\treturn size\n}\n\nfunc (t tensor) index(idx ...int) tensor {\n\t// 1. Error Handling (Partially Adjusted)\n\tif len(idx) > len(t.dims) {\n\t\tpanic(\"Too many indices for tensor dimensions\")\n\t}\n\tfor i, dim := range idx {\n\t\tif dim < 0 || dim >= t.dims[i] {\n\t\t\tpanic(\"Index out of bounds\")\n\t\t}\n\t}\n\t// 2. Calculate Linear Index (Partially Adjusted)\n\tlinearIndex := idx[0]\n\tstride := t.size()\n\tfor i := 1; i < len(idx); i++ {\n\t\tstride /= t.dims[i]\n\t\tlinearIndex += idx[i] * stride\n\t}\n\t// 3. Adjust Dimensions and Return Sub-Tensor\n\tnewDims := t.dims[len(idx):]                  // Keep remaining dimensions\n\tend := linearIndex + t.subTensorSize(newDims) // Size based on remaining dimensions\n\n\treturn tensor{\n\t\tdata: t.data[linearIndex:end],\n\t\tdims: newDims,\n\t}\n}\n\n// Helper function to calculate the size of a sub-tensor\nfunc (t tensor) subTensorSize(idx []int) int {\n\tsubTensorSize := 1\n\tfor _, dim := range t.dims[len(idx):] {\n\t\tsubTensorSize *= dim\n\t}\n\treturn subTensorSize\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#tensors","position":15},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Matrix Multiplication"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#matrix-multiplication","position":16},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Matrix Multiplication"},"content":"matmulForward performs matrix multiplication and adds bias.\nParameters:\n\nout: output matrix\n\ninp: input matrix\n\nweight: weight matrix\n\nbias: bias vector\n\nB: batch size\n\nT: sequence length (number of time steps)\n\nC: input dimension (number of features)\n\nOC: number of output channels\n\nMost of the time spent in inference is in this function. Because we’re only doing this on a CPU this implemenation is very, very slow, and this is where different implementations would use a GPU/CUDA/Metal implementation to do parallel computation.\n\nOn CPU, many architectures have an optimisation called Basic Linear Algebra Subprograms \n\nBLAS. This allows for tiling (breaking matricies into smaller pieces and processing) or Single Instruction Multiple Data (SIMD).\n\n\n\nMatrix multiplication\n\nfunc matmulForward(out, inp, weight, bias []float32, B, T, C, OC int) {\n\t// Iterate over each batch\n\tvar wg sync.WaitGroup\n\tfor b := 0; b < B; b++ {\n\t\t// Iterate over each time step in the sequence\n\t\tfor t := 0; t < T; t++ {\n\t\t\twg.Add(1)\n\t\t\tgo func(b, t int) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\t// Calculate the index in the output slice\n\t\t\t\tinp_bt := inp[b*T*C+t*C:]\n\t\t\t\tout_bt := out[b*T*OC+t*OC:]\n\t\t\t\tfor o := 0; o < OC; o++ {\n\t\t\t\t\tvar val float32\n\t\t\t\t\tif bias != nil {\n\t\t\t\t\t\tval = bias[o]\n\t\t\t\t\t}\n\t\t\t\t\t// Calculate the index in the weight slice\n\t\t\t\t\twrow := weight[o*C:]\n\t\t\t\t\t// Perform the dot product between the input and weight row\n\t\t\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t\t\tval += inp_bt[i] * wrow[i]\n\t\t\t\t\t}\n\t\t\t\t\t// Store the output value in the output slice\n\t\t\t\t\tout_bt[o] = val\n\t\t\t\t}\n\t\t\t}(b, t)\n\t\t}\n\t}\n\twg.Wait()\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#matrix-multiplication","position":17},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"GPT"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#gpt","position":18},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"GPT"},"content":"\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#gpt","position":19},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Table of contents"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#table-of-contents-1","position":20},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Table of contents"},"content":"","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#table-of-contents-1","position":21},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Parameters vs Activations","lvl2":"Table of contents"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#parameters-vs-activations","position":22},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Parameters vs Activations","lvl2":"Table of contents"},"content":"Parameters - The bulk of what makes up “the model”. Most of the bytes you download comes from this part.\n\nActivations - Output of mathematical operations between the input and the parameters.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#parameters-vs-activations","position":23},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Forward pass","lvl2":"Table of contents"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#forward-pass","position":24},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Forward pass","lvl2":"Table of contents"},"content":"A forward pass is the “inference” stage - this section is what’s occuring when you talk with ChatGPT.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#forward-pass","position":25},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Preparing","lvl3":"Forward pass","lvl2":"Table of contents"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#preparing","position":26},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Preparing","lvl3":"Forward pass","lvl2":"Table of contents"},"content":"This section transforms text into a vector representation that can be processed by a neural network.\n\nTokenizer - Converts text to numeric ids that can be processed.\n\nData Loading - This section describes how data is loaded, including batching, tokenization, and offsetting.\n\nEmbedding - Converts these ids into n dimensional vector space","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#preparing","position":27},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"N-Layers","lvl3":"Forward pass","lvl2":"Table of contents"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#n-layers","position":28},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"N-Layers","lvl3":"Forward pass","lvl2":"Table of contents"},"content":"This section is repeated for every layer. GPT-2 has 12 layers.\n\nMasked Multi-Head Attention - Allows all tokens in the context window to impact other tokens in the context window\n\nAdd and Norm - Adds residual stream and normalises outputs\n\nFeed Forward - Feed forward is a standard MLP. Allows for more complex connections to be formed than just the attention mechanism alone.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#n-layers","position":29},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Final transformations","lvl3":"Forward pass","lvl2":"Table of contents"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#final-transformations","position":30},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Final transformations","lvl3":"Forward pass","lvl2":"Table of contents"},"content":"This section takes the higher dimensionality representations of our activations and processes it to give us our final output\n\nLinear - Transformation that reduces dimensionality into “logits” which are correlated to how likely each token is (-inf==never, +inf=100% certainty)\n\nSoftmax - This takes the logits and creates a probability distribution that adds up to 100%\n\nSampling - This samples the probability distribution and returns the single token that’s needed to make the next prediction","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#final-transformations","position":31},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"A complete forward pass","lvl3":"Forward pass","lvl2":"Table of contents"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#a-complete-forward-pass","position":32},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"A complete forward pass","lvl3":"Forward pass","lvl2":"Table of contents"},"content":"This section puts all of this together.\n\nForward","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#a-complete-forward-pass","position":33},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Backwards pass","lvl2":"Table of contents"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#backwards-pass","position":34},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Backwards pass","lvl2":"Table of contents"},"content":"This is “training”. Companies spend billions of dollars optimizing to make this as fast as possible.\n\nBackward Pass - Calculating the gradients for gradient descent + backprop.\n\nOptimizer - Determines how fast the model learns\n\nTraining our model - Let’s train gpt.\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#backwards-pass","position":35},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Data loading"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#data-loading","position":36},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Data loading"},"content":"\n\nimport (\n\t\"bytes\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"io\"\n)\n\nconst Int32ByteLen = 4\n\ntype DataLoader struct {\n\tfilename        string\n\tbatchSize       int\n\tseqLength       int\n\tcurrentPosition int64\n\tfileSize        int64\n\tNumBatches      int\n\tdata            []int32\n\tdataAll         []int32\n}\n\nfunc NewDataLoader(filename string, batchSize, seqLength int) (*DataLoader, error) {\n\tfile, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newDataLoader(file, batchSize, seqLength)\n}\n\nfunc newDataLoader(file io.Reader, batchSize, seqLength int) (*DataLoader, error) {\n\tdata, err := io.ReadAll(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsize := len(data)\n\tif size < (batchSize*seqLength+1)*Int32ByteLen {\n\t\treturn nil, errors.New(\"error: file size is too small for the batch size and sequence length\")\n\t}\n\tloader := &DataLoader{\n\t\tbatchSize:  batchSize,\n\t\tseqLength:  seqLength,\n\t\tNumBatches: size / (batchSize * seqLength * Int32ByteLen),\n\t\tdata:       make([]int32, size/Int32ByteLen),\n\t\tfileSize:   int64(size / Int32ByteLen),\n\t}\n\tif err := binary.Read(bytes.NewReader(data), binary.LittleEndian, loader.data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn loader, nil\n}\n\nfunc newDataLoaderFromInts(data []int32, batchSize, seqLength int) (*DataLoader, error) {\n\tsize := len(data)\n\tif size < (batchSize*seqLength + 1) {\n\t\treturn nil, errors.New(\"error: file size is too small for the batch size and sequence length\")\n\t}\n\tloader := &DataLoader{\n\t\tbatchSize:  batchSize,\n\t\tseqLength:  seqLength,\n\t\tNumBatches: size / (batchSize * seqLength),\n\t\tdata:       data,\n\t\tfileSize:   int64(size),\n\t}\n\treturn loader, nil\n}\n\nfunc (loader *DataLoader) Reset() {\n\tloader.currentPosition = 0\n}\n\nfunc (loader *DataLoader) NextBatch() ([]int32, []int32, error) {\n\tnextPos := loader.currentPosition + int64(loader.batchSize*loader.seqLength)\n\tif nextPos+1 > loader.fileSize {\n\t\tloader.Reset()\n\t\tnextPos = loader.currentPosition + int64(loader.batchSize*loader.seqLength)\n\t}\n\t// don't  x4 because we're indexing int32 not byte\n\tinputs := loader.data[loader.currentPosition:nextPos]\n\ttargets := loader.data[loader.currentPosition+1 : nextPos+1]\n\tloader.currentPosition = nextPos\n\treturn inputs, targets, nil\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#data-loading","position":37},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Parameters "},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#parameters","position":38},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Parameters "},"content":"A Parameter is a numerical value that determines the strength of the connection between neurons. These connections are similar to synapses in the human brain, and the parameters are like the knobs that adjust the strength of those connections.\n\nThere are two main types of parameters in neural networks:\n\nWeights: These are associated with each connection between neurons. They multiply the signal coming from one neuron before it’s passed on to the next neuron. A higher weight means a stronger connection and a greater influence on the receiving neuron.\n\nBiases: These are added to the sum of the weighted inputs at each neuron. They act like a baseline shift, allowing the neuron to activate even if the weighted inputs are weak.\n\nback to top\n\n// ParameterTensors are the parameters of the model\ntype ParameterTensors struct {\n\tMemory        []float32\n\tWordTokEmbed  tensor // (V, C) - Word/Token Embedding weights (Vocabulary size, Embedding dimension)\n\tWordPosEmbed  tensor // (maxT, C) - Positional Embedding weights (Maximum Sequence length, Embedding dimension)\n\tLayerNorm1W   tensor // (L, C) - Weights for Layer Normalization 1 (Number of layers, Embedding dimension)\n\tLayerNorm1B   tensor // (L, C) - Biases for Layer Normalization 1\n\tQueryKeyValW  tensor // (L, 3*C, C) - Attention QKV weights (Layers, 3 * Embedding dimension, Embedding dimension)\n\tQueryKeyValB  tensor // (L, 3*C) - Attention QKV biases\n\tAttProjW      tensor // (L, C, C) - Attention projection weights (Layers, Embedding dimension, Embedding dimension)\n\tAttProjB      tensor // (L, C) - Attention projection biases\n\tLayer2NormW   tensor // (L, C) - Weights for Layer Normalization 2\n\tLayer2NormB   tensor // (L, C) - Biases for Layer Normalization 2\n\tFeedFwdW      tensor // (L, 4*C, C) - Feed-forward layer weights (Layers, 4 * Embedding Dimension, Embedding Dimension)\n\tFeedFwdB      tensor // (L, 4*C) - Feed-forward layer biases\n\tFeedFwdProjW  tensor // (L, C, 4*C) - Feed-forward projection weights\n\tFeedFwdProjB  tensor // (L, C)- Feed-forward projection biases\n\tLayerFinNormW tensor // (C) - Final layer normalization weights\n\tLayerFinNormB tensor // (C) - Final layer normalization biases\n}\n\nfunc newParameterTensors(V, C, maxSeqLen, L int) ParameterTensors {\n\tvar tensor ParameterTensors\n\ttensor.Init(V, C, maxSeqLen, L)\n\treturn tensor\n}\n\nfunc (tensor *ParameterTensors) Len() int {\n\treturn len(tensor.Memory)\n}\n\n// Init initialises the ParameterTensors with specific sizes for each tensor based on the model architecture.\nfunc (tensor *ParameterTensors) Init(V, C, maxSeqLen, L int) {\n\ttensor.Memory = make([]float32,\n\t\tV*C+ // WordTokEmbed\n\t\t\tmaxSeqLen*C+ // WordPosEmbed\n\t\t\tL*C+ // LayerNorm1W\n\t\t\tL*C+ // LayerNorm1B\n\t\t\tL*3*C*C+ // QueryKeyValW\n\t\t\tL*3*C+ // QueryKeyValB\n\t\t\tL*C*C+ // AttProjW\n\t\t\tL*C+ // AttProjB\n\t\t\tL*C+ // Layer2NormW\n\t\t\tL*C+ // Layer2NormB\n\t\t\tL*4*C*C+ // FeedFwdW\n\t\t\tL*4*C+ // FeedFwdB\n\t\t\tL*C*4*C+ // FeedFwdProjW\n\t\t\tL*C+ // FeedFwdProjB\n\t\t\tC+ // LayerFinNormW\n\t\t\tC, // LayerFinNormB\n\t)\n\tvar ptr int\n\tmemPtr := tensor.Memory\n\ttensor.WordTokEmbed, ptr = newTensor(memPtr, V, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.WordPosEmbed, ptr = newTensor(memPtr, maxSeqLen, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm1W, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm1B, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.QueryKeyValW, ptr = newTensor(memPtr, L, 3*C, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.QueryKeyValB, ptr = newTensor(memPtr, L, 3*C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.AttProjW, ptr = newTensor(memPtr, L, C, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.AttProjB, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Layer2NormW, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Layer2NormB, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedFwdW, ptr = newTensor(memPtr, L, 4*C, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedFwdB, ptr = newTensor(memPtr, L, 4*C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedFwdProjW, ptr = newTensor(memPtr, L, C, 4*C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedFwdProjB, ptr = newTensor(memPtr, L, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerFinNormW, ptr = newTensor(memPtr, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerFinNormB, ptr = newTensor(memPtr, C)\n\tmemPtr = memPtr[ptr:]\n\tif len(memPtr) != 0 {\n\t\tpanic(\"something went real bad here\")\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#parameters","position":39},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Activations"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#activations","position":40},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Activations"},"content":"An activation is the output of the input, and a mathematical operation. If the weight determines the strength of the function, the activation is the output.\n\nback to top\n\n\n// ActivationTensors\ntype ActivationTensors struct {\n\tMemory             []float32\n\tEncoded            tensor // (B, T, C) - Initial encoded input representations (Batch size, Sequence length, Embedding dimension)\n\tLayer1Act          tensor // (L, B, T, C) - Activations after Layer Normalization 1\n\tLayerNorm1Mean     tensor // (L, B, T) - Mean values for Layer Normalization 1\n\tLayerNorm1Rstd     tensor // (L, B, T) - Reciprocal of standard deviation for Layer Normalization 1\n\tQueryKeyVal        tensor // (L, B, T, 3*C) - Combined Query, Key, Value representations for attention\n\tAttentionInter     tensor // (L, B, T, C) - Intermediate attention-like result\n\tPreAttention       tensor // (L, B, NH, T, T) - Pre-attention scores\n\tAttention          tensor // (L, B, NH, T, T) - Normalized attention weights (Number of layers, Batch size, Number of Attention Heads, Sequence length, Sequence length)\n\tAttentionProj      tensor // (L, B, T, C) - Projected attention outputs\n\tResidual2          tensor // (L, B, T, C) - Residual connection after attention\n\tLayerNorm2Act      tensor // (L, B, T, C) - Activations after Layer Normalization 2\n\tLayerNorm2Mean     tensor // (L, B, T) - Mean values for Layer Normalization 2\n\tLayerNorm2Rstd     tensor // (L, B, T) - Reciprocal of standard deviation for Layer Normalization 2\n\tFeedForward        tensor // (L, B, T, 4*C) - Intermediate Feed-Forward Network activations\n\tFeedForwardGelu    tensor // (L, B, T, 4*C) - FeedForward activations after applying GELU (non-linearity)\n\tFeedForwardProj    tensor // (L, B, T, C) - Projected output of the Feed-Forward Network\n\tResidual3          tensor // (L, B, T, C) - Residual connection after Feed-Forward Network\n\tLayerNormFinal     tensor // (B, T, C) - Final activations after Layer Normalization\n\tLayerNormFinalMean tensor // (B, T) - Mean values for final Layer Normalization\n\tLayerNormFinalStd  tensor // (B, T) - Reciprocal of standard deviation for final Layer Normalization\n\tLogits             tensor // (B, T, V) - Raw output scores (before softmax)\n\tProbabilities      tensor // (B, T, V) - Softmax probabilities over the vocabulary\n\tLosses             tensor // (B, T) - Loss values per token in the batch\n}\n\nfunc (tensor *ActivationTensors) Init(B, C, T, L, NH, V int) {\n\ttensor.Memory = make([]float32,\n\t\tB*T*C+\n\t\t\tL*B*T*C+\n\t\t\tL*B*T+\n\t\t\tL*B*T+\n\t\t\tL*B*T*C*3+\n\t\t\tL*B*T*C+\n\t\t\tL*B*NH*T*T+\n\t\t\tL*B*NH*T*T+\n\t\t\tL*B*T*C+\n\t\t\tL*B*T*C+\n\t\t\tL*B*T*C+\n\t\t\tL*B*T+\n\t\t\tL*B*T+\n\t\t\tL*B*T*C*4+\n\t\t\tL*B*T*C*4+\n\t\t\tL*B*T*C+\n\t\t\tL*B*T*C+\n\t\t\tB*T*C+\n\t\t\tB*T+\n\t\t\tB*T+\n\t\t\tB*T*V+\n\t\t\tB*T*V+\n\t\t\tB*T)\n\tvar ptr int\n\tmemPtr := tensor.Memory\n\ttensor.Encoded, ptr = newTensor(memPtr, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Layer1Act, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm1Mean, ptr = newTensor(memPtr, L, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm1Rstd, ptr = newTensor(memPtr, L, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.QueryKeyVal, ptr = newTensor(memPtr, L, B, T, C*3)\n\tmemPtr = memPtr[ptr:]\n\ttensor.AttentionInter, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.PreAttention, ptr = newTensor(memPtr, L, B, NH, T, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Attention, ptr = newTensor(memPtr, L, B, NH, T, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.AttentionProj, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Residual2, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm2Act, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm2Mean, ptr = newTensor(memPtr, L, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNorm2Rstd, ptr = newTensor(memPtr, L, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedForward, ptr = newTensor(memPtr, L, B, T, C*4)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedForwardGelu, ptr = newTensor(memPtr, L, B, T, C*4)\n\tmemPtr = memPtr[ptr:]\n\ttensor.FeedForwardProj, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Residual3, ptr = newTensor(memPtr, L, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNormFinal, ptr = newTensor(memPtr, B, T, C)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNormFinalMean, ptr = newTensor(memPtr, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.LayerNormFinalStd, ptr = newTensor(memPtr, B, T)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Logits, ptr = newTensor(memPtr, B, T, V)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Probabilities, ptr = newTensor(memPtr, B, T, V)\n\tmemPtr = memPtr[ptr:]\n\ttensor.Losses, ptr = newTensor(memPtr, B, T)\n\tmemPtr = memPtr[ptr:]\n\tif len(memPtr) != 0 {\n\t\tpanic(\"something went real bad here\")\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#activations","position":41},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Tokenizer"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#tokenizer","position":42},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Tokenizer"},"content":"\n\nTokenization is the fundamental process of transforming text data into a format the model can understand. It involves breaking down sentences into smaller units called tokens.\n\nback to top\n\nimport (\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"os\"\n\t\"sort\"\n)\n\nconst GPT2_EOT int32 = 50256\n\ntype Tokenizer struct {\n\tvocabSize  uint32\n\ttokenTable []string         // tokenTable maps token id to string\n\ttokenMap   map[string]int32 // tokenMap maps token to id\n\tinit       bool\n}\n\nfunc newTokenizer(vocab []string) Tokenizer {\n\ttokenizer := Tokenizer{\n\t\tvocabSize:  uint32(len(vocab)),\n\t\ttokenTable: vocab,\n\t\ttokenMap:   make(map[string]int32),\n\t\tinit:       true,\n\t}\n\tfor i, token := range vocab {\n\t\ttokenizer.tokenMap[token] = int32(i)\n\t}\n\treturn tokenizer\n}\n\nfunc NewTokenizer(filename string) (Tokenizer, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn Tokenizer{}, err\n\t}\n\tdefer f.Close()\n\theader := make([]uint32, 256)\n\tif err := binary.Read(f, binary.LittleEndian, header); err != nil {\n\t\treturn Tokenizer{}, err\n\t}\n\tif header[0] != 20240328 || header[1] != 1 {\n\t\treturn Tokenizer{}, errors.New(\"incorrect header for tokenizer\")\n\t}\n\ttok := Tokenizer{\n\t\tvocabSize:  header[2],\n\t\ttokenTable: make([]string, header[2]),\n\t\ttokenMap:   make(map[string]int32),\n\t\tinit:       true,\n\t}\n\tvar length byte\n\tfor i := range tok.tokenTable {\n\t\tif err := binary.Read(f, binary.LittleEndian, &length); err != nil {\n\t\t\treturn tok, err\n\t\t}\n\t\tif length <= 0 {\n\t\t\treturn tok, errors.New(\"tokenizer failure\")\n\t\t}\n\t\ttokenBytes := make([]byte, length)\n\t\tif err := binary.Read(f, binary.LittleEndian, tokenBytes); err != nil {\n\t\t\treturn tok, err\n\t\t}\n\t\ttok.tokenTable[i] = string(tokenBytes)\n\t\ttok.tokenMap[tok.tokenTable[i]] = int32(i)\n\t}\n\treturn tok, nil\n}\n\ntype TokenizerJSON struct {\n\tVersion string `json:\"version\"`\n\tModel   struct {\n\t\tType          string            `json:\"type\"`\n\t\tVocab         map[string]int    `json:\"vocab\"`\n\t\tMergesData    []string          `json:\"merges,omitempty\"`\n\t\tSpecialTokens map[string]string `json:\"special_tokens\"`\n\t} `json:\"model\"`\n}\n\nfunc NewTokenizerJson(filename string) (Tokenizer, error) {\n\t// Read the JSON file\n\tfileContent, err := os.ReadFile(filename)\n\tif err != nil {\n\t\treturn Tokenizer{}, err\n\t}\n\n\t// Unmarshal JSON into our struct\n\tvar tokenizerData TokenizerJSON\n\tif err := json.Unmarshal(fileContent, &tokenizerData); err != nil {\n\t\treturn Tokenizer{}, err\n\t}\n\n\t// Create a new Tokenizer instance\n\ttok := Tokenizer{\n\t\tvocabSize:  uint32(len(tokenizerData.Model.Vocab)),\n\t\ttokenTable: make([]string, len(tokenizerData.Model.Vocab)),\n\t\ttokenMap:   make(map[string]int32),\n\t\tinit:       true,\n\t}\n\n\t// Create a slice of token-id pairs for sorting\n\tvar tokenIDPairs []struct {\n\t\tToken string\n\t\tID    int\n\t}\n\tfor token, id := range tokenizerData.Model.Vocab {\n\t\t// Convert the first two bytes to the 'Ġ' character if they match 0xC4 0xA0\n\t\tif len(token) >= 2 && token[0] == 0xC4 && token[1] == 0xA0 {\n\t\t\ttoken = \" \" + token[2:]\n\t\t}\n\t\ttokenIDPairs = append(tokenIDPairs, struct {\n\t\t\tToken string\n\t\t\tID    int\n\t\t}{token, id})\n\t}\n\n\t// Sort the token-id pairs by ID\n\tsort.Slice(tokenIDPairs, func(i, j int) bool {\n\t\treturn tokenIDPairs[i].ID < tokenIDPairs[j].ID\n\t})\n\n\t// Populate tokenTable and tokenMap\n\tfor i, pair := range tokenIDPairs {\n\t\ttok.tokenTable[i] = pair.Token\n\t\ttok.tokenMap[pair.Token] = int32(i)\n\t}\n\n\treturn tok, nil\n}\n\nfunc (t Tokenizer) Decode(tokens []int32) (string, error) {\n\ts := \"\"\n\tfor _, token := range tokens {\n\t\tif token >= int32(len(t.tokenTable)) {\n\t\t\treturn \"\", errors.New(\"not valid token\")\n\t\t}\n\t\tif token != GPT2_EOT {\n\t\t\ts += t.tokenTable[token]\n\t\t}\n\t}\n\treturn s, nil\n}\n\nfunc (t Tokenizer) Encode(text string) ([]int32, error) {\n\ttokens := []int32{}\n\tfor len(text) > 0 {\n\t\tlongestMatch := \"\"\n\t\tlongestMatchToken := int32(GPT2_EOT)\n\t\tfor i := len(text); i > 0; i-- {\n\t\t\tsubStr := text[:i]\n\t\t\tif token, exists := t.tokenMap[subStr]; exists {\n\t\t\t\tlongestMatch = subStr\n\t\t\t\tlongestMatchToken = token\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif longestMatch == \"\" {\n\t\t\t// If no match found, treat the first character as an unknown token\n\t\t\ttokens = append(tokens, GPT2_EOT)\n\t\t\ttext = text[1:]\n\t\t} else {\n\t\t\ttokens = append(tokens, longestMatchToken)\n\t\t\ttext = text[len(longestMatch):]\n\t\t}\n\t}\n\treturn tokens, nil\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#tokenizer","position":43},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Tokenize some text","lvl2":"Tokenizer"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#tokenize-some-text","position":44},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Tokenize some text","lvl2":"Tokenizer"},"content":"\n\n%%\ntokenizer, err := NewTokenizerJson(\"/Users/joshcarp/Documents/the-interactive-transformer/tokenizer.json\"); if err != nil {\n    panic(err)\n}\ngonbui.RequestInput(\"Tokenize some text: \", false)\nreader := bufio.NewReader(os.Stdin)\ninputText, err := reader.ReadString('\\n')\nif err != nil {\n    panic(err)\n}\nif err != nil { panic(err) }\nencoded, err := tokenizer.Encode(inputText)\nfmt.Println(\"encoded: \", encoded)\ndecoded, err := tokenizer.Decode(encoded)\nfmt.Println(\"decoded: \", decoded)\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#tokenize-some-text","position":45},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Embedding","lvl2":"Tokenizer"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#embedding","position":46},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Embedding","lvl2":"Tokenizer"},"content":"\n\nencoderForward iterates through the batch/sequence and combines the word token embeddings\nwith the word position embeddings. This allows out vector to encode tokens and positions in one vector.\n\n\n\nWord embeddings\n\nback to top\n\nfunc encoderForward(out []float32, inp []int32, wte []float32, wpe []float32, B, T, C int) {\n\t// Iterate over each batch\n\tfor b := 0; b < B; b++ {\n\t\t// Iterate over each time step in the sequence\n\t\tfor t := 0; t < T; t++ {\n\t\t\t// Calculate the index in the output slice. Each vector is C elements long.\n\t\t\tstartOutIndex := b*T*C + t*C\n\t\t\t// Calculate the token ID index in the input\n\t\t\t// inp is the tokenized input, each number in inp char is an index within wte (word token embeddings)\n\t\t\tix := inp[b*T+t]\n\t\t\t// Calculate the index in the token embeddings slice\n\t\t\t// inp -> id -> wte[id]\n\t\t\tstartWteIndex := ix * int32(C)\n\t\t\t// Calculate the index in the position embeddings slice\n\t\t\t// Wpe starts at 0 (when t is zero) which is basically mapping directly to index\n\t\t\tstartWpeIndex := t * C\n\t\t\t// Add the vectors from `wte` and `wpe` and store the result in `out`\n\t\t\t// here we combine the vectors in the C dimensions.\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\tout[startOutIndex+i] = wte[startWteIndex+int32(i)] + wpe[startWpeIndex+i]\n\t\t\t}\n\t\t}\n\t}\n}\n\n%test\nfunc TestEncoderForwardExplicit(t *testing.T) {\n    inp := []int32{1, 0} // [1 -> wte (2, 3), wpe(4, 5)] [0 -> wte (0, 1), wpe(6, 7)]\n    wte := []float32{0, 1, 2, 3}\n    wpe := []float32{4, 5, 6, 7}\n    B := 1 // Batch size\n    T := 1 // Sequence Len\n    C := 2 // Dimensions\n    out := make([]float32, len(inp))\n    encoderForward(out, inp, wte, wpe, B, T, C)\n    expectedOut := []float32{6, 8}\n    assert.Equal(t, expectedOut, out)\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#embedding","position":47},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Layernorm forward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#layernorm-forward","position":48},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Layernorm forward"},"content":"layernormForward normalizes the activations in each layer.\nIt improves convergence in training and reduces sensitivity to initial parameters.\nFor each vector, the mean and variance are calculated.\n\nParameters:\n\nout: output activations (B,T,C)\n\nmean: mean values (B,T) for each position (b,t)\n\nrstd: reciprocal standard deviations (B,T) for each position (b,t)\n\ninp: input activations (B,T,C)\n\nweight: learnable weight (C) for scaling\n\nbias: learnable bias (C) for shifting\n\nB: batch size\n\nT: sequence length (number of time steps)\n\nC: embedding dimension (number of features)","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#layernorm-forward","position":49},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Layernorm forward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-1","position":50},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Layernorm forward"},"content":"Layer Normalization - Layernorm was introduced in this paper.\n\nback to top\n\nfunc layernormForward(out, mean, rstd, inp, weight, bias []float32, B, T, C int) {\n\tvar eps float32 = 1e-5\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\tx := inp[b*T*C+t*C:]\n\t\t\t// Calculate mean\n\t\t\tvar m float32 = 0.0\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\tm += x[i]\n\t\t\t}\n\t\t\tm /= float32(C)\n\t\t\t// Calculate variance\n\t\t\tvar v float32 = 0.0\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\txshift := x[i] - m\n\t\t\t\tv += xshift * xshift\n\t\t\t}\n\t\t\tv /= float32(C)\n\t\t\t// Calculate rstd (reciprocal standard deviation)\n\t\t\ts := 1.0 / Sqrt((v)+eps)\n\t\t\t// Normalize, scale, shift, and store output\n\t\t\toutBT := out[b*T*C+t*C:]\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t// subtract mean to center data\n\t\t\t\t// divide by std to scale variance\n\t\t\t\t// (val - mean) / std\n\t\t\t\tn := s * (x[i] - m)\n\t\t\t\t// Multiply the weight\n\t\t\t\to := n*weight[i] + bias[i]\n\t\t\t\toutBT[i] = o\n\t\t\t}\n\t\t\t// Store mean and rstd for backward pass\n\t\t\tmean[b*T+t] = m\n\t\t\trstd[b*T+t] = s\n\t\t}\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-1","position":51},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"AttentionForward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#attentionforward","position":52},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"AttentionForward"},"content":"\n\nattentionForward performs the attention forward pass.\n\nattention is the only layer that mixes information across time\nevery other operation is applied at every (b,t) position independently\n(and of course, no layer mixes information across batch)\n\nParameters:\n\nout: output matrix (B,T,C)\n\npreatt: pre-attention scores (B,NH,T,T)\n\natt: post-attention scores (B,NH,T,T)\n\ninp: input matrix (B,T,3C) holding Query, Key, Value vectors\n\nB: batch size\n\nT: sequence length (number of time steps)\n\nC: input dimension (number of features)\n\nNH: number of attention heads","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#attentionforward","position":53},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"AttentionForward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-2","position":54},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"AttentionForward"},"content":"Attention Is All You Need - The attention mechanism was introduced in the original transformer paper.\n\nback to top\n\nfunc attentionForward(out, preatt, att, inp []float32, B, T, C, NH int) {\n\tC3 := C * 3  // This is the dimensions for the key, query and values\n\ths := C / NH // head size\n\tscale := 1.0 / Sqrt(float32(hs))\n\t// Iterate over batch, sequence length, and number of heads\n\tvar wg sync.WaitGroup\n\tfor b := 0; b < B; b++ {\n\t\t// Sequence length\n\t\tfor t := 0; t < T; t++ {\n\t\t\tfor h := 0; h < NH; h++ {\n\t\t\t\twg.Add(1)\n\t\t\t\tgo func(b, t, h int) {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\t// Calculate indices for query, pre-attention, and attention arrays\n\t\t\t\t\t// query is any particular input asking for information from other inputs\n\t\t\t\t\tqueryT := inp[b*T*C3+t*C3+h*hs:] // inp[B][T][C3]\n\t\t\t\t\tpreattBth := preatt[b*NH*T*T+h*T*T+t*T:]\n\t\t\t\t\tattBth := att[b*NH*T*T+h*T*T+t*T:]\n\t\t\t\t\t// Pass 1: Calculate query dot key and max value\n\t\t\t\t\t// The dot product is described in the paper as being better because\n\t\t\t\t\t// it can be optimized with matrix multiplication\n\t\t\t\t\tvar maxval float32 = -10000.0\n\t\t\t\t\t// range from 0 to the current inp\n\t\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\t\t// Calculate key index for t2\n\t\t\t\t\t\tkey_t2 := inp[b*T*C3+t2*C3+h*hs+C:] // +C because it's key\n\t\t\t\t\t\t// Compute dot product and update max value\n\t\t\t\t\t\tvar val float32\n\t\t\t\t\t\tfor i := 0; i < hs; i++ {\n\t\t\t\t\t\t\tval += queryT[i] * key_t2[i]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tval *= scale\n\t\t\t\t\t\tif val > maxval {\n\t\t\t\t\t\t\tmaxval = val\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// preatt[b][h][t1][t2] == dot product (similarity) between query vector at position t1 and\n\t\t\t\t\t\t// key vector at t2.\n\t\t\t\t\t\tpreattBth[t2] = val\n\t\t\t\t\t}\n\t\t\t\t\t// Pass 2: Calculate the exp and keep track of sum\n\t\t\t\t\t// Calculate exponential sum and update preatt and att arrays\n\t\t\t\t\t// maps the max value to zero,\n\t\t\t\t\t// and everything else negative.\n\t\t\t\t\t// when the exp function is called then the range of numbers will be\n\t\t\t\t\t// between 0 and e.\n\t\t\t\t\tvar expsum float32\n\t\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\t\texpv := Exp((preattBth[t2]) - maxval)\n\t\t\t\t\t\t// expsum is a sum of all the exp'd pre_att values\n\t\t\t\t\t\texpsum += expv\n\t\t\t\t\t\t// att_bth[t2] is the exp'd preatt_bth[t2]\n\t\t\t\t\t\tattBth[t2] = expv\n\t\t\t\t\t}\n\t\t\t\t\tvar expsum_inv float32\n\t\t\t\t\tif expsum != 0.0 {\n\t\t\t\t\t\texpsum_inv = 1.0 / expsum\n\t\t\t\t\t}\n\t\t\t\t\t// Pass 3: Normalize to get softmax\n\t\t\t\t\t// from 0 -> t2: att_bth[t2] = exp(preatt[t2]) / sum(exp(preatt[:]))\n\t\t\t\t\t// for everything else it's zero\n\t\t\t\t\tfor t2 := 0; t2 < T; t2++ {\n\t\t\t\t\t\tif t2 <= t {\n\t\t\t\t\t\t\tattBth[t2] *= expsum_inv\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// Causal attention mask (optional; used for debugging and comparison)\n\t\t\t\t\t\t\tattBth[t2] = 0.0\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Pass 4: Accumulate weighted values into the output of attention\n\t\t\t\t\t// out = attention * values\n\t\t\t\t\t// The values in this instance are the initial token/position embeddings that have gone through many linear\n\t\t\t\t\t// transformations at this point.\n\t\t\t\t\t// This is simply applying the learned attention \"weights\" to the lkqv values.\n\t\t\t\t\t// These weights must change a whole bunch after back propagation.\n\t\t\t\t\tout_bth := out[b*T*C+t*C+h*hs:]\n\t\t\t\t\tfor i := 0; i < hs; i++ {\n\t\t\t\t\t\tout_bth[i] = 0.0\n\t\t\t\t\t}\n\t\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\t\tvalue_t2 := inp[b*T*C3+t2*C3+h*hs+C*2:] // +C*2 because it's value\n\t\t\t\t\t\tatt_btht2 := attBth[t2]\n\t\t\t\t\t\tfor i := 0; i < hs; i++ {\n\t\t\t\t\t\t\tout_bth[i] += att_btht2 * value_t2[i]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}(b, t, h)\n\t\t\t}\n\t\t}\n\t}\n\twg.Wait()\n}\n\n%test\nfunc TestAttentionForward(t *testing.T) {\n\ttype args struct {\n\t\tinp []float32\n\t\tB   int\n\t\tT   int\n\t\tC   int\n\t\tNH  int\n\t}\n\ttests := []struct {\n\t\tname       string\n\t\targs       args\n\t\twantOut    []float32\n\t\twantPreatt []float32\n\t\twantAtt    []float32\n\t}{\n\t\t{\n\t\t\tname: \"Small Input Test\",\n\t\t\targs: args{\n\t\t\t\tinp: []float32{1, 2, 3, 4, 5, 6},\n\t\t\t\tB:   1,\n\t\t\t\tT:   1,\n\t\t\t\tC:   2,\n\t\t\t\tNH:  1,\n\t\t\t},\n\t\t\twantOut:    []float32{5, 6},\n\t\t\twantPreatt: []float32{7.7781744},\n\t\t\twantAtt:    []float32{1},\n\t\t},\n\t\t{\n\t\t\tname: \"Larger Input Test\",\n\t\t\targs: args{\n\t\t\t\tinp: []float32{ // (B, T, C3)\n\t\t\t\t\t/* B = 1 */\n\t\t\t\t\t/* T =  0 */\n\t\t\t\t\t/*qry*/ 1, 2, 3, // query compared against (4, 5, 6) but not (13, 14, 15) because it's in the future (t=1)\n\t\t\t\t\t/*key*/ 4, 5, 6,\n\t\t\t\t\t/*val*/ 7, 8, 9,\n\t\t\t\t\t/* T =  1 */\n\t\t\t\t\t/*qry*/ 10, 11, 12, // will be compared against (4, 5, 6) (t-1) and (13, 14, 15)\n\t\t\t\t\t/*key*/ 13, 14, 15,\n\t\t\t\t\t/*val*/ 16, 17, 18, // vals are updated to\n\t\t\t\t},\n\t\t\t\tB:  1,\n\t\t\t\tT:  2,\n\t\t\t\tC:  3,\n\t\t\t\tNH: 1,\n\t\t\t},\n\t\t\twantOut: []float32{ // (B, T, C)\n\t\t\t\t/*      B = 0       */\n\t\t\t\t/*      T = 0       */\n\t\t\t\t/* C =  0    1    2 */\n\t\t\t\t/*  */ 7, 8, 9,\n\t\t\t\t/* T = 1 */\n\t\t\t\t/* C =  0    1    2 */\n\t\t\t\t/*  */ 16, 17, 18,\n\t\t\t},\n\t\t\twantPreatt: []float32{ // (B, NH, T, T)\n\t\t\t\t/* B =  0    */\n\t\t\t\t/* NH = 0    */\n\t\t\t\t/*T =   1  2 */\n\t\t\t\t/*T=1*/ 18.475208, 0, // preatt: 18 -> 1, 0 -> 0\n\t\t\t\t/*T=2*/ 96.417496, 267.89053, // 96 -> 9, 267 -> 1\n\t\t\t},\n\t\t\twantAtt: []float32{ // (B, NH, T, T)\n\t\t\t\t/* B = 0     */\n\t\t\t\t/* NH = 0    */\n\t\t\t\t/*T =   1  2 */\n\t\t\t\t/*T=1*/ 1, 0,\n\t\t\t\t/*T=2*/ 0, 1,\n\t\t\t},\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tout, preatt, att := make([]float32, len(tt.wantOut)), make([]float32, len(tt.wantPreatt)), make([]float32, len(tt.wantAtt))\n\t\t\tattentionForward(out, preatt, att, tt.args.inp, tt.args.B, tt.args.T, tt.args.C, tt.args.NH)\n\t\t\tassert.InDeltaSlice(t, tt.wantOut, out, 1e-4, fmt.Sprintf(\"want: %v got: %v\", tt.wantOut, out))\n\t\t\tassert.InDeltaSlice(t, tt.wantPreatt, preatt, 1e-4, fmt.Sprintf(\"want: %v got: %v\", tt.wantPreatt, preatt))\n\t\t\tassert.InDeltaSlice(t, tt.wantAtt, att, 1e-4, fmt.Sprintf(\"want: %v got: %v\", tt.wantAtt, att))\n\t\t})\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-2","position":55},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Residual forward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#residual-forward","position":56},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Residual forward"},"content":"\n\nresidualForward implements a simple residual connection, a common technique used in deep neural networks to improve training and performance.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#residual-forward","position":57},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Residual forward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-3","position":58},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Residual forward"},"content":"Deep Residual Learning for Image Recognition - The introduction of residuals allowed for deeper networks. Before this paper the depth of a neural network was limited because it would diverge enough and back propagation was really, really difficult to do because of vanishing gradients. Residuals essentially have a “short circuit” past a block which limits how much the neural networks can influence.\n\nback to top\n\nfunc residualForward(out, inp1, inp2 []float32, N int) {\n\tfor i := 0; i < N; i++ {\n\t\tout[i] = inp1[i] + inp2[i]\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-3","position":59},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"geluForward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#geluforward","position":60},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"geluForward"},"content":"The geluForward function applies the GELU activation to the input values stored in the inp slice and writes the activated values to the out slice.\n\ngeluForward is the Gaussian Error Linear Units activation function.\nIt leaves positive values mostly unchanged but\nmaps negative value close to zero.\nThis introduces “non-linearity” to the neural network and allows for the model to fit to functions that aren’t just linear regressions.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#geluforward","position":61},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"geluForward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-4","position":62},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"geluForward"},"content":"Gaussian Error Linear Units (GELUs) - Activation function that leaves positive values unchanged but maps negative numbers to near zero. Other architectures use different activation functions. For example, OpenElm uses SwiGLU.\n\nback to top\n\nvar GELUSCALEFACTOR = Sqrt(2.0 / math.Pi)\nfunc geluForward(out, inp []float32, n int) {\n\tfor i := 0; i < n; i++ {\n\t\tx := inp[i]\n\t\tcube := 0.044715 * x * x * x\n\t\tout[i] = 0.5 * x * (1.0 + Tanh(GELUSCALEFACTOR*(x+cube)))\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-4","position":63},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Softmax"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#softmax","position":64},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Softmax"},"content":"\n\nsoftmaxForward calculates the softmax probabilities for a batch of input logits, converting them into a probability distribution over multiple classes. It’s a common operation in neural networks, especially for classification tasks.\n\nback to top\n\nfunc softmaxForward(probs, logits []float32, B, T, V int) {\n\tvar wg sync.WaitGroup\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\twg.Add(1)\n\t\t\tgo func(b, t int) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tbaseIndex := b*T*V + t*V\n\t\t\t\tlogitsBT := logits[baseIndex : baseIndex+V]\n\t\t\t\tprobsBT := probs[baseIndex : baseIndex+V]\n\t\t\t\t// Numerical Stability\n\t\t\t\tvar maxval float32 = -10000.0\n\t\t\t\tfor i := 0; i < V; i++ {\n\t\t\t\t\tif logitsBT[i] > maxval {\n\t\t\t\t\t\tmaxval = logitsBT[i]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Calculate exponentials and sum\n\t\t\t\tvar sum float32\n\t\t\t\tfor i := 0; i < V; i++ {\n\t\t\t\t\tprobsBT[i] = Exp((logitsBT[i] - maxval))\n\t\t\t\t\tsum += probsBT[i] // Using float32 for potential precision gain\n\t\t\t\t}\n\t\t\t\t// Normalize\n\t\t\t\tfor i := 0; i < V; i++ {\n\t\t\t\t\tprobsBT[i] /= sum\n\t\t\t\t}\n\t\t\t}(b, t)\n\t\t}\n\t}\n\twg.Wait()\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#softmax","position":65},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"CrossEntropyForward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#crossentropyforward","position":66},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"CrossEntropyForward"},"content":"The function crossEntropyForward calculates the cross-entropy loss for a batch of predicted probability distributions and their corresponding target labels.\n\nback to top\n\n// crossEntropyForward\nfunc crossEntropyForward(losses []float32, probs []float32, targets []int32, B, T, V int) {\n\t// Iterate over each batch\n\tfor b := 0; b < B; b++ {\n\t\t// Iterate over each time step in the sequence\n\t\tfor t := 0; t < T; t++ {\n\t\t\t// Calculate the index in the probability slice\n\t\t\tstartIndex := int32(b*T*V + t*V)\n\t\t\t// Get the correct index in the logits for the current batch and time step\n\t\t\tix := targets[b*T+t]\n\t\t\t// Calculate the cross-entropy loss\n\t\t\tprob := probs[startIndex+ix]\n\t\t\t// Calculate the negative log of the probability for the correct target index\n\t\t\tlosses[b*T+t] = -Log((prob))\n\t\t}\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#crossentropyforward","position":67},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Putting it all together"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#putting-it-all-together","position":68},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Putting it all together"},"content":"\n\ntype GPT2Config struct {\n\tMaxSeqLen int `json:\"max_seq_len\"`\n\tV         int `json:\"vocab_size\"`\n\tL         int `json:\"num_layers\"`\n\tNH        int `json:\"num_heads\"`\n\tC         int `json:\"channels\"`\n\tEOT       int32\n}\n\n\ntype GPT2 struct {\n\tTokenizer Tokenizer\n\tConfig    GPT2Config // Hyper-parameters of the model\n\t// Params has the actual weights of the model. Params.Memory is for convenience to be able to set/reset parameters simply\n\tParams ParameterTensors // Weights of the model\n\t// Grads contains the delta/gradient that will eventually be applied to the params in the model\n\tGrads ParameterTensors // Gradients of the weights\n\t// Fields for AdamW optimizer\n\tMMemory []float32         // First moment estimates (for AdamW)\n\tVMemory []float32         // Second moment estimates (for AdamW)\n\tActs    ActivationTensors // Activations of the model\n\t// gradients of the activations\n\tGradsActs ActivationTensors\n\tB         int     // Current batch size (B)\n\tT         int     // Current sequence length (T)\n\tInputs    []int32 // Input tokens\n\tTargets   []int32 // Target tokens\n\tMeanLoss  float32 // Mean loss after a forward pass\n\tRand      *rand.Rand\n}\n\n\nfunc loadFromReader(f io.Reader) (*GPT2, error) {\n\theader := make([]int32, 256)\n\terr := binary.Read(f, binary.LittleEndian, header)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error reading model header: %v\", err)\n\t}\n\tif header[0] != 20240326 || header[1] != 1 {\n\t\treturn nil, fmt.Errorf(\"bad model file format\")\n\t}\n\tmodel := &GPT2{\n\t\tConfig: GPT2Config{\n\t\t\tMaxSeqLen: int(header[2]),\n\t\t\tV:         int(header[3]),\n\t\t\tL:         int(header[4]),\n\t\t\tNH:        int(header[5]),\n\t\t\tC:         int(header[6]),\n\t\t\tEOT:       GPT2_EOT,\n\t\t},\n\t\tRand: rand.New(rand.NewSource(21)),\n\t}\n\tmodel.Params.Init(model.Config.V, model.Config.C, model.Config.MaxSeqLen, model.Config.L)\n\tif err := binary.Read(f, binary.LittleEndian, model.Params.Memory); err != nil {\n\t\treturn nil, fmt.Errorf(\"error reading model: %v\", err)\n\t}\n\treturn model, nil\n}\n// LoadGPT2Model loads the GPT-2 model from a checkpoint file.\nfunc LoadGPT2Model(checkpointPath, tokenizerFile string) (*GPT2, error) {\n\t// File Reading\n\tf, err := os.Open(checkpointPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Error opening model file: %v\", err)\n\t}\n\tdefer f.Close()\n\t// Read Model Header\n\tmodel, err := loadFromReader(f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif tokenizerFile == \"\" {\n\t\treturn model, err\n\t}\n\ttok, err := NewTokenizer(tokenizerFile)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmodel.Tokenizer = tok\n\treturn model, nil\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#putting-it-all-together","position":69},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Forward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#forward","position":70},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Forward"},"content":"The function Forward implements the forward pass of a GPT-2 language model. It takes a sequence of input tokens and a sequence of target tokens (if available) as input, and it calculates the model’s output probabilities for the next token in the sequence.\n\nback to top\n\nfunc (model *GPT2) Forward(input, target []int32, B, T int) {\n\tV, L, NH, C := model.Config.V, model.Config.L, model.Config.NH, model.Config.C\n\tif model.Acts.Memory == nil {\n\t\tmodel.B, model.T = B, T\n\t\tmodel.Acts.Init(B, C, T, L, NH, V)\n\t\tmodel.Inputs = make([]int32, B*T)\n\t\tmodel.Targets = make([]int32, B*T)\n\t}\n\tcopy(model.Inputs, input)\n\tcopy(model.Targets, target)\n\tparams, acts := model.Params, model.Acts\n\t// This encodes the word token embeddings with the positional embeddings\n\t// so that those vectors have spacial information and aren't just purely made up of the\n\t// token embeddings. The result of this is stored in acts.Encoded.\n\t// Input is a slice of ids/tokens that correspond to the vectors in WTE and their index is the \"position\"\n\tencoderForward(acts.Encoded.data, input, params.WordTokEmbed.data, params.WordPosEmbed.data, B, T, C)\n\tvar residual []float32\n\tfor l := 0; l < L; l++ {\n\t\t// residual is a connection between the last layers output, or the initial token/pos embedding (as applied above)\n\t\tif l == 0 {\n\t\t\tresidual = acts.Encoded.data\n\t\t} else {\n\t\t\tresidual = acts.Residual3.data[(l-1)*B*T*C:]\n\t\t}\n\t\t// Parameters\n\t\tl_ln1w := params.LayerNorm1W.data[l*C:]\n\t\tl_ln1b := params.LayerNorm1B.data[l*C:]\n\t\tl_qkvw := params.QueryKeyValW.data[l*3*C*C:]\n\t\tl_qkvb := params.QueryKeyValB.data[l*3*C:]\n\t\tl_attprojw := params.AttProjW.data[l*C*C:]\n\t\tl_attprojb := params.AttProjB.data[l*C:]\n\t\tl_ln2w := params.Layer2NormW.data[l*C:]\n\t\tl_ln2b := params.Layer2NormB.data[l*C:]\n\t\tl_fcw := params.FeedFwdW.data[l*4*C*C:]\n\t\tl_fcb := params.FeedFwdB.data[l*4*C:]\n\t\tl_fcprojw := params.FeedFwdProjW.data[l*C*4*C:]\n\t\tl_fcprojb := params.FeedFwdProjB.data[l*C:]\n\t\t// Activations\n\t\tl_ln1 := acts.Layer1Act.data[l*B*T*C:]\n\t\tl_ln1_mean := acts.LayerNorm1Mean.data[l*B*T:]\n\t\tl_ln1_rstd := acts.LayerNorm1Rstd.data[l*B*T:]\n\t\tl_qkv := acts.QueryKeyVal.data[l*B*T*3*C:]\n\t\tl_atty := acts.AttentionInter.data[l*B*T*C:]\n\t\tl_preatt := acts.PreAttention.data[l*B*NH*T*T:]\n\t\tl_att := acts.Attention.data[l*B*NH*T*T:]\n\t\tl_attproj := acts.AttentionProj.data[l*B*T*C:]\n\t\tl_residual2 := acts.Residual2.data[l*B*T*C:]\n\t\tl_ln2 := acts.LayerNorm2Act.data[l*B*T*C:]\n\t\tl_ln2_mean := acts.LayerNorm2Mean.data[l*B*T:]\n\t\tl_ln2_rstd := acts.LayerNorm2Rstd.data[l*B*T:]\n\t\tl_fch := acts.FeedForward.data[l*B*T*4*C:]\n\t\tl_fch_gelu := acts.FeedForwardGelu.data[l*B*T*4*C:]\n\t\tl_fcproj := acts.FeedForwardProj.data[l*B*T*C:]\n\t\tl_residual3 := acts.Residual3.data[l*B*T*C:]\n\t\t// Here we normalise the layer so that the mean is 0 and the standard deviation is ~1.\n\t\t// residual contains the un-edited activations\n\t\tlayernormForward(l_ln1, l_ln1_mean, l_ln1_rstd, residual /*inp*/, l_ln1w /*weight*/, l_ln1b /*bias*/, B, T, C)\n\t\t/*\n\t\t\t\t\tl_qkvw = weight = Query Key Val Weights (C * 3C)\n\t\t\t\t\tl_ln1 = inp = layer activations\n\t\t\t\t\tl_qkvb = bias = Query Key Val Bias\n\t\t\t\t\tl_qkv = out = key/query/value matrix\n\t\t\t\tHere we're matrix multiplying  l_ln1(inp)*l_qkvw(weight) + l_qkvb(bias)\n\t\t\t\tThis matrix multiplication essentially gets a layer activation for the model inputs (activations) which are multiplied\n\t\t\t\tby the model weights.\n\t\t\tThis does the input \"projection\" via linear transformations via the model query/key/value weights into higher dimensionality.\n\t\t*/\n\t\tmatmulForward(l_qkv, l_ln1, l_qkvw, l_qkvb, B, T, C, 3*C)\n\t\t/*\n\t\t\tThe attention forward pass takes these query/key/value vectors, along with the model attention weights\n\t\t\tThe model pre-attention scores, after the forward pass, have the un-normalised attention scores\n\t\t\tatt has the attention acores and l_atty has the attention scores + the query/key/value scores\n\t\t\tl_qkv has the projection of the activations into a higher dimension.\n\t\t\tl_preatt: has the projection qkv vectors dot product(similarity), between an input's query and another input's key.\n\t\t\t\tThis basically goes like this:\n\t\t\t\tword a: has a query vector \"what am i looking for\"\n\t\t\t\tword b: has a query vector \"what do i need\"\n\t\t\t\tif they're similar, these vectors will be similar, therefore the scores will be high and be stored in l_preatt\n\t\t\tthe v in the qkv is the original token/position embeddings which have been through a number of linear transformations at this point.\n\t\t*/\n\t\tattentionForward(l_atty, l_preatt, l_att, l_qkv, B, T, C, NH)\n\n\t\t/*\n\t\t\tHere we do another matrix multiplication of attention weights and biases\n\t\t\tThis projects the l_atty into another dimension. These will probably also get back propagated.\n\t\t*/\n\t\tmatmulForward(l_attproj, l_atty, l_attprojw, l_attprojb, B, T, C, C)\n\t\t/*\n\t\t\tThe residual forward simply adds the attention projection and the residual layer, which is the\n\t\t\tweights(or activations?) before any of the previous transformations. This allows a stronger signal and\n\t\t\tprevents weight dropout and i think makes back propagation more efficient.\n\t\t*/\n\t\tresidualForward(l_residual2, residual, l_attproj, B*T*C)\n\t\t/*\n\t\t\tThe weights in this level are the layer 2 activations, which are multiplied with the residual through the above sections\n\t\t\tThis is normalised and everything into layernorm2\n\t\t*/\n\t\tlayernormForward(l_ln2, l_ln2_mean, l_ln2_rstd, l_residual2, l_ln2w, l_ln2b, B, T, C)\n\t\t/*\n\t\t\tFeedforward is just another layer of a multi layer perceptron to make the \"higher level\" connections.\n\t\t*/\n\t\tmatmulForward(l_fch, l_ln2, l_fcw, l_fcb, B, T, C, 4*C)\n\t\t/*\n\t\t\tThis is an acitvation function which maps large values to close to one and smaller values to zero.\n\t\t*/\n\t\tgeluForward(l_fch_gelu, l_fch, B*T*4*C)\n\t\t/*\n\t\t\tThis now squishes the last layer into a smaller dimension so it can be added to the next layer.\n\t\t*/\n\t\tmatmulForward(l_fcproj, l_fch_gelu, l_fcprojw, l_fcprojb, B, T, 4*C, C)\n\t\t/*\n\t\t\tNow we set the next residual layer as the output of this layer. This is the l_fcproj + the current layer residual\n\t\t*/\n\t\tresidualForward(l_residual3, l_residual2, l_fcproj, B*T*C)\n\t}\n\tresidual = acts.Residual3.data[(L-1)*B*T*C:]\n\n\t/*\n\t\tNow this is the last thing. We're layer norming the final layer activations so that the logits can be calculated\n\n\t*/\n\tlayernormForward(acts.LayerNormFinal.data, acts.LayerNormFinalMean.data, acts.LayerNormFinalStd.data, residual, params.LayerFinNormW.data, params.LayerFinNormB.data, B, T, C)\n\t/*\n\t\t\tMatrix multiplying the Word Token embedding gives us the logits.\n\t\tThis is calculating a weighted sum. More likely tokens will be blown up and less likely will be zero or negative.\n\t*/\n\tmatmulForward(acts.Logits.data, acts.LayerNormFinal.data, params.WordTokEmbed.data, nil, B, T, C, V)\n\t/*\n\t\tAfter all of this we can softmax the logits to get probabilities over the entire vocabulary\n\t*/\n\tsoftmaxForward(acts.Probabilities.data, acts.Logits.data, B, T, V)\n\t// also forward the cross-entropy loss function if we have the targets\n\tif len(target) > 0 {\n\t\t/*\n\t\t\tThis compares the probabilities for each token and compares it to the target to calculate a loss.\n\t\t*/\n\t\tcrossEntropyForward(model.Acts.Losses.data, model.Acts.Probabilities.data, target, B, T, V)\n\t\t// for convenience also evaluate the mean loss\n\t\tvar meanLoss float32\n\t\tfor i := range model.Acts.Losses.data {\n\t\t\tmeanLoss += model.Acts.Losses.data[i]\n\t\t}\n\t\tmeanLoss /= float32(B * T)\n\t\tmodel.MeanLoss = meanLoss\n\n\t} else {\n\t\tmodel.MeanLoss = -1.0\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#forward","position":71},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Sampling"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#sampling","position":72},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Sampling"},"content":"The probabilities are a float array of:\n\nindex/tokenid:probability\n\ncoin is a random value between 0 and 1.\n\nWe start with a cumulative sum, and when it gets above our target coin, we return.\n\nThis makes it that the most likely token returned is the one that has the most probability, but we still have the possibiility of choosing other ones, proportional to how likley they are.\n\nback to top\n\nfunc sampleMult(probabilities []float32, coin float32) int {\n\tvar cdf float32\n\tfor i, prob := range probabilities {\n\t\tcdf += prob\n\t\tif coin < cdf {\n\t\t\treturn i\n\t\t}\n\t}\n\treturn len(probabilities) - 1\n}\n\n\nfunc (model *GPT2) Inference(input string) (string, error) {\n\tB, T, nTokens := 1, 64, 20\n\tstart := time.Now()\n\tdefer func() {\n\t\tfmt.Printf(\"inference time took: %v\\n\", time.Now().Sub(start))\n\t}()\n\ttokens, err := model.Tokenizer.Encode(input)\n\t//prompt_len := len(tokens)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif len(tokens) < T {\n\t\tfor i := len(tokens); i <= T; i++ {\n\t\t\ttokens = append(tokens, model.Config.EOT)\n\t\t}\n\t}\n\tfmt.Printf(\"input is %d tokens long\\n\", len(tokens))\n\tmodel.Forward(tokens, tokens[1:], B, T)\n\tfor t := 1; t < nTokens; t++ {\n\t\t// for each t, we re-compute all activations between 0 and t\n\t\t// leaving this alone because you want separate code for inference anyway\n\t\t// the inference here is just for sanity checking purposes\n\t\tmodel.Forward(tokens, nil, B, t)\n\t\tprobabilities := model.Acts.Probabilities.data[(t-1)*model.Config.V:]\n\t\tcoin := model.Rand.Float32()\n\t\tnextToken2 := sampleMult(probabilities, coin)\n\t\ttokens[t] = rune(nextToken2)\n\t\tout, err := model.Tokenizer.Decode([]int32{tokens[t]})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tfmt.Print(out)\n\n\t}\n\treturn model.Tokenizer.Decode(tokens)\n}\n\nfunc newGPT2(MaxSeqLen, V, L, NH, C int, vocab []string) GPT2 {\n\tmodel := GPT2{\n\t\tConfig: GPT2Config{\n\t\t\tMaxSeqLen: MaxSeqLen,\n\t\t\tV:         V,\n\t\t\tL:         L,\n\t\t\tNH:        NH,\n\t\t\tC:         C,\n\t\t},\n\t\tParams:    newParameterTensors(V, C, MaxSeqLen, L),\n\t\tTokenizer: newTokenizer(vocab),\n\t\tRand:      rand.New(rand.NewSource(21)),\n\t}\n\treturn model\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#sampling","position":73},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Do some inference"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#do-some-inference","position":74},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Do some inference"},"content":"\n\n%%\n\npath := \"/Users/joshcarp/Documents/the-interactive-transformer/\"\nmodel, err := LoadGPT2Model(path+\"/gpt2_124M.bin\", path+\"/gpt2_tokenizer.bin\")\nif err != nil {\n    panic(err)\n}\ngonbui.RequestInput(\"gpt2 text complete: \", false)\nreader := bufio.NewReader(os.Stdin)\ninputText, err := reader.ReadString('\\n')\nif err != nil {\n    panic(err)\n}\n_, err = model.Inference(inputText)\nif err != nil {\n    panic(err)\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#do-some-inference","position":75},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Backward Pass"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#backward-pass","position":76},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"Backward Pass"},"content":"The backwards pass is where the “learning” happens. It is used to update the weights of the model.\nIf we’re using the model for inference, deploying it as a chatbot, etc, we don’t do a backwards pass.\n\nThe backward pass calculates the difference between the predicted tokens (before the sampling), and calculates a gradient based on the learning algorithm.\n\n\n\nBackpropagation","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#backward-pass","position":77},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Backward Pass"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-5","position":78},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl2":"Backward Pass"},"content":"Learning representations by back-propagating errors - Back-propagation was introduced here, couldn’t find the original paper. This was done by Hinton and co and was what lead to the AI era of the 80s.\n\nback to top\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-5","position":79},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"crossentropySoftmaxBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#crossentropysoftmaxbackward","position":80},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"crossentropySoftmaxBackward"},"content":"The function computes the gradients of the logits (dlogits) with respect to the loss, given the probabilities (probs) and target labels (targets).\nThis gradient information is used during backpropagation to update the weights and biases of the network to minimize the cross-entropy loss.\n\nback to top\n\n// crossentropySoftmaxBackward calculates the cross entropy\nfunc crossentropySoftmaxBackward(dlogits, dlosses, probs []float32, targets []int32, B, T, V int) {\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\tbaseIndex := b*T*V + t*V\n\t\t\tdlogitsBT := dlogits[baseIndex : baseIndex+V]\n\t\t\tprobsBT := probs[baseIndex : baseIndex+V]\n\t\t\tdloss := dlosses[b*T+t]\n\t\t\tix := targets[b*T+t]\n\t\t\tfor i := 0; i < V; i++ {\n\t\t\t\tp := probsBT[i]\n\t\t\t\tvar indicator float32\n\t\t\t\tif int32(i) == ix {\n\t\t\t\t\tindicator = 1.0\n\t\t\t\t} else {\n\t\t\t\t\tindicator = 0.0\n\t\t\t\t}\n\t\t\t\tdlogitsBT[i] += (p - indicator) * dloss\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#crossentropysoftmaxbackward","position":81},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"matmulBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#matmulbackward","position":82},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"matmulBackward"},"content":"The function computes the gradients of the inputs (dinp), weights (dweight), and biases (dbias) for a matrix multiplication operation. These gradients are necessary for adjusting the model parameters during training to minimize the error.\n\nback to top\n\nfunc matmulBackward(dinp, dweight, dbias, dout, inp, weight []float32, B, T, C, OC int) {\n\tvar wg sync.WaitGroup\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\twg.Add(1)\n\t\t\tgo func(b, t int) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tdoutBt := dout[b*T*OC+t*OC:]\n\t\t\t\tdinpBt := dinp[b*T*C+t*C:]\n\t\t\t\tfor o := 0; o < OC; o++ {\n\t\t\t\t\twrow := weight[o*C:]\n\t\t\t\t\td := doutBt[o]\n\t\t\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t\t\tdinpBt[i] += wrow[i] * d\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}(b, t)\n\t\t}\n\t}\n\twg.Wait()\n\tfor o := 0; o < OC; o++ {\n\t\twg.Add(1)\n\t\tgo func(o int) {\n\t\t\tdefer wg.Done()\n\t\t\tfor b := 0; b < B; b++ {\n\t\t\t\tfor t := 0; t < T; t++ {\n\t\t\t\t\tdoutBt := dout[b*T*OC+t*OC:]\n\t\t\t\t\tinpBt := inp[b*T*C+t*C:]\n\t\t\t\t\tdwrow := dweight[o*C:]\n\t\t\t\t\td := doutBt[o]\n\t\t\t\t\tif dbias != nil {\n\t\t\t\t\t\tdbias[o] += d\n\t\t\t\t\t}\n\t\t\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t\t\tdwrow[i] += inpBt[i] * d\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}(o)\n\t}\n\twg.Wait()\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#matmulbackward","position":83},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"layernormBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#layernormbackward","position":84},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"layernormBackward"},"content":"The function layernormBackward calculates the gradients for the backward pass of a Layer Normalization (LayerNorm) operation in a neural network. Here’s a breakdown of what it does:\n\nLayer Normalization is a technique used to normalize the activations of a layer across its features, improving the training stability and performance of deep neural networks. It involves normalizing the input to have zero mean and unit variance. This function calculates the gradients needed to update the weights and biases of the LayerNorm operation during backpropagation.\n\nback to top\n\nfunc layernormBackward(dinp, dweight, dbias, dout, inp, weight, mean, rstd []float32, B, T, C int) {\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\tbaseIndex := b*T*C + t*C\n\t\t\tdoutBT := dout[baseIndex : baseIndex+C]\n\t\t\tinpBT := inp[baseIndex : baseIndex+C]\n\t\t\tdinpBT := dinp[baseIndex : baseIndex+C]\n\t\t\tmeanBT := mean[b*T+t]\n\t\t\trstdBT := rstd[b*T+t]\n\n\t\t\t// Reduce operations\n\t\t\tvar dnormMean float32 = 0.0\n\t\t\tvar dnormNormMean float32 = 0.0\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\tnormBTI := (inpBT[i] - meanBT) * rstdBT\n\t\t\t\tdnormI := weight[i] * doutBT[i]\n\t\t\t\tdnormMean += dnormI\n\t\t\t\tdnormNormMean += dnormI * normBTI\n\t\t\t}\n\t\t\tdnormMean /= float32(C)\n\t\t\tdnormNormMean /= float32(C)\n\n\t\t\t// Accumulation loop\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\tnormBTI := (inpBT[i] - meanBT) * rstdBT\n\t\t\t\tdnormI := weight[i] * doutBT[i]\n\t\t\t\tdbias[i] += doutBT[i]\n\t\t\t\tdweight[i] += normBTI * doutBT[i]\n\n\t\t\t\tvar dval float32\n\t\t\t\tdval += dnormI                  // Term 1\n\t\t\t\tdval -= dnormMean               // Term 2\n\t\t\t\tdval -= normBTI * dnormNormMean // Term 3\n\t\t\t\tdval *= rstdBT                  // Final scale\n\t\t\t\tdinpBT[i] += dval\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#layernormbackward","position":85},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"residualBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#residualbackward","position":86},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"residualBackward"},"content":"The function residualBackward calculates the gradients for the backward pass of a residual connection in a neural network. Here’s a breakdown of what it does:\n\nback to top\n\nfunc residualBackward(dinp1, dinp2, dout []float32, N int) {\n\tfor i := 0; i < N; i++ {\n\t\tdinp1[i] += dout[i]\n\t\tdinp2[i] += dout[i]\n\t}\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#residualbackward","position":87},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"geluBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#gelubackward","position":88},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"geluBackward"},"content":"Computes the gradient of the Gaussian Error Linear Unit (GELU) activation function for backpropagation in a neural network.\n\nback to top\n\n// geluBackward computes the backward pass of the GeLU non-linearity\nfunc geluBackward(dinp, inp, dout []float32, n int) {\n\tfor i := 0; i < n; i++ {\n\t\tx := inp[i]\n\t\tcube := 0.044715 * x * x * x\n\t\ttanhArg := GELUSCALEFACTOR * (x + cube)\n\t\ttanhOut := Tanh(tanhArg)\n\t\tcoshfOut := Cosh(tanhArg)\n\t\tsechOut := 1.0 / (coshfOut * coshfOut)\n\t\tlocalGrad := 0.5*(1.0+tanhOut) + x*0.5*sechOut*GELUSCALEFACTOR*(1.0+3.0*0.044715*x*x)\n\t\tdinp[i] += localGrad * dout[i]\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#gelubackward","position":89},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"attentionBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#attentionbackward","position":90},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"attentionBackward"},"content":"The attentionBackward function implements the backward pass for a self-attention mechanism in a neural network. This is a crucial part of training attention-based models, like transformers. It calculates the gradients of the attention weights, queries, keys, and values with respect to the outputs of the attention layer, allowing the model to adjust its parameters to improve performance.\n\nback to top\n\n// attentionBackward performs the backward pass for an attention mechanism\nfunc attentionBackward(dinp, dpreatt, datt, dout, inp, att []float32, B, T, C, NH int) {\n\t// C3 is 3 times C, representing the size of Q, K, and V combined\n\tC3 := C * 3\n\t// hs is the size of each head\n\ths := C / NH\n\t// scale is the factor used in the forward pass to scale the dot product\n\tscale := 1.0 / Sqrt(float32(hs))\n\t// Iterate through batch, time, and heads\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\tfor h := 0; h < NH; h++ {\n\t\t\t\t// Calculate the indices for the arrays in this specific iteration\n\t\t\t\tattBTH := att[b*NH*T*T+h*T*T+t*T:]\n\t\t\t\tdattBTH := datt[b*NH*T*T+h*T*T+t*T:]\n\t\t\t\tdpreattBTH := dpreatt[b*NH*T*T+h*T*T+t*T:]\n\t\t\t\tdqueryT := dinp[b*T*C3+t*C3+h*hs:]\n\t\t\t\tqueryT := inp[b*T*C3+t*C3+h*hs:]\n\t\t\t\t// Backward pass 4: value accumulation\n\t\t\t\tdoutBTH := dout[b*T*C+t*C+h*hs:]\n\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\tvalueT2 := inp[b*T*C3+t2*C3+h*hs+C*2:]\n\t\t\t\t\tdvalueT2 := dinp[b*T*C3+t2*C3+h*hs+C*2:]\n\t\t\t\t\tfor i := 0; i < hs; i++ {\n\t\t\t\t\t\t// Compute gradients for attention and value accumulation\n\t\t\t\t\t\tdattBTH[t2] += valueT2[i] * doutBTH[i]\n\t\t\t\t\t\tdvalueT2[i] += attBTH[t2] * doutBTH[i]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Backward pass 2 & 3: softmax backward\n\t\t\t\t// Softmax does not require input (preatt) to backward\n\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\tfor t3 := 0; t3 <= t; t3++ {\n\t\t\t\t\t\tvar indicator float32\n\t\t\t\t\t\tif t2 == t3 {\n\t\t\t\t\t\t\tindicator = 1.0\n\t\t\t\t\t\t}\n\t\t\t\t\t\tlocalDerivative := attBTH[t2] * (indicator - attBTH[t3])\n\t\t\t\t\t\tdpreattBTH[t3] += localDerivative * dattBTH[t2]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Backward pass 1: query @ key matmul\n\t\t\t\tfor t2 := 0; t2 <= t; t2++ {\n\t\t\t\t\tkeyT2 := inp[b*T*C3+t2*C3+h*hs+C:]\n\t\t\t\t\tdkeyT2 := dinp[b*T*C3+t2*C3+h*hs+C:]\n\t\t\t\t\tfor i := 0; i < hs; i++ {\n\t\t\t\t\t\t// Compute gradients for query and key\n\t\t\t\t\t\tdqueryT[i] += keyT2[i] * dpreattBTH[t2] * scale\n\t\t\t\t\t\tdkeyT2[i] += queryT[i] * dpreattBTH[t2] * scale\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#attentionbackward","position":91},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"matmulBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#matmulbackward-1","position":92},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"matmulBackward"},"content":"The function computes the gradients of the inputs (dinp), weights (dweight), and biases (dbias) for a matrix multiplication operation. These gradients are necessary for adjusting the model parameters during training to minimize the error.\n\ndinp: A slice of floats representing the gradients of the outputs with respect to the inputs of the matrix multiplication. This is often calculated by the subsequent layer in the network.\ndweight: A slice of floats representing the gradients of the outputs with respect to the weights. Initially, this slice is filled with zeros.\ndbias: A slice of floats representing the gradients of the outputs with respect to the biases. Initially, this slice is filled with zeros.\ndout: A slice of floats representing the outputs of the matrix multiplication.\ninp: A slice of floats representing the inputs to the matrix multiplication.\nweight: A slice of floats representing the weights of the matrix multiplication.\nB: The batch size (number of samples).\nT: The time steps or sequence length.\nC: The number of input features.\nOC: The number of output features.\n\nback to top\n\nfunc matmulBackward(dinp, dweight, dbias, dout, inp, weight []float32, B, T, C, OC int) {\n\tvar wg sync.WaitGroup\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\twg.Add(1)\n\t\t\tgo func(b, t int) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tdoutBt := dout[b*T*OC+t*OC:]\n\t\t\t\tdinpBt := dinp[b*T*C+t*C:]\n\t\t\t\tfor o := 0; o < OC; o++ {\n\t\t\t\t\twrow := weight[o*C:]\n\t\t\t\t\td := doutBt[o]\n\t\t\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t\t\tdinpBt[i] += wrow[i] * d\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}(b, t)\n\t\t}\n\t}\n\twg.Wait()\n\tfor o := 0; o < OC; o++ {\n\t\twg.Add(1)\n\t\tgo func(o int) {\n\t\t\tdefer wg.Done()\n\t\t\tfor b := 0; b < B; b++ {\n\t\t\t\tfor t := 0; t < T; t++ {\n\t\t\t\t\tdoutBt := dout[b*T*OC+t*OC:]\n\t\t\t\t\tinpBt := inp[b*T*C+t*C:]\n\t\t\t\t\tdwrow := dweight[o*C:]\n\t\t\t\t\td := doutBt[o]\n\t\t\t\t\tif dbias != nil {\n\t\t\t\t\t\tdbias[o] += d\n\t\t\t\t\t}\n\t\t\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t\t\tdwrow[i] += inpBt[i] * d\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}(o)\n\t}\n\twg.Wait()\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#matmulbackward-1","position":93},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"encoderBackward"},"type":"lvl2","url":"/projects/the-interactive-transformer/interactive-transformer#encoderbackward","position":94},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl2":"encoderBackward"},"content":"encoderBackward calculates gradients during backpropagation\nParameters:\n\ndwte: gradients with respect to word embeddings (wte)\n\ndwpe: gradients with respect to positional embeddings (wpe)\n\ndout: the gradient to apply to dwte and dwpe\n\ninp: input tokens (ids that refer to indexes within wte)\n\nB: batch size\n\nT: sequence length (number of time steps)\n\nC: embedding dimension (number of features)\n\nback to top\n\n\nfunc encoderBackward(dwte, dwpe []float32, dout []float32, inp []int32, B, T, C int) {\n\t// Iterate over the batch and time steps\n\tfor b := 0; b < B; b++ {\n\t\tfor t := 0; t < T; t++ {\n\t\t\t// Calculate offsets for indexing\n\t\t\tdoutBTOffset := b*T*C + t*C\n\t\t\tix := inp[b*T+t]              // Get the input token id\n\t\t\tdwteIxOffset := ix * int32(C) // Calculate the offset for dwte\n\t\t\tdwpeTOffset := t * C          // Calculate the offset for dwpe\n\n\t\t\t// Iterate over the embedding dimension and apply computations\n\t\t\tfor i := 0; i < C; i++ {\n\t\t\t\t// Get the gradient value from dout\n\t\t\t\td := dout[doutBTOffset+i]\n\t\t\t\t// Update the gradients for word embeddings (dwte) and positional embeddings (dwpe)\n\t\t\t\tdwte[dwteIxOffset+int32(i)] += d\n\t\t\t\tdwpe[dwpeTOffset+i] += d\n\t\t\t}\n\t\t}\n\t}\n}\n\n\nfunc (model *GPT2) ZeroGradient() {\n\tfor i := range model.GradsActs.Memory {\n\t\tmodel.GradsActs.Memory[i] = 0.0\n\t}\n\tfor i := range model.Grads.Memory {\n\t\tmodel.Grads.Memory[i] = 0.0\n\t}\n}\n\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#encoderbackward","position":95},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Optimiser","lvl2":"encoderBackward"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#optimiser","position":96},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Optimiser","lvl2":"encoderBackward"},"content":"The optimiser implementation keeps track of the weights that are being changed, and how fast they’re being changed.\n\nMost neural network back propagation algorithms use AdamW, which is a weight-decay ontop of the Adam optimiser.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#optimiser","position":97},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl3":"Optimiser","lvl2":"encoderBackward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-6","position":98},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl3":"Optimiser","lvl2":"encoderBackward"},"content":"Adam: A Method for Stochastic Optimization - Introduced the Adam optimiser.\n\nDECOUPLED WEIGHT DECAY REGULARIZATION - Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.\n\nback to top\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-6","position":99},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Optimiser","lvl2":"encoderBackward"},"type":"lvl3","url":"/projects/the-interactive-transformer/interactive-transformer#optimiser-1","position":100},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl3":"Optimiser","lvl2":"encoderBackward"},"content":"The optimiser implementation keeps track of the weights that are being changed, and how fast they’re being changed.\n\nMost neural network back propagation algorithms use AdamW, which is a weight-decay ontop of the Adam optimiser.","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#optimiser-1","position":101},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl3":"Optimiser","lvl2":"encoderBackward"},"type":"lvl4","url":"/projects/the-interactive-transformer/interactive-transformer#papers-7","position":102},{"hierarchy":{"lvl1":"The Interactive Transformer","lvl4":"Papers","lvl3":"Optimiser","lvl2":"encoderBackward"},"content":"Adam: A Method for Stochastic Optimization - Introduced the Adam optimiser.\n\nDECOUPLED WEIGHT DECAY REGULARIZATION - Introduces AdamW optimiser used in the first transformer. Adam with weight where weight increases as time goes on.\n\nback to top\n\nfunc (model *GPT2) Update(learningRate, beta1, beta2, eps, weightDecay float32, t int) {\n\t// Lazy memory allocation\n\tif model.MMemory == nil {\n\t\tmodel.MMemory = make([]float32, model.Params.Len())\n\t\tmodel.VMemory = make([]float32, model.Params.Len())\n\t}\n\t// Parameter updates\n\tfor i := 0; i < model.Params.Len(); i++ {\n\t\tparameter := model.Params.Memory[i]\n\t\tgradient := model.Grads.Memory[i]\n\t\t// Momentum update\n\t\tm := beta1*model.MMemory[i] + (1.0-beta1)*gradient\n\t\t// RMSprop update\n\t\tv := beta2*model.VMemory[i] + (1.0-beta2)*gradient*gradient\n\t\t// Bias correction\n\t\tmHat := m / (1.0 - Pow(beta1, float32(t)))\n\t\tvHat := v / (1.0 - Pow(beta2, float32(t)))\n\t\t// Parameter update\n\t\tmodel.MMemory[i] = m\n\t\tmodel.VMemory[i] = v\n\t\tmodel.Params.Memory[i] -= learningRate * (mHat/(Sqrt(vHat)+eps) + weightDecay*parameter)\n\t}\n}\n\n\n\nfunc (model *GPT2) Backward() error {\n\t//// double check we forwarded previously, with targets\n\tif model.MeanLoss == -1.0 {\n\t\treturn errors.New(\"error: must forward with targets before backward\")\n\t}\n\t// lazily allocate the memory for gradients of the weights and activations, if needed\n\t// convenience shortcuts\n\tB, T, V, L, NH, C := model.B, model.T, model.Config.V, model.Config.L, model.Config.NH, model.Config.C\n\tif len(model.Grads.Memory) == 0 {\n\t\tmodel.Grads.Init(V, C, model.Config.MaxSeqLen, L)\n\t\tmodel.GradsActs.Init(B, C, T, L, NH, V)\n\t\tmodel.ZeroGradient()\n\t}\n\t// backward pass\n\tparams, grads, acts, gradsActs := model.Params, model.Grads, model.Acts, model.GradsActs\n\t// we kick off the chain by filling in dlosses with 1.0f/(B*T), to get the mean loss\n\tdlossMean := 1.0 / float32(B*T)\n\tfor i := range gradsActs.Losses.data {\n\t\tgradsActs.Losses.data[i] = dlossMean\n\t}\n\tcrossentropySoftmaxBackward(gradsActs.Logits.data, gradsActs.Losses.data, acts.Probabilities.data, model.Targets, B, T, V)\n\tmatmulBackward(gradsActs.LayerNormFinal.data, grads.WordTokEmbed.data, nil, gradsActs.Logits.data, acts.LayerNormFinal.data, params.WordTokEmbed.data, B, T, C, V)\n\tresidual := acts.Residual3.data[(L-1)*B*T*C:]       // last layer's residual\n\tdresidual := gradsActs.Residual3.data[(L-1)*B*T*C:] // write to last layer's residual\n\tlayernormBackward(dresidual, grads.LayerFinNormW.data, grads.LayerFinNormB.data, gradsActs.LayerNormFinal.data, residual, params.LayerFinNormW.data, acts.LayerNormFinalMean.data, acts.LayerNormFinalStd.data, B, T, C)\n\tfor l := L - 1; l >= 0; l-- {\n\t\tif l == 0 {\n\t\t\tresidual = acts.Encoded.data\n\t\t\tdresidual = gradsActs.Encoded.data\n\t\t} else {\n\t\t\tresidual = acts.Residual3.data[(l-1)*B*T*C:]\n\t\t\tdresidual = gradsActs.Residual3.data[(l-1)*B*T*C:]\n\t\t}\n\n\t\t// Assuming you have a 'params' variable of your ParameterTensors type\n\t\tl_ln1w := params.LayerNorm1W.data[l*C:]\n\t\tl_qkvw := params.QueryKeyValW.data[l*3*C*C:]\n\t\tl_attprojw := params.AttProjW.data[l*C*C:]\n\t\tl_ln2w := params.Layer2NormW.data[l*C:]\n\t\tl_fcw := params.FeedFwdW.data[l*4*C*C:]\n\t\tl_fcprojw := params.FeedFwdProjW.data[l*C*4*C:]\n\t\t// Gradients of weights\n\t\tdl_ln1w := grads.LayerNorm1W.data[l*C:]\n\t\tdl_ln1b := grads.LayerNorm1B.data[l*C:]\n\t\tdl_qkvw := grads.QueryKeyValW.data[l*3*C*C:]\n\t\tdl_qkvb := grads.QueryKeyValB.data[l*3*C:]\n\t\tdl_attprojw := grads.AttProjW.data[l*C*C:]\n\t\tdl_attprojb := grads.AttProjB.data[l*C:]\n\t\tdl_ln2w := grads.Layer2NormW.data[l*C:]\n\t\tdl_ln2b := grads.Layer2NormB.data[l*C:]\n\t\tdl_fcw := grads.FeedFwdW.data[l*4*C*C:]\n\t\tdl_fcb := grads.FeedFwdB.data[l*4*C:]\n\t\tdl_fcprojw := grads.FeedFwdProjW.data[l*C*4*C:]\n\t\tdl_fcprojb := grads.FeedFwdProjB.data[l*C:]\n\t\t// Activations\n\t\tl_ln1 := acts.Layer1Act.data[l*B*T*C:]\n\t\tl_ln1_mean := acts.LayerNorm1Mean.data[l*B*T:]\n\t\tl_ln1_rstd := acts.LayerNorm1Rstd.data[l*B*T:]\n\t\tl_qkv := acts.QueryKeyVal.data[l*B*T*3*C:]\n\t\tl_atty := acts.AttentionInter.data[l*B*T*C:]\n\t\tl_att := acts.Attention.data[l*B*NH*T*T:]\n\t\tl_residual2 := acts.Residual2.data[l*B*T*C:]\n\t\tl_ln2 := acts.LayerNorm2Act.data[l*B*T*C:]\n\t\tl_ln2_mean := acts.LayerNorm2Mean.data[l*B*T:]\n\t\tl_ln2_rstd := acts.LayerNorm2Rstd.data[l*B*T:]\n\t\tl_fch := acts.FeedForward.data[l*B*T*4*C:]\n\t\tl_fch_gelu := acts.FeedForwardGelu.data[l*B*T*4*C:]\n\n\t\tdl_ln1 := gradsActs.Layer1Act.data[l*B*T*C:]\n\t\tdl_qkv := gradsActs.QueryKeyVal.data[l*B*T*3*C:]\n\t\tdl_atty := gradsActs.AttentionInter.data[l*B*T*C:]\n\t\tdl_preatt := gradsActs.PreAttention.data[l*B*NH*T*T:]\n\t\tdl_att := gradsActs.Attention.data[l*B*NH*T*T:]\n\t\tdl_attproj := gradsActs.AttentionProj.data[l*B*T*C:]\n\t\tdl_residual2 := gradsActs.Residual2.data[l*B*T*C:]\n\t\tdl_ln2 := gradsActs.LayerNorm2Act.data[l*B*T*C:]\n\t\tdl_fch := gradsActs.FeedForward.data[l*B*T*4*C:]\n\t\tdl_fch_gelu := gradsActs.FeedForwardGelu.data[l*B*T*4*C:]\n\t\tdl_fcproj := gradsActs.FeedForwardProj.data[l*B*T*C:]\n\t\tdl_residual3 := gradsActs.Residual3.data[l*B*T*C:]\n\t\tresidualBackward(dl_residual2, dl_fcproj, dl_residual3, B*T*C)\n\t\tmatmulBackward(dl_fch_gelu, dl_fcprojw, dl_fcprojb, dl_fcproj, l_fch_gelu, l_fcprojw, B, T, 4*C, C)\n\t\tgeluBackward(dl_fch, l_fch, dl_fch_gelu, B*T*4*C)\n\t\tmatmulBackward(dl_ln2, dl_fcw, dl_fcb, dl_fch, l_ln2, l_fcw, B, T, C, 4*C)\n\t\tlayernormBackward(dl_residual2, dl_ln2w, dl_ln2b, dl_ln2, l_residual2, l_ln2w, l_ln2_mean, l_ln2_rstd, B, T, C)\n\t\tresidualBackward(dresidual, dl_attproj, dl_residual2, B*T*C)\n\t\tmatmulBackward(dl_atty, dl_attprojw, dl_attprojb, dl_attproj, l_atty, l_attprojw, B, T, C, C)\n\t\tattentionBackward(dl_qkv, dl_preatt, dl_att, dl_atty, l_qkv, l_att, B, T, C, NH)\n\t\tmatmulBackward(dl_ln1, dl_qkvw, dl_qkvb, dl_qkv, l_ln1, l_qkvw, B, T, C, 3*C)\n\t\tlayernormBackward(dresidual, dl_ln1w, dl_ln1b, dl_ln1, residual, l_ln1w, l_ln1_mean, l_ln1_rstd, B, T, C)\n\t}\n\t// Here we want to apply our gradients to our encoded data.\n\tencoderBackward(grads.WordTokEmbed.data, grads.WordPosEmbed.data, gradsActs.Encoded.data, model.Inputs, B, T, C)\n\treturn nil\n}\n\n\n\nfunc (model *GPT2) Train(valDataloader, trainDataloader *DataLoader, B, T int) error {\n\tfmt.Printf(\"train dataset num_batches: %d\\n\", valDataloader.NumBatches)\n\tconst genMaxLength, valNumBatches = 20, 3\n\tfor step := 0; step <= 3; step++ {\n\t\tif step%1 == 0 {\n\t\t\tvar valLoss float32\n\t\t\tvalDataloader.Reset()\n\t\t\tfor i := 0; i < valNumBatches; i++ {\n\t\t\t\tinput, target, err := valDataloader.NextBatch()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tmodel.Forward(input, target, B, T)\n\t\t\t\tvalLoss += model.MeanLoss\n\t\t\t}\n\t\t\tvalLoss /= float32(valNumBatches)\n\t\t\tfmt.Printf(\"val loss %f\\n\", valLoss)\n\t\t}\n\t\t// do a training step\n\t\tstart := time.Now()\n\t\tinput, targets, err := trainDataloader.NextBatch()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmodel.Forward(input, targets, B, T)\n\t\tmodel.ZeroGradient()\n\t\tmodel.Backward()\n\t\tmodel.Update(1e-4, 0.9, 0.999, 1e-8, 0.0, step+1)\n\t\tfmt.Printf(\"step %d: train loss %f (took %v ms)\\n\", step, model.MeanLoss, time.Since(start))\n\t}\n\treturn nil\n}\n\n%main\nmodel, err := LoadGPT2Model(\"./gpt2_124M.bin\", \"./gpt2_tokenizer.bin\")\nif err != nil {\n    log.Fatal(err)\n}\nB, T := 4, 64\ntrainDataloader, err := NewDataLoader(\"./TinyStories_train.bin\", B, T)\nif err != nil {\n    log.Fatal(err)\n}\nfmt.Printf(\"train dataset num_batches: %d\\n\", trainDataloader.NumBatches)\nvalDataloader, err := NewDataLoader(\"./TinyStories_val.bin\", B, T)\nif err != nil {\n    log.Fatal(err)\n}\nif err := model.Train(valDataloader, trainDataloader, B, T); err != nil {\n    log.Fatal(err)\n}\n\n","type":"content","url":"/projects/the-interactive-transformer/interactive-transformer#papers-7","position":103}]}