<!DOCTYPE html><html lang="en" class="" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Papers - ML Notes</title><meta property="og:title" content="Papers - ML Notes"/><meta name="generator" content="mystmd"/><meta name="keywords" content=""/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/build/_assets/app-H3NBUYVS.css"/><link rel="stylesheet" href="/build/_assets/thebe-core-VKVHG5VY.css"/><link rel="stylesheet" href="/myst-theme.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/><script>
  const savedTheme = localStorage.getItem("myst:theme");
  const theme = window.matchMedia("(prefers-color-scheme: light)").matches ? 'light' : 'dark';
  const classes = document.documentElement.classList;
  const hasAnyTheme = classes.contains('light') || classes.contains('dark');
  if (!hasAnyTheme) classes.add(savedTheme ?? theme);
</script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 sticky w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-nowrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center shrink-0"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 0 1 3.75 6h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 6.75ZM3 12a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75A.75.75 0 0 1 3 12Zm0 5.25a.75.75 0 0 1 .75-.75h16.5a.75.75 0 0 1 0 1.5H3.75a.75.75 0 0 1-.75-.75Z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/"><span class="text-md sm:text-xl tracking-tight sm:mr-5">ML Notes</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R3iop:" data-state="closed" class="flex items-center h-10 aspect-square sm:w-64 text-left text-gray-400 border border-gray-300 dark:border-gray-600 rounded-lg bg-gray-50 dark:bg-gray-700 hover:ring-blue-500 dark:hover:ring-blue-500 hover:border-blue-500 dark:hover:border-blue-500"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="p-2.5 h-10 w-10 aspect-square"><path fill-rule="evenodd" d="M10.5 3.75a6.75 6.75 0 1 0 0 13.5 6.75 6.75 0 0 0 0-13.5ZM2.25 10.5a8.25 8.25 0 1 1 14.59 5.28l4.69 4.69a.75.75 0 1 1-1.06 1.06l-4.69-4.69A8.25 8.25 0 0 1 2.25 10.5Z" clip-rule="evenodd"></path></svg><span class="hidden sm:block grow">Search</span><div aria-hidden="true" class="items-center hidden mx-1 font-mono text-sm text-gray-400 sm:flex gap-x-1"><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none hide-mac">CTRL</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none show-mac">⌘</kbd><kbd class="px-2 py-1 border border-gray-300 dark:border-gray-600 rounded-md shadow-[0px_2px_0px_0px_rgba(0,0,0,0.08)] dark:shadow-none ">K</kbd><script>
;(() => {
const script = document.currentScript;
const root = script.parentElement;

const isMac = /mac/i.test(
      window.navigator.userAgentData?.platform ?? window.navigator.userAgent,
    );
root.querySelectorAll(".hide-mac").forEach(node => {node.classList.add(isMac ? "hidden" : "block")});
root.querySelectorAll(".show-mac").forEach(node => {node.classList.add(!isMac ? "hidden" : "block")});
})()</script></div></button><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode." aria-label="Toggle theme between light and dark mode."><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button><div class="block sm:hidden"></div><div class="hidden sm:block"></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><div class="flex-grow py-6 overflow-y-auto"><nav aria-label="Navigation" class="overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px] lg:hidden"><div class="w-full px-1 dark:text-white"></div></nav><div class="my-3 border-b-2 lg:hidden"></div><nav aria-label="Table of Contents" class="flex-grow overflow-y-hidden transition-opacity ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="ML Notes" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/">ML Notes</a><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><a title="Projects" class="block break-words focus:outline outline-blue-200 outline-2 rounded py-2 grow" href="/projects">Projects</a><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rbd8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rbd8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="closed" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none hover:bg-slate-300/30"><a title="Glossary" class="block break-words focus:outline outline-blue-200 outline-2 rounded py-2 grow" href="/glossary">Glossary</a><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rfd8p:" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="closed" id="radix-:Rfd8p:" hidden="" class="pl-3 pr-[2px] collapsible-content"></div></div><div data-state="open" class="w-full"><div class="flex flex-row w-full gap-2 px-2 my-1 text-left rounded-lg outline-none bg-blue-300/30"><a title="Papers" aria-current="page" class="block break-words focus:outline outline-blue-200 outline-2 rounded py-2 grow font-semibold text-blue-800 dark:text-blue-200 active" href="/paper-notes">Papers</a><button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" aria-label="Open Folder" type="button" aria-controls="radix-:Rjd8p:" aria-expanded="true" data-state="open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="open" id="radix-:Rjd8p:" class="pl-3 pr-[2px] collapsible-content"><a title="Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/paper-notes/perplexity-based-data-pruning">Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</a></div></div></div></nav></div><div class="flex-none py-6 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><article class="article content article-grid grid-gap"><main class="article-grid subgrid-gap col-screen"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="mb-8 pt-9"><div class="flex items-center h-6 mb-5 text-sm font-light"><div class="flex-grow"></div><a href="https://github.com/joshcarp/ml-notes" title="GitHub Repository: joshcarp/ml-notes" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rd4fop:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.25rem" height="1.25rem"><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 0 0 5.25 21h13.5A2.25 2.25 0 0 0 21 18.75V16.5M16.5 12 12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Papers</h1></div><div class="block my-10 lg:sticky lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:60px"><nav></nav></div><div id="skip-to-article"></div><div id="izkRFT27q3" class="relative group/block article-grid subgrid-gap col-screen"><p>A collection of papers with summaries and quick access links.</p><h2 id="paper-notes" class="relative group"><span class="heading-text">Paper Notes</span><a class="no-underline text-inherit hover:text-inherit px-2 font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#paper-notes" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><figure id="O8xTiiyr0C" class="fig-table"><table><tbody><tr><th class="">Title</th><th class="">Tags</th><th class="">Full Notes</th></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2411.07191" target="_blank" rel="noreferrer" class="hover-link">The Super Weight in Large Language Models</a></cite></td><td class="">Model internals</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2410.02725" target="_blank" rel="noreferrer" class="hover-link">Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation</a></cite></td><td class=""></td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2404.03592" target="_blank" rel="noreferrer" class="hover-link">ReFT: Representation Finetuning for Language Models</a></cite></td><td class="">Fine-tuning, Model representations</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://doi.org/10.5555/2627435.2670313" rel="noreferrer">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></td><td class="">Model performance, Optimization, Model architecture</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2402.06111" target="_blank" rel="noreferrer" class="hover-link">Observation-based unit test generation at Meta</a></cite></td><td class="">Automated testing, Software engineering</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2403.20327" target="_blank" rel="noreferrer" class="hover-link">Gecko: Versatile Text Embeddings Distilled from Large Language Models</a></cite></td><td class="">Embeddings, Model distillation</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2210.07128" target="_blank" rel="noreferrer" class="hover-link">Language Models of Code are Few-Shot Commonsense Learners</a></cite></td><td class="">Code models, Transfer learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2407.10969" target="_blank" rel="noreferrer" class="hover-link">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></cite></td><td class="">Efficiency, Model performance</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2207.01780" target="_blank" rel="noreferrer" class="hover-link">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</a></cite></td><td class="">Code generation, Reinforcement learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2405.20541" target="_blank" rel="noreferrer" class="hover-link">Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</a></cite></td><td class="">Data pruning, Perplexity</td><td class=""><a href="/paper-notes/perplexity-based-data-pruning">Details</a></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2402.17764" target="_blank" rel="noreferrer" class="hover-link">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></cite></td><td class="">Hardware optimization, Model compression</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2305.17493" target="_blank" rel="noreferrer" class="hover-link">The Curse of Recursion: Training on Generated Data Makes Models Forget</a></cite></td><td class="">Model collapse, Training data</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" rel="noreferrer">Chain of thought prompting</a></td><td class="">Prompting strategies</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2404.07143" target="_blank" rel="noreferrer" class="hover-link">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</a></cite></td><td class="">Attention mechanisms, Context window</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.07496" target="_blank" rel="noreferrer" class="hover-link">TextGrad</a></cite></td><td class="">Agent systems, Text optimization</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.02528" target="_blank" rel="noreferrer" class="hover-link">Scalable MatMul-free Language Modeling</a></cite></td><td class="">Attention mechanisms, Model efficiency</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1712.00676" target="_blank" rel="noreferrer" class="hover-link">Will humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?</a></cite></td><td class="">AI in software development, Future of coding</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2311.17035" target="_blank" rel="noreferrer" class="hover-link">Scalable Extraction of Training Data from (Production) Language Models</a></cite></td><td class="">Data accumulation, Model performance, Curated datasets</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2305.07759" target="_blank" rel="noreferrer" class="hover-link">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></cite></td><td class="">Dataset creation, Small language models</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2311.12983" target="_blank" rel="noreferrer" class="hover-link">GAIA: A Benchmark for General AI Assistants</a></cite></td><td class="">AI assistants, Benchmarking</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://www.anthropic.com/news/mapping-mind-language-model" rel="noreferrer">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></td><td class="">Feature extraction, Interpretability</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.04692" target="_blank" rel="noreferrer" class="hover-link">Mixture-of-Agents Enhances Large Language Model Capabilities</a></cite></td><td class="">Model performance, Multi-agent systems</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2311.05884" target="_blank" rel="noreferrer" class="hover-link">Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems</a></cite></td><td class="">Transformers, Model architecture, Model performance</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2307.11760" target="_blank" rel="noreferrer" class="hover-link">Large Language Models Understand and Can Be Enhanced by Emotional Stimuli</a></cite></td><td class="">emotional stimuli, Model behavior</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2402.09171" target="_blank" rel="noreferrer" class="hover-link">Automated Unit Test Improvement using Large Language Models at Meta</a></cite></td><td class="">Automated testing, Software engineering</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2404.01413" target="_blank" rel="noreferrer" class="hover-link">Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</a></cite></td><td class="">Data accumulation, Model collapse prevention</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2102.04518" target="_blank" rel="noreferrer" class="hover-link">A\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks</a></cite></td><td class="">Reinforcement learning, Search algorithms</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2402.06196" target="_blank" rel="noreferrer" class="hover-link">Large Language Models: A Survey</a></cite></td><td class="">LLM capabilities, Survey</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2402.14433" target="_blank" rel="noreferrer" class="hover-link">A Language Model’s Guide Through Latent Space</a></cite></td><td class="">Interpretability, Latent space</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2205.05124" target="_blank" rel="noreferrer" class="hover-link">Extracting Latent Steering Vectors from Pretrained Language Models</a></cite></td><td class="">Interpretability, Latent space</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://www.anthropic.com/research/many-shot-jailbreaking" rel="noreferrer">Many-shot jailbreaking</a></td><td class="">Jailbreaking, Model safety</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1910.10683" target="_blank" rel="noreferrer" class="hover-link">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></cite></td><td class="">Transfer learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2308.10248" target="_blank" rel="noreferrer" class="hover-link">Activation Addition: Steering Language Models Without Optimization</a></cite></td><td class="">Activation manipulation, Model steering</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2107.03374" target="_blank" rel="noreferrer" class="hover-link">Evaluating Large Language Models Trained on Code</a></cite></td><td class="">Code generation, Model evaluation</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.02543" target="_blank" rel="noreferrer" class="hover-link">To Believe or Not to Believe Your LLM</a></cite></td><td class="">Hallucination detection, Uncertainty quantification</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.04692" target="_blank" rel="noreferrer" class="hover-link">Mixture of Agents</a></cite></td><td class="">Multi-agent systems, Prompting</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2404.14619" target="_blank" rel="noreferrer" class="hover-link">OpenELM: An Efficient Language Model Family with Open Training and Inference Framework</a></cite></td><td class="">Efficiency, Model architecture</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1706.03741" target="_blank" rel="noreferrer" class="hover-link">Deep Reinforcement Learning from Human Preferences</a></cite></td><td class="">human feedback, Reinforcement learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2307.08925" target="_blank" rel="noreferrer" class="hover-link">Federated Large Language Model: A Position Paper</a></cite></td><td class="">Distributed training, Federated learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2212.02508" target="_blank" rel="noreferrer" class="hover-link">MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning</a></cite></td><td class="">Music representation, Self-supervised learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2302.13971" target="_blank" rel="noreferrer" class="hover-link">LLaMA: Open and Efficient Foundation Language Models</a></cite></td><td class="">Model architecture, Open-source LLMs</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/" rel="noreferrer">Phi1: Textbooks Are All You Need</a></td><td class="">Curated datasets, Model efficiency</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1607.06450" target="_blank" rel="noreferrer" class="hover-link">Layer Normalization</a></cite></td><td class="">Model internals, Optimization</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1706.03762" target="_blank" rel="noreferrer" class="hover-link">Attention Is All You Need</a></cite></td><td class="">OG papers, Transformers</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2406.09412" target="_blank" rel="noreferrer" class="hover-link">Explore the Limits of Omni-modal Pretraining at Scale</a></cite></td><td class="">Multi-modal models, Pretraining</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1606.08415" target="_blank" rel="noreferrer" class="hover-link">Gaussian Error Linear Units (GELUs)</a></cite></td><td class="">Activation functions, Model internals</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2401.09796" target="_blank" rel="noreferrer" class="hover-link">A Fast, Performant, Secure Distributed Training Framework For Large Language Model</a></cite></td><td class="">Distributed training, Security</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.1910.01108" target="_blank" rel="noreferrer" class="hover-link">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a></cite></td><td class="">Efficiency, Model distillation</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noreferrer">Improving Language Understanding by Generative Pre-Training</a></td><td class="">OG papers, Pre-training</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2203.02155" target="_blank" rel="noreferrer" class="hover-link">Training language models to follow instructions with human feedback</a></cite></td><td class="">Instruction following, Reinforcement learning</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2307.09288" target="_blank" rel="noreferrer" class="hover-link">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></cite></td><td class="">Fine-tuning, Open-source LLMs</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.48550/ARXIV.2104.09864" target="_blank" rel="noreferrer" class="hover-link">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></cite></td><td class="">Embeddings, Model architecture</td><td class=""></td></tr><tr><td class=""><a target="_blank" href="https://www.nature.com/articles/s42256-023-00748-9" rel="noreferrer">Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence</a></td><td class="">Biological Brains</td><td class=""></td></tr><tr><td class=""><cite data-state="closed"><a href="https://doi.org/10.1111/tops.12737" target="_blank" rel="noreferrer" class="hover-link">Understanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library</a></cite></td><td class="">Biological Brains</td><td class=""></td></tr></tbody></table></figure></div><div></div><section id="references" class="article-grid subgrid-gap col-screen"><div><button class="float-right p-1 px-2 text-xs border rounded hover:border-blue-500 dark:hover:border-blue-400">Show All</button><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#references" title="Link to References" aria-label="Link to References">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-https://doi.org/10.48550/arxiv.2411.07191">Yu, M., Wang, D., Shan, Q., Reed, C., & Wan, A. (2024). <i>The Super Weight in Large Language Models</i>. arXiv. <a target="_blank" rel="noreferrer" href="https://doi.org/10.48550/ARXIV.2411.07191">10.48550/ARXIV.2411.07191</a></li><li class="break-words" id="cite-https://doi.org/10.48550/arxiv.2410.02725">Manvi, R., Singh, A., & Ermon, S. (2024). <i>Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation</i>. arXiv. <a target="_blank" rel="noreferrer" href="https://doi.org/10.48550/ARXIV.2410.02725">10.48550/ARXIV.2410.02725</a></li><li class="break-words" id="cite-https://doi.org/10.48550/arxiv.2404.03592">Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., & Potts, C. (2024). <i>ReFT: Representation Finetuning for Language Models</i>. arXiv. <a target="_blank" rel="noreferrer" href="https://doi.org/10.48550/ARXIV.2404.03592">10.48550/ARXIV.2404.03592</a></li><li class="break-words" id="cite-https://doi.org/10.48550/arxiv.2402.06111">Alshahwan, N., Harman, M., Marginean, A., Tal, R., & Wang, E. (2024). <i>Observation-based unit test generation at Meta</i>. arXiv. <a target="_blank" rel="noreferrer" href="https://doi.org/10.48550/ARXIV.2402.06111">10.48550/ARXIV.2402.06111</a></li><li class="break-words" id="cite-https://doi.org/10.48550/arxiv.2403.20327">Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J. R., Hui, K., Boratko, M., Kapadia, R., Ding, W., Luan, Y., Duddu, S. M. K., Abrego, G. H., Shi, W., Gupta, N., Kusupati, A., Jain, P., Jonnalagadda, S. R., Chang, M.-W., & Naim, I. (2024). <i>Gecko: Versatile Text Embeddings Distilled from Large Language Models</i>. arXiv. <a target="_blank" rel="noreferrer" href="https://doi.org/10.48550/ARXIV.2403.20327">10.48550/ARXIV.2403.20327</a></li><li class="text-center list-none"><button class="p-2 border rounded hover:border-blue-500 dark:hover:border-blue-400">Show all 46 references</button></li></ol></div></section><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/glossary/transformer-1"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">ML Notes</div>The Transformer</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/paper-notes/perplexity-based-data-pruning"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">ML Notes</div>Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5 21 12m0 0-7.5 7.5M21 12H3"></path></svg></div></a></div></main></article><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script><link rel="modulepreload" href="/build/entry.client-UNPC4GT3.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCTKKCIL.js"/><link rel="modulepreload" href="/build/_shared/chunk-UAI5KRM7.js"/><link rel="modulepreload" href="/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/build/_shared/chunk-JLDGA2DL.js"/><link rel="modulepreload" href="/build/_shared/chunk-YAIQ7LUU.js"/><link rel="modulepreload" href="/build/_shared/chunk-OCWQY3HK.js"/><link rel="modulepreload" href="/build/_shared/chunk-ZQWAZXET.js"/><link rel="modulepreload" href="/build/_shared/chunk-HYMQ7M2K.js"/><link rel="modulepreload" href="/build/_shared/chunk-3CVK3PYF.js"/><link rel="modulepreload" href="/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/build/_shared/chunk-IQBJE7PC.js"/><link rel="modulepreload" href="/build/_shared/chunk-5CFTM6YW.js"/><link rel="modulepreload" href="/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/build/root-HROFNPGU.js"/><link rel="modulepreload" href="/build/_shared/chunk-N544LW6X.js"/><link rel="modulepreload" href="/build/routes/$-WNZNXUO2.js"/><script>window.__remixContext = {"url":"/paper-notes","state":{"loaderData":{"root":{"config":{"title":"ML Notes","options":{"logo_text":"ML Notes","folders":true},"myst":"1.3.18","nav":[],"actions":[],"projects":[{"title":"ML Notes","github":"https://github.com/joshcarp/ml-notes","id":"af717f67-a8e5-4337-ac20-caf05da70397","thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"https://github.com/joshcarp/ml-notes","ref":"HEAD"}},"toc":[{"file":"intro.md"},{"children":[{"file":"projects/cognition.to.md"},{"file":"projects/the-interactive-transformer/interactive-transformer.ipynb","title":"The Interactive Transformer"}],"file":"projects/index.md"},{"children":[{"file":"glossary/dropout.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"},{"file":"glossary/dropout.md"},{"file":"glossary/feed_forward_network.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/lora.md"},{"file":"glossary/model_collapse.md"},{"file":"glossary/poly_semanticity.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/rag.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/residual_stream.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"}],"file":"glossary/index.md"},{"children":[{"file":"paper-notes/perplexity-based-data-pruning.md"}],"file":"paper-notes/index.md","title":"Paper Notes"}],"exports":[],"bibliography":[],"index":"intro","pages":[{"slug":"projects.index","title":"Projects","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"projects.cognition-to","title":"CognitionTO Community","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.index","title":"Glossary","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"glossary.dropout","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.dropout-1","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.feed-forward-network","title":"Feed-Forward Network","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning-1","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.lora","title":"LoRA (Low-Rank Adaptation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.model-collapse","title":"Model Collapse","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.poly-semanticity","title":"Poly-semanticity","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training-1","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.rag","title":"RAG (Retrieval-Augmented Generation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning-1","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.residual-stream","title":"Residual Stream","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward-1","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer-1","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"paper-notes.index","title":"Papers","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"paper-notes.perplexity-based-data-pruning","title":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"CONTENT_CDN_PORT":"3101","MODE":"static"},"routes/$":{"config":{"title":"ML Notes","options":{"logo_text":"ML Notes","folders":true},"myst":"1.3.18","nav":[],"actions":[],"projects":[{"title":"ML Notes","github":"https://github.com/joshcarp/ml-notes","id":"af717f67-a8e5-4337-ac20-caf05da70397","thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"https://github.com/joshcarp/ml-notes","ref":"HEAD"}},"toc":[{"file":"intro.md"},{"children":[{"file":"projects/cognition.to.md"},{"file":"projects/the-interactive-transformer/interactive-transformer.ipynb","title":"The Interactive Transformer"}],"file":"projects/index.md"},{"children":[{"file":"glossary/dropout.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"},{"file":"glossary/dropout.md"},{"file":"glossary/feed_forward_network.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/lora.md"},{"file":"glossary/model_collapse.md"},{"file":"glossary/poly_semanticity.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/rag.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/residual_stream.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"}],"file":"glossary/index.md"},{"children":[{"file":"paper-notes/perplexity-based-data-pruning.md"}],"file":"paper-notes/index.md","title":"Paper Notes"}],"exports":[],"bibliography":[],"index":"intro","pages":[{"slug":"projects.index","title":"Projects","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"projects.cognition-to","title":"CognitionTO Community","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.index","title":"Glossary","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"glossary.dropout","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.dropout-1","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.feed-forward-network","title":"Feed-Forward Network","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning-1","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.lora","title":"LoRA (Low-Rank Adaptation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.model-collapse","title":"Model Collapse","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.poly-semanticity","title":"Poly-semanticity","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training-1","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.rag","title":"RAG (Retrieval-Augmented Generation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning-1","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.residual-stream","title":"Residual Stream","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward-1","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer-1","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"paper-notes.index","title":"Papers","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"paper-notes.perplexity-based-data-pruning","title":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}]},"page":{"kind":"Article","sha256":"de6f13421b9e894a3495f69d13e261da2b558605fa45aba906e17ebef11a1aea","slug":"paper-notes.index","location":"/paper-notes/index.md","dependencies":[],"frontmatter":{"title":"Papers","content_includes_title":false,"github":"https://github.com/joshcarp/ml-notes","exports":[{"format":"md","filename":"index.md","url":"/build/index-ab00802d9242182da19d48d67abc124d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"A collection of papers with summaries and quick access links.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"T6qc4QPiaQ"}],"key":"rWvp6sa3H1"},{"type":"heading","depth":2,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Paper Notes","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"tfVa1AM20q"}],"identifier":"paper-notes","label":"Paper Notes","html_id":"paper-notes","implicit":true,"key":"fcqw6TcGWj"},{"type":"container","kind":"table","children":[{"type":"table","children":[{"type":"tableRow","children":[{"type":"tableCell","header":true,"children":[{"type":"text","value":"Title","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"GzLi5Z9P34"}],"key":"vWvzb3hkLU"},{"type":"tableCell","header":true,"children":[{"type":"text","value":"Tags","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"vGG6UDomnu"}],"key":"aZ1JvFDI46"},{"type":"tableCell","header":true,"children":[{"type":"text","value":"Full Notes","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"PDYEZAU8aC"}],"key":"BogwkGCsPm"}],"key":"fXoH4vw8Eb"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2411.07191","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"The Super Weight in Large Language Models","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"UDQ1n5F3PH"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2411.07191","identifier":"https://doi.org/10.48550/arXiv.2411.07191","enumerator":"1","key":"F9dYDPlDCn"}],"key":"vhlgC9qKFl"},{"type":"tableCell","children":[{"type":"text","value":"Model internals","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"AG1BaCQgmK"}],"key":"OoozxhpoyL"},{"type":"tableCell","children":[],"key":"g6ilKYD3Ea"}],"key":"pY5CYgZAY4"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2410.02725","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"gmNssyESfK"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2410.02725","identifier":"https://doi.org/10.48550/arXiv.2410.02725","enumerator":"2","key":"Bs67mC9iN9"}],"key":"TL5U2kFUL3"},{"type":"tableCell","children":[],"key":"ml90mxvuJz"},{"type":"tableCell","children":[],"key":"HDfhMIKHs1"}],"key":"xrQpfcAx9n"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.03592","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"ReFT: Representation Finetuning for Language Models","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"zGOJL59BOO"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.03592","identifier":"https://doi.org/10.48550/arXiv.2404.03592","enumerator":"3","key":"kkeUUu1c2Z"}],"key":"FTJa8aPOnu"},{"type":"tableCell","children":[{"type":"text","value":"Fine-tuning, Model representations","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"jDu6zEVAx1"}],"key":"FnrpF6jIs0"},{"type":"tableCell","children":[],"key":"kWQYWzvv0b"}],"key":"eePUEWcZ8C"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://doi.org/10.5555/2627435.2670313","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Dropout: A Simple Way to Prevent Neural Networks from Overfitting","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"qZqHQTliDW"}],"urlSource":"https://dl.acm.org/doi/abs/10.5555/2627435.2670313","data":{"doi":"10.5555/2627435.2670313"},"internal":false,"protocol":"doi","key":"yMrNqhlRGQ"}],"key":"BbwVgy1sQr"},{"type":"tableCell","children":[{"type":"text","value":"Model performance, Optimization, Model architecture","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"v7p2Bnmvws"}],"key":"UKFlWgoJId"},{"type":"tableCell","children":[],"key":"G7fR7bwhi2"}],"key":"fncC6GMqpq"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.06111","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Observation-based unit test generation at Meta","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"BnB9o2aBVe"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.06111","identifier":"https://doi.org/10.48550/arXiv.2402.06111","enumerator":"4","key":"WHuJI8Lhlz"}],"key":"vh5nTJXuL6"},{"type":"tableCell","children":[{"type":"text","value":"Automated testing, Software engineering","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"HJ80FBhDum"}],"key":"bdf7qxnVfL"},{"type":"tableCell","children":[],"key":"beCjmbEpHD"}],"key":"n99sy6ajiT"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2403.20327","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"PWjO0FeGAP"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2403.20327","identifier":"https://doi.org/10.48550/arXiv.2403.20327","enumerator":"5","key":"sKdsWBPsv1"}],"key":"J4cuKVC0q3"},{"type":"tableCell","children":[{"type":"text","value":"Embeddings, Model distillation","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"LFGXM5ZqzV"}],"key":"IAxOADzMug"},{"type":"tableCell","children":[],"key":"ZElp0Xp8Ga"}],"key":"U1UtaogmlD"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2210.07128","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Language Models of Code are Few-Shot Commonsense Learners","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"Kvjr5NeC8M"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2210.07128","identifier":"https://doi.org/10.48550/arXiv.2210.07128","enumerator":"6","key":"SDN7iTO2IY"}],"key":"Httx9iVK11"},{"type":"tableCell","children":[{"type":"text","value":"Code models, Transfer learning","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"vfngGuhv9r"}],"key":"xqwCvRiVvW"},{"type":"tableCell","children":[],"key":"aTWoi4sK7M"}],"key":"VoMm0MQlYP"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2407.10969","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"P1kqtlmmEj"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2407.10969","identifier":"https://doi.org/10.48550/arXiv.2407.10969","enumerator":"7","key":"A0MPXSx8mS"}],"key":"j1OnXVTZkI"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model performance","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"FVvuqxdXYj"}],"key":"jz2ddDoQAJ"},{"type":"tableCell","children":[],"key":"Na7TyaeqMx"}],"key":"X7VCDqxajA"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2207.01780","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"hsIdQRwGZY"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2207.01780","identifier":"https://doi.org/10.48550/arXiv.2207.01780","enumerator":"8","key":"SCkjSKikkF"}],"key":"u5GYNlvioG"},{"type":"tableCell","children":[{"type":"text","value":"Code generation, Reinforcement learning","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"RMoklcupP0"}],"key":"HqVaJ47ya2"},{"type":"tableCell","children":[],"key":"GSuA1jotYQ"}],"key":"zaLyNpx1uf"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2405.20541","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"K1uX80K7Cd"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2405.20541","identifier":"https://doi.org/10.48550/arXiv.2405.20541","enumerator":"9","key":"ZNVjwsj754"}],"key":"qIi18ONjpJ"},{"type":"tableCell","children":[{"type":"text","value":"Data pruning, Perplexity","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"QdjAcZEW3t"}],"key":"AWrClcHCvW"},{"type":"tableCell","children":[{"type":"link","url":"/paper-notes/perplexity-based-data-pruning","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"Details","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"cQXIXRPFLY"}],"urlSource":"perplexity-based-data-pruning","dataUrl":"/paper-notes.perplexity-based-data-pruning.json","internal":true,"protocol":"file","key":"RJnPlNu4Hc"}],"key":"X3Px01B4Md"}],"key":"YEvDufzNnG"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.17764","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"P7uC5pcHLY"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.17764","identifier":"https://doi.org/10.48550/arXiv.2402.17764","enumerator":"10","key":"r2pWR2N2Yp"}],"key":"T9D9smu9Kf"},{"type":"tableCell","children":[{"type":"text","value":"Hardware optimization, Model compression","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"e7kNNy10k6"}],"key":"LBEZUjv1BS"},{"type":"tableCell","children":[],"key":"s5f0I8DE70"}],"key":"F8jyTOv14c"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2305.17493","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"The Curse of Recursion: Training on Generated Data Makes Models Forget","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"iTGTs13WOw"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2305.17493","identifier":"https://doi.org/10.48550/arXiv.2305.17493","enumerator":"11","key":"jkTymQfDYn"}],"key":"gV7C9WFrKN"},{"type":"tableCell","children":[{"type":"text","value":"Model collapse, Training data","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"hCdUpKKIHw"}],"key":"QPzgRNoFlp"},{"type":"tableCell","children":[],"key":"YYGgLgSYLS"}],"key":"qjYDuwIrhq"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"Chain of thought prompting","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"QsOw9L2mxk"}],"urlSource":"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html","key":"BqKrlVb0SS"}],"key":"tQaN4Gmf8z"},{"type":"tableCell","children":[{"type":"text","value":"Prompting strategies","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"D33pNkI0EA"}],"key":"i9vhIfkrQd"},{"type":"tableCell","children":[],"key":"NMNS9bOXG5"}],"key":"nhJlNxney4"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.07143","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"Hppv4CqiVL"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.07143","identifier":"https://doi.org/10.48550/arXiv.2404.07143","enumerator":"12","key":"olO8KyZw7q"}],"key":"bo9Y8zoQ1W"},{"type":"tableCell","children":[{"type":"text","value":"Attention mechanisms, Context window","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"duFOLedc3A"}],"key":"ovybQCJyNW"},{"type":"tableCell","children":[],"key":"Uq4Cf35PuA"}],"key":"UraHgK56TI"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.07496","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"TextGrad","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"zQ6yflWkF1"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.07496","identifier":"https://doi.org/10.48550/arXiv.2406.07496","enumerator":"13","key":"af679LQmBL"}],"key":"AfCihXJdke"},{"type":"tableCell","children":[{"type":"text","value":"Agent systems, Text optimization","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"XwvnK70Ncv"}],"key":"rZ2hRbCLU8"},{"type":"tableCell","children":[],"key":"qvazEuudLe"}],"key":"BfFoqrmKaZ"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.02528","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"Scalable MatMul-free Language Modeling","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"TvnRFmQaKF"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.02528","identifier":"https://doi.org/10.48550/arXiv.2406.02528","enumerator":"14","key":"FVxKjBTFuN"}],"key":"G6Xu2Ly49Z"},{"type":"tableCell","children":[{"type":"text","value":"Attention mechanisms, Model efficiency","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"mnQQ1PZXKl"}],"key":"Pns6gTqmGm"},{"type":"tableCell","children":[],"key":"qnp9BZsrNt"}],"key":"lkjwo1tGY8"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1712.00676","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"Will humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"eX9PEM2vkQ"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1712.00676","identifier":"https://doi.org/10.48550/arXiv.1712.00676","enumerator":"15","key":"gz07TLdTMM"}],"key":"NNHugbinSH"},{"type":"tableCell","children":[{"type":"text","value":"AI in software development, Future of coding","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"iL711BHlZ7"}],"key":"beMCMyt2RJ"},{"type":"tableCell","children":[],"key":"k6eRGS0Xo3"}],"key":"vagYYG4STu"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.17035","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"Scalable Extraction of Training Data from (Production) Language Models","position":{"start":{"line":64,"column":1},"end":{"line":64,"column":1}},"key":"n1EC5YfiwT"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.17035","identifier":"https://doi.org/10.48550/arXiv.2311.17035","enumerator":"16","key":"lXspMDvme3"}],"key":"Jkrr5dmRxM"},{"type":"tableCell","children":[{"type":"text","value":"Data accumulation, Model performance, Curated datasets","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"Laod7K6snL"}],"key":"ke7deW2CoN"},{"type":"tableCell","children":[],"key":"PXxT1dQlLu"}],"key":"somtQ5Wb9n"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2305.07759","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"lnZINa1K0k"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2305.07759","identifier":"https://doi.org/10.48550/arXiv.2305.07759","enumerator":"17","key":"Dz5ArCnjrv"}],"key":"rjo1G3GTuF"},{"type":"tableCell","children":[{"type":"text","value":"Dataset creation, Small language models","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"jZKczv08O4"}],"key":"NXLgoFivjT"},{"type":"tableCell","children":[],"key":"o6YsDaR4Di"}],"key":"hGLhfejFlN"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.12983","position":{"start":{"line":70,"column":1},"end":{"line":70,"column":1}},"children":[{"type":"text","value":"GAIA: A Benchmark for General AI Assistants","position":{"start":{"line":70,"column":1},"end":{"line":70,"column":1}},"key":"pv2B8kQMoo"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.12983","identifier":"https://doi.org/10.48550/arXiv.2311.12983","enumerator":"18","key":"LFve4C47fQ"}],"key":"fxNy1mY2aw"},{"type":"tableCell","children":[{"type":"text","value":"AI assistants, Benchmarking","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"IDZVGgMUUP"}],"key":"KW7Mw3rJuH"},{"type":"tableCell","children":[],"key":"RVNHwqHuN9"}],"key":"OHOmkT8gf5"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.anthropic.com/news/mapping-mind-language-model","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet","position":{"start":{"line":73,"column":1},"end":{"line":73,"column":1}},"key":"ML9Ai5WmpZ"}],"urlSource":"https://www.anthropic.com/news/mapping-mind-language-model","key":"vAgiQ0veic"}],"key":"nihl0s4dHl"},{"type":"tableCell","children":[{"type":"text","value":"Feature extraction, Interpretability","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"hkT0obA8tO"}],"key":"in1JLsRwtk"},{"type":"tableCell","children":[],"key":"TKz2xmQ807"}],"key":"bXKTwMTwLE"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.04692","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"Mixture-of-Agents Enhances Large Language Model Capabilities","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"dw5kNxir27"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.04692","identifier":"https://doi.org/10.48550/arXiv.2406.04692","enumerator":"19","key":"SrbdFmcQlU"}],"key":"PbMdRu5OEW"},{"type":"tableCell","children":[{"type":"text","value":"Model performance, Multi-agent systems","position":{"start":{"line":77,"column":1},"end":{"line":77,"column":1}},"key":"ry10nOnv5S"}],"key":"zPbfEBmNUK"},{"type":"tableCell","children":[],"key":"g0QtpnM9tt"}],"key":"nedH0Q3DLZ"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2311.05884","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"children":[{"type":"text","value":"Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems","position":{"start":{"line":79,"column":1},"end":{"line":79,"column":1}},"key":"ZoMdj7mNCK"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2311.05884","identifier":"https://doi.org/10.48550/arXiv.2311.05884","enumerator":"20","key":"GjTqbxPGdz"}],"key":"RazUMdoYkd"},{"type":"tableCell","children":[{"type":"text","value":"Transformers, Model architecture, Model performance","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"snOSRLWpSJ"}],"key":"Oz0zrr3wG7"},{"type":"tableCell","children":[],"key":"kJNS3kK3Nv"}],"key":"QSv9lRyIEp"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.11760","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"Large Language Models Understand and Can Be Enhanced by Emotional Stimuli","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"UWYy3YIpya"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.11760","identifier":"https://doi.org/10.48550/arXiv.2307.11760","enumerator":"21","key":"cvUoZHXRcl"}],"key":"z6bzMxNErG"},{"type":"tableCell","children":[{"type":"text","value":"emotional stimuli, Model behavior","position":{"start":{"line":83,"column":1},"end":{"line":83,"column":1}},"key":"xaVM6dFn7a"}],"key":"QZAynMO09w"},{"type":"tableCell","children":[],"key":"eDvojMz55H"}],"key":"L9aBiNrbkM"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.09171","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"children":[{"type":"text","value":"Automated Unit Test Improvement using Large Language Models at Meta","position":{"start":{"line":85,"column":1},"end":{"line":85,"column":1}},"key":"BVFvjX2Cwy"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.09171","identifier":"https://doi.org/10.48550/arXiv.2402.09171","enumerator":"22","key":"RM8SiHFBvo"}],"key":"k9a0PAPlOH"},{"type":"tableCell","children":[{"type":"text","value":"Automated testing, Software engineering","position":{"start":{"line":86,"column":1},"end":{"line":86,"column":1}},"key":"yEA1WPeLpk"}],"key":"xLlPtoE445"},{"type":"tableCell","children":[],"key":"LuMvBcfBvq"}],"key":"oEgEEm8lW5"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.01413","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"children":[{"type":"text","value":"Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data","position":{"start":{"line":88,"column":1},"end":{"line":88,"column":1}},"key":"Mlbwo6kRWV"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.01413","identifier":"https://doi.org/10.48550/arXiv.2404.01413","enumerator":"23","key":"fZ6PbvTtNk"}],"key":"dV7qn2hSUK"},{"type":"tableCell","children":[{"type":"text","value":"Data accumulation, Model collapse prevention","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"ymtRkM16rW"}],"key":"fm6xnO6GRF"},{"type":"tableCell","children":[],"key":"SFV2Wzkt2i"}],"key":"B95tjevOnJ"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2102.04518","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"text","value":"A\\ Search Without Expansions: Learning Heuristic Functions With Deep Q-Networks","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"key":"hEUnDzCeqK"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2102.04518","identifier":"https://doi.org/10.48550/arXiv.2102.04518","enumerator":"24","key":"RQixcNaBG0"}],"key":"z0RydfjFlW"},{"type":"tableCell","children":[{"type":"text","value":"Reinforcement learning, Search algorithms","position":{"start":{"line":92,"column":1},"end":{"line":92,"column":1}},"key":"IMSLs1JrFO"}],"key":"MmetaBuul6"},{"type":"tableCell","children":[],"key":"KRSTkOuwgE"}],"key":"CleMNdlHuA"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.06196","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"children":[{"type":"text","value":"Large Language Models: A Survey","position":{"start":{"line":94,"column":1},"end":{"line":94,"column":1}},"key":"yQMYuaibTd"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.06196","identifier":"https://doi.org/10.48550/arXiv.2402.06196","enumerator":"25","key":"zjxrxpPm75"}],"key":"nju3Hz42Gl"},{"type":"tableCell","children":[{"type":"text","value":"LLM capabilities, Survey","position":{"start":{"line":95,"column":1},"end":{"line":95,"column":1}},"key":"GHR4BuLXXz"}],"key":"Z88mVnbWBz"},{"type":"tableCell","children":[],"key":"SSmDqfcEFJ"}],"key":"ZUKotEpYYD"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2402.14433","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"A Language Model’s Guide Through Latent Space","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"zXlGkWfwcH"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2402.14433","identifier":"https://doi.org/10.48550/arXiv.2402.14433","enumerator":"26","key":"OFP9kkVyGS"}],"key":"PXCBP74sRb"},{"type":"tableCell","children":[{"type":"text","value":"Interpretability, Latent space","position":{"start":{"line":98,"column":1},"end":{"line":98,"column":1}},"key":"rrM8QBBwol"}],"key":"SOMXxvzc1H"},{"type":"tableCell","children":[],"key":"T2MtRPRu5s"}],"key":"KinbuJyMC7"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2205.05124","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"children":[{"type":"text","value":"Extracting Latent Steering Vectors from Pretrained Language Models","position":{"start":{"line":100,"column":1},"end":{"line":100,"column":1}},"key":"HMlhyXmZ9c"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2205.05124","identifier":"https://doi.org/10.48550/arXiv.2205.05124","enumerator":"27","key":"gUEfqK3Nxs"}],"key":"J2QTU9myuk"},{"type":"tableCell","children":[{"type":"text","value":"Interpretability, Latent space","position":{"start":{"line":101,"column":1},"end":{"line":101,"column":1}},"key":"ZE9f8Hwzps"}],"key":"LXLMUvItXS"},{"type":"tableCell","children":[],"key":"lFQAV4gjXU"}],"key":"DhHH5V5FW1"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.anthropic.com/research/many-shot-jailbreaking","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"Many-shot jailbreaking","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"zOJa1bWKvx"}],"urlSource":"https://www.anthropic.com/research/many-shot-jailbreaking","key":"AnYLfhQDNR"}],"key":"TcEhHVOBew"},{"type":"tableCell","children":[{"type":"text","value":"Jailbreaking, Model safety","position":{"start":{"line":104,"column":1},"end":{"line":104,"column":1}},"key":"cL3ClEfcm0"}],"key":"FhcAl4PYS4"},{"type":"tableCell","children":[],"key":"PI23wevR3D"}],"key":"nB1XUN18VM"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1910.10683","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"children":[{"type":"text","value":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","position":{"start":{"line":106,"column":1},"end":{"line":106,"column":1}},"key":"vAW8FnzdGc"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1910.10683","identifier":"https://doi.org/10.48550/arXiv.1910.10683","enumerator":"28","key":"DKZLeUkRIR"}],"key":"oNTQPw2XBU"},{"type":"tableCell","children":[{"type":"text","value":"Transfer learning","position":{"start":{"line":107,"column":1},"end":{"line":107,"column":1}},"key":"QxH1fomA8G"}],"key":"yAmRaA8613"},{"type":"tableCell","children":[],"key":"yIncVZIfIg"}],"key":"tdcU9sloeb"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2308.10248","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"children":[{"type":"text","value":"Activation Addition: Steering Language Models Without Optimization","position":{"start":{"line":109,"column":1},"end":{"line":109,"column":1}},"key":"GjwKCttWPc"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2308.10248","identifier":"https://doi.org/10.48550/arXiv.2308.10248","enumerator":"29","key":"Emx7MgPUJX"}],"key":"DF4aXTaYdS"},{"type":"tableCell","children":[{"type":"text","value":"Activation manipulation, Model steering","position":{"start":{"line":110,"column":1},"end":{"line":110,"column":1}},"key":"fP7v0sATSs"}],"key":"XSHjNJ7ZeF"},{"type":"tableCell","children":[],"key":"TZe6jTQHzF"}],"key":"hVdFprZXO6"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2107.03374","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"children":[{"type":"text","value":"Evaluating Large Language Models Trained on Code","position":{"start":{"line":112,"column":1},"end":{"line":112,"column":1}},"key":"pZ3K9Ysvwe"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2107.03374","identifier":"https://doi.org/10.48550/arXiv.2107.03374","enumerator":"30","key":"hhJPR1MJvR"}],"key":"mY64HOrFTS"},{"type":"tableCell","children":[{"type":"text","value":"Code generation, Model evaluation","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"key":"d0XGRZGkqu"}],"key":"Y8EfT745l4"},{"type":"tableCell","children":[],"key":"Ssr3Vgf37w"}],"key":"B7SQp7Kh5L"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.02543","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"To Believe or Not to Believe Your LLM","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"T9mmazpFld"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.02543","identifier":"https://doi.org/10.48550/arXiv.2406.02543","enumerator":"31","key":"AWcICnMNOM"}],"key":"hQnuVV0Tnr"},{"type":"tableCell","children":[{"type":"text","value":"Hallucination detection, Uncertainty quantification","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"lvQA9RLgTy"}],"key":"eckjZCCE8N"},{"type":"tableCell","children":[],"key":"zj3mji1Iq1"}],"key":"HHxre9TsxJ"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.04692","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"children":[{"type":"text","value":"Mixture of Agents","position":{"start":{"line":118,"column":1},"end":{"line":118,"column":1}},"key":"LOpSujrMwy"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.04692","identifier":"https://doi.org/10.48550/arXiv.2406.04692","enumerator":"19","key":"h9znMAQ76e"}],"key":"a8YGPvECvU"},{"type":"tableCell","children":[{"type":"text","value":"Multi-agent systems, Prompting","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"KMbZ6iWVNT"}],"key":"X9ckGYhkph"},{"type":"tableCell","children":[],"key":"UuiFaxtrI3"}],"key":"KqdzuiVblp"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2404.14619","position":{"start":{"line":121,"column":1},"end":{"line":121,"column":1}},"children":[{"type":"text","value":"OpenELM: An Efficient Language Model Family with Open Training and Inference Framework","position":{"start":{"line":121,"column":1},"end":{"line":121,"column":1}},"key":"IKmorwCEuR"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2404.14619","identifier":"https://doi.org/10.48550/arXiv.2404.14619","enumerator":"32","key":"XKte6Z03Do"}],"key":"pdTVdnEtN8"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model architecture","position":{"start":{"line":122,"column":1},"end":{"line":122,"column":1}},"key":"ZsKxwq4iFb"}],"key":"ncNvXXicH0"},{"type":"tableCell","children":[],"key":"AB6RwqF2PC"}],"key":"sshrNCFgQ2"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1706.03741","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"children":[{"type":"text","value":"Deep Reinforcement Learning from Human Preferences","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"VR7MSduMZF"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1706.03741","identifier":"https://doi.org/10.48550/arXiv.1706.03741","enumerator":"33","key":"kVufekr2ui"}],"key":"ecbXRP2ZF3"},{"type":"tableCell","children":[{"type":"text","value":"human feedback, Reinforcement learning","position":{"start":{"line":125,"column":1},"end":{"line":125,"column":1}},"key":"MviRoZw1JY"}],"key":"HtSy8O2MDr"},{"type":"tableCell","children":[],"key":"hNE1VZvcYw"}],"key":"NphIZupwJP"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.08925","position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"children":[{"type":"text","value":"Federated Large Language Model: A Position Paper","position":{"start":{"line":127,"column":1},"end":{"line":127,"column":1}},"key":"SjioADOyRT"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.08925","identifier":"https://doi.org/10.48550/arXiv.2307.08925","enumerator":"34","key":"mJpcvJe84k"}],"key":"rDxMQtMGtk"},{"type":"tableCell","children":[{"type":"text","value":"Distributed training, Federated learning","position":{"start":{"line":128,"column":1},"end":{"line":128,"column":1}},"key":"s41crJSRA3"}],"key":"KkPrcOSPnU"},{"type":"tableCell","children":[],"key":"M4CDRTqLT1"}],"key":"AqpN1kubwy"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2212.02508","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"children":[{"type":"text","value":"MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning","position":{"start":{"line":130,"column":1},"end":{"line":130,"column":1}},"key":"GSU02wTe9Z"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2212.02508","identifier":"https://doi.org/10.48550/arXiv.2212.02508","enumerator":"35","key":"kP46EayrvJ"}],"key":"dYwhSFTSYB"},{"type":"tableCell","children":[{"type":"text","value":"Music representation, Self-supervised learning","position":{"start":{"line":131,"column":1},"end":{"line":131,"column":1}},"key":"LjIFxPPhyP"}],"key":"DW5gfrCtCS"},{"type":"tableCell","children":[],"key":"bHGbgaPoOP"}],"key":"e2x374FtIT"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2302.13971","position":{"start":{"line":133,"column":1},"end":{"line":133,"column":1}},"children":[{"type":"text","value":"LLaMA: Open and Efficient Foundation Language Models","position":{"start":{"line":133,"column":1},"end":{"line":133,"column":1}},"key":"UfAjSE9374"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2302.13971","identifier":"https://doi.org/10.48550/arXiv.2302.13971","enumerator":"36","key":"LtXy2hhmjK"}],"key":"QVnLDQyJPL"},{"type":"tableCell","children":[{"type":"text","value":"Model architecture, Open-source LLMs","position":{"start":{"line":134,"column":1},"end":{"line":134,"column":1}},"key":"NOL43QaE7P"}],"key":"IAPSzbjlrO"},{"type":"tableCell","children":[],"key":"klaP2AQwWM"}],"key":"WKmxQjCc6R"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"text","value":"Phi1: Textbooks Are All You Need","position":{"start":{"line":136,"column":1},"end":{"line":136,"column":1}},"key":"DTigMc2ojh"}],"urlSource":"https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/","key":"nGv9RETgRj"}],"key":"E0e8fQapFN"},{"type":"tableCell","children":[{"type":"text","value":"Curated datasets, Model efficiency","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"h0Y9fh9dax"}],"key":"PDixLZoV9A"},{"type":"tableCell","children":[],"key":"SQaIdXZBXS"}],"key":"F9SFFiPFZY"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1607.06450","position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"children":[{"type":"text","value":"Layer Normalization","position":{"start":{"line":139,"column":1},"end":{"line":139,"column":1}},"key":"xzWoETeUZT"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1607.06450","identifier":"https://doi.org/10.48550/arXiv.1607.06450","enumerator":"37","key":"NqR1KY9dji"}],"key":"vD2k7Xq9Di"},{"type":"tableCell","children":[{"type":"text","value":"Model internals, Optimization","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"EzdBl4Bq1x"}],"key":"ZkjtTLqm9l"},{"type":"tableCell","children":[],"key":"hacaPSdtS6"}],"key":"fL20HIE1sA"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1706.03762","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"children":[{"type":"text","value":"Attention Is All You Need","position":{"start":{"line":142,"column":1},"end":{"line":142,"column":1}},"key":"HH9SU3ff1U"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1706.03762","identifier":"https://doi.org/10.48550/arXiv.1706.03762","enumerator":"38","key":"jlfSN4kNHw"}],"key":"azZygdQEMX"},{"type":"tableCell","children":[{"type":"text","value":"OG papers, Transformers","position":{"start":{"line":143,"column":1},"end":{"line":143,"column":1}},"key":"PxAiOaS4eK"}],"key":"TttEA2DLBt"},{"type":"tableCell","children":[],"key":"OSnloNe7Jb"}],"key":"LzGa7saxCb"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2406.09412","position":{"start":{"line":145,"column":1},"end":{"line":145,"column":1}},"children":[{"type":"text","value":"Explore the Limits of Omni-modal Pretraining at Scale","position":{"start":{"line":145,"column":1},"end":{"line":145,"column":1}},"key":"mGqOLrRdZc"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2406.09412","identifier":"https://doi.org/10.48550/arXiv.2406.09412","enumerator":"39","key":"gNOCr6zlj7"}],"key":"Nkdxy7Fs8F"},{"type":"tableCell","children":[{"type":"text","value":"Multi-modal models, Pretraining","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"lLvYu1RYRd"}],"key":"Gciq58K1wT"},{"type":"tableCell","children":[],"key":"rWxFcMgIEj"}],"key":"CFjcw7kq3a"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1606.08415","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"children":[{"type":"text","value":"Gaussian Error Linear Units (GELUs)","position":{"start":{"line":148,"column":1},"end":{"line":148,"column":1}},"key":"NzRDeBih67"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1606.08415","identifier":"https://doi.org/10.48550/arXiv.1606.08415","enumerator":"40","key":"MGVQY8wxw9"}],"key":"CNQAukWBtD"},{"type":"tableCell","children":[{"type":"text","value":"Activation functions, Model internals","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"Bqev8KbZJl"}],"key":"OphWQO8R5G"},{"type":"tableCell","children":[],"key":"UW1akV6XTM"}],"key":"aDBu1SngmS"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2401.09796","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"children":[{"type":"text","value":"A Fast, Performant, Secure Distributed Training Framework For Large Language Model","position":{"start":{"line":151,"column":1},"end":{"line":151,"column":1}},"key":"rtjS9QBVPj"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2401.09796","identifier":"https://doi.org/10.48550/arXiv.2401.09796","enumerator":"41","key":"xzTow3goLV"}],"key":"nui4znCvli"},{"type":"tableCell","children":[{"type":"text","value":"Distributed training, Security","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"CjWA73YEZ7"}],"key":"FyuFmM4UC3"},{"type":"tableCell","children":[],"key":"pEwIw4Rgi9"}],"key":"vz0R8wFObQ"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.1910.01108","position":{"start":{"line":154,"column":1},"end":{"line":154,"column":1}},"children":[{"type":"text","value":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","position":{"start":{"line":154,"column":1},"end":{"line":154,"column":1}},"key":"hy47oviBgs"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.1910.01108","identifier":"https://doi.org/10.48550/arXiv.1910.01108","enumerator":"42","key":"JfO8Lhu5CI"}],"key":"gnEj8jTXo2"},{"type":"tableCell","children":[{"type":"text","value":"Efficiency, Model distillation","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"yg9tUlsWzx"}],"key":"a3g4R9x8Zu"},{"type":"tableCell","children":[],"key":"S5YcaxBQ4O"}],"key":"kwMTYbakTT"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"children":[{"type":"text","value":"Improving Language Understanding by Generative Pre-Training","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"zeHEjgyPK7"}],"urlSource":"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf","key":"JUFZTSInjQ"}],"key":"YcR1928nqM"},{"type":"tableCell","children":[{"type":"text","value":"OG papers, Pre-training","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"iTj3wxFjRD"}],"key":"OxlL9gYGlQ"},{"type":"tableCell","children":[],"key":"OVaHxaoDyM"}],"key":"CoDWm1kaxc"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2203.02155","position":{"start":{"line":160,"column":1},"end":{"line":160,"column":1}},"children":[{"type":"text","value":"Training language models to follow instructions with human feedback","position":{"start":{"line":160,"column":1},"end":{"line":160,"column":1}},"key":"wgvsoaPiLT"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2203.02155","identifier":"https://doi.org/10.48550/arXiv.2203.02155","enumerator":"43","key":"agyuAHoWqx"}],"key":"TEL6tQm3ic"},{"type":"tableCell","children":[{"type":"text","value":"Instruction following, Reinforcement learning","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"pm4XQVTQUU"}],"key":"CTm4K0xmC9"},{"type":"tableCell","children":[],"key":"cpyIKgmxrK"}],"key":"AJ3gcw0DFB"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2307.09288","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"children":[{"type":"text","value":"Llama 2: Open Foundation and Fine-Tuned Chat Models","position":{"start":{"line":163,"column":1},"end":{"line":163,"column":1}},"key":"RRacUoXo1U"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2307.09288","identifier":"https://doi.org/10.48550/arXiv.2307.09288","enumerator":"44","key":"iwh11wXnmy"}],"key":"lJmVJqYsMQ"},{"type":"tableCell","children":[{"type":"text","value":"Fine-tuning, Open-source LLMs","position":{"start":{"line":164,"column":1},"end":{"line":164,"column":1}},"key":"X9Xx1kbruJ"}],"key":"fCPvvs1ySB"},{"type":"tableCell","children":[],"key":"Ik9AK3ocRP"}],"key":"WDz5SLDuhS"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://doi.org/10.48550/arXiv.2104.09864","position":{"start":{"line":166,"column":1},"end":{"line":166,"column":1}},"children":[{"type":"text","value":"RoFormer: Enhanced Transformer with Rotary Position Embedding","position":{"start":{"line":166,"column":1},"end":{"line":166,"column":1}},"key":"gROMg4jA8c"}],"kind":"narrative","label":"https://doi.org/10.48550/arxiv.2104.09864","identifier":"https://doi.org/10.48550/arXiv.2104.09864","enumerator":"45","key":"D5mY7bvhjG"}],"key":"JnLIBUCwEk"},{"type":"tableCell","children":[{"type":"text","value":"Embeddings, Model architecture","position":{"start":{"line":167,"column":1},"end":{"line":167,"column":1}},"key":"WnqtFSTO1b"}],"key":"AjvSfr3818"},{"type":"tableCell","children":[],"key":"g32htuwVK5"}],"key":"afrA6W5LRM"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"link","url":"https://www.nature.com/articles/s42256-023-00748-9","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"children":[{"type":"text","value":"Spatially embedded recurrent neural networks reveal widespread links between structural and functional neuroscience findings | Nature Machine Intelligence","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"Lz0l1IvizE"}],"urlSource":"https://www.nature.com/articles/s42256-023-00748-9","key":"OU45vQkoUE"}],"key":"ARAmir5RID"},{"type":"tableCell","children":[{"type":"text","value":"Biological Brains","position":{"start":{"line":170,"column":1},"end":{"line":170,"column":1}},"key":"PywjuAj2zJ"}],"key":"Ldv37Q3mze"},{"type":"tableCell","children":[],"key":"ZmUnr4PpJI"}],"key":"WMiiieIqEB"},{"type":"tableRow","children":[{"type":"tableCell","children":[{"type":"cite","url":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12737","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"children":[{"type":"text","value":"Understanding Human Cognition Through Computational Modeling - Hsiao - 2024 - Topics in Cognitive Science - Wiley Online Library","position":{"start":{"line":172,"column":1},"end":{"line":172,"column":1}},"key":"z0VLazQUFJ"}],"kind":"narrative","label":"Hsiao_2024","identifier":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12737","enumerator":"46","key":"B1MlKrZUDo"}],"key":"Jczy0AcJs8"},{"type":"tableCell","children":[{"type":"text","value":"Biological Brains","position":{"start":{"line":173,"column":1},"end":{"line":173,"column":1}},"key":"b25RXAiHzU"}],"key":"E5mTLOFpPr"},{"type":"tableCell","children":[],"key":"X8gmhKcGe4"}],"key":"Q9IsNr4JNT"}],"key":"ZflDi4eU0y"}],"enumerator":"1","key":"O8xTiiyr0C"}],"key":"izkRFT27q3"}],"key":"BcrbsjdvYZ"},"references":{"cite":{"order":["https://doi.org/10.48550/arxiv.2411.07191","https://doi.org/10.48550/arxiv.2410.02725","https://doi.org/10.48550/arxiv.2404.03592","https://doi.org/10.48550/arxiv.2402.06111","https://doi.org/10.48550/arxiv.2403.20327","https://doi.org/10.48550/arxiv.2210.07128","https://doi.org/10.48550/arxiv.2407.10969","https://doi.org/10.48550/arxiv.2207.01780","https://doi.org/10.48550/arxiv.2405.20541","https://doi.org/10.48550/arxiv.2402.17764","https://doi.org/10.48550/arxiv.2305.17493","https://doi.org/10.48550/arxiv.2404.07143","https://doi.org/10.48550/arxiv.2406.07496","https://doi.org/10.48550/arxiv.2406.02528","https://doi.org/10.48550/arxiv.1712.00676","https://doi.org/10.48550/arxiv.2311.17035","https://doi.org/10.48550/arxiv.2305.07759","https://doi.org/10.48550/arxiv.2311.12983","https://doi.org/10.48550/arxiv.2406.04692","https://doi.org/10.48550/arxiv.2311.05884","https://doi.org/10.48550/arxiv.2307.11760","https://doi.org/10.48550/arxiv.2402.09171","https://doi.org/10.48550/arxiv.2404.01413","https://doi.org/10.48550/arxiv.2102.04518","https://doi.org/10.48550/arxiv.2402.06196","https://doi.org/10.48550/arxiv.2402.14433","https://doi.org/10.48550/arxiv.2205.05124","https://doi.org/10.48550/arxiv.1910.10683","https://doi.org/10.48550/arxiv.2308.10248","https://doi.org/10.48550/arxiv.2107.03374","https://doi.org/10.48550/arxiv.2406.02543","https://doi.org/10.48550/arxiv.2404.14619","https://doi.org/10.48550/arxiv.1706.03741","https://doi.org/10.48550/arxiv.2307.08925","https://doi.org/10.48550/arxiv.2212.02508","https://doi.org/10.48550/arxiv.2302.13971","https://doi.org/10.48550/arxiv.1607.06450","https://doi.org/10.48550/arxiv.1706.03762","https://doi.org/10.48550/arxiv.2406.09412","https://doi.org/10.48550/arxiv.1606.08415","https://doi.org/10.48550/arxiv.2401.09796","https://doi.org/10.48550/arxiv.1910.01108","https://doi.org/10.48550/arxiv.2203.02155","https://doi.org/10.48550/arxiv.2307.09288","https://doi.org/10.48550/arxiv.2104.09864","Hsiao_2024"],"data":{"https://doi.org/10.48550/arxiv.2411.07191":{"label":"https://doi.org/10.48550/arxiv.2411.07191","enumerator":"1","doi":"10.48550/ARXIV.2411.07191","html":"Yu, M., Wang, D., Shan, Q., Reed, C., \u0026 Wan, A. (2024). \u003ci\u003eThe Super Weight in Large Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2411.07191\"\u003e10.48550/ARXIV.2411.07191\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2411.07191"},"https://doi.org/10.48550/arxiv.2410.02725":{"label":"https://doi.org/10.48550/arxiv.2410.02725","enumerator":"2","doi":"10.48550/ARXIV.2410.02725","html":"Manvi, R., Singh, A., \u0026 Ermon, S. (2024). \u003ci\u003eAdaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2410.02725\"\u003e10.48550/ARXIV.2410.02725\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2410.02725"},"https://doi.org/10.48550/arxiv.2404.03592":{"label":"https://doi.org/10.48550/arxiv.2404.03592","enumerator":"3","doi":"10.48550/ARXIV.2404.03592","html":"Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., \u0026 Potts, C. (2024). \u003ci\u003eReFT: Representation Finetuning for Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.03592\"\u003e10.48550/ARXIV.2404.03592\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2404.03592"},"https://doi.org/10.48550/arxiv.2402.06111":{"label":"https://doi.org/10.48550/arxiv.2402.06111","enumerator":"4","doi":"10.48550/ARXIV.2402.06111","html":"Alshahwan, N., Harman, M., Marginean, A., Tal, R., \u0026 Wang, E. (2024). \u003ci\u003eObservation-based unit test generation at Meta\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.06111\"\u003e10.48550/ARXIV.2402.06111\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2402.06111"},"https://doi.org/10.48550/arxiv.2403.20327":{"label":"https://doi.org/10.48550/arxiv.2403.20327","enumerator":"5","doi":"10.48550/ARXIV.2403.20327","html":"Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J. R., Hui, K., Boratko, M., Kapadia, R., Ding, W., Luan, Y., Duddu, S. M. K., Abrego, G. H., Shi, W., Gupta, N., Kusupati, A., Jain, P., Jonnalagadda, S. R., Chang, M.-W., \u0026 Naim, I. (2024). \u003ci\u003eGecko: Versatile Text Embeddings Distilled from Large Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2403.20327\"\u003e10.48550/ARXIV.2403.20327\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2403.20327"},"https://doi.org/10.48550/arxiv.2210.07128":{"label":"https://doi.org/10.48550/arxiv.2210.07128","enumerator":"6","doi":"10.48550/ARXIV.2210.07128","html":"Madaan, A., Zhou, S., Alon, U., Yang, Y., \u0026 Neubig, G. (2022). \u003ci\u003eLanguage Models of Code are Few-Shot Commonsense Learners\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2210.07128\"\u003e10.48550/ARXIV.2210.07128\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2210.07128"},"https://doi.org/10.48550/arxiv.2407.10969":{"label":"https://doi.org/10.48550/arxiv.2407.10969","enumerator":"7","doi":"10.48550/ARXIV.2407.10969","html":"Wang, H., Ma, S., Wang, R., \u0026 Wei, F. (2024). \u003ci\u003eQ-Sparse: All Large Language Models can be Fully Sparsely-Activated\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2407.10969\"\u003e10.48550/ARXIV.2407.10969\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2407.10969"},"https://doi.org/10.48550/arxiv.2207.01780":{"label":"https://doi.org/10.48550/arxiv.2207.01780","enumerator":"8","doi":"10.48550/ARXIV.2207.01780","html":"Le, H., Wang, Y., Gotmare, A. D., Savarese, S., \u0026 Hoi, S. C. H. (2022). \u003ci\u003eCodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2207.01780\"\u003e10.48550/ARXIV.2207.01780\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2207.01780"},"https://doi.org/10.48550/arxiv.2405.20541":{"label":"https://doi.org/10.48550/arxiv.2405.20541","enumerator":"9","doi":"10.48550/ARXIV.2405.20541","html":"Ankner, Z., Blakeney, C., Sreenivasan, K., Marion, M., Leavitt, M. L., \u0026 Paul, M. (2024). \u003ci\u003ePerplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2405.20541\"\u003e10.48550/ARXIV.2405.20541\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2405.20541"},"https://doi.org/10.48550/arxiv.2402.17764":{"label":"https://doi.org/10.48550/arxiv.2402.17764","enumerator":"10","doi":"10.48550/ARXIV.2402.17764","html":"Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., \u0026 Wei, F. (2024). \u003ci\u003eThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.17764\"\u003e10.48550/ARXIV.2402.17764\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2402.17764"},"https://doi.org/10.48550/arxiv.2305.17493":{"label":"https://doi.org/10.48550/arxiv.2305.17493","enumerator":"11","doi":"10.48550/ARXIV.2305.17493","html":"Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., \u0026 Anderson, R. (2023). \u003ci\u003eThe Curse of Recursion: Training on Generated Data Makes Models Forget\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2305.17493\"\u003e10.48550/ARXIV.2305.17493\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2305.17493"},"https://doi.org/10.48550/arxiv.2404.07143":{"label":"https://doi.org/10.48550/arxiv.2404.07143","enumerator":"12","doi":"10.48550/ARXIV.2404.07143","html":"Munkhdalai, T., Faruqui, M., \u0026 Gopal, S. (2024). \u003ci\u003eLeave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.07143\"\u003e10.48550/ARXIV.2404.07143\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2404.07143"},"https://doi.org/10.48550/arxiv.2406.07496":{"label":"https://doi.org/10.48550/arxiv.2406.07496","enumerator":"13","doi":"10.48550/ARXIV.2406.07496","html":"Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., \u0026 Zou, J. (2024). \u003ci\u003eTextGrad: Automatic “Differentiation” via Text\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.07496\"\u003e10.48550/ARXIV.2406.07496\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2406.07496"},"https://doi.org/10.48550/arxiv.2406.02528":{"label":"https://doi.org/10.48550/arxiv.2406.02528","enumerator":"14","doi":"10.48550/ARXIV.2406.02528","html":"Zhu, R.-J., Zhang, Y., Sifferman, E., Sheaves, T., Wang, Y., Richmond, D., Zhou, P., \u0026 Eshraghian, J. K. (2024). \u003ci\u003eScalable MatMul-free Language Modeling\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.02528\"\u003e10.48550/ARXIV.2406.02528\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2406.02528"},"https://doi.org/10.48550/arxiv.1712.00676":{"label":"https://doi.org/10.48550/arxiv.1712.00676","enumerator":"15","doi":"10.48550/ARXIV.1712.00676","html":"Billings, J. J., McCaskey, A. J., Vallee, G., \u0026 Watson, G. (2017). \u003ci\u003eWill humans even write code in 2040 and what would that mean for extreme heterogeneity in computing?\u003c/i\u003e arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1712.00676\"\u003e10.48550/ARXIV.1712.00676\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1712.00676"},"https://doi.org/10.48550/arxiv.2311.17035":{"label":"https://doi.org/10.48550/arxiv.2311.17035","enumerator":"16","doi":"10.48550/ARXIV.2311.17035","html":"Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tramèr, F., \u0026 Lee, K. (2023). \u003ci\u003eScalable Extraction of Training Data from (Production) Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.17035\"\u003e10.48550/ARXIV.2311.17035\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2311.17035"},"https://doi.org/10.48550/arxiv.2305.07759":{"label":"https://doi.org/10.48550/arxiv.2305.07759","enumerator":"17","doi":"10.48550/ARXIV.2305.07759","html":"Eldan, R., \u0026 Li, Y. (2023). \u003ci\u003eTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\u003c/i\u003e arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2305.07759\"\u003e10.48550/ARXIV.2305.07759\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2305.07759"},"https://doi.org/10.48550/arxiv.2311.12983":{"label":"https://doi.org/10.48550/arxiv.2311.12983","enumerator":"18","doi":"10.48550/ARXIV.2311.12983","html":"Mialon, G., Fourrier, C., Swift, C., Wolf, T., LeCun, Y., \u0026 Scialom, T. (2023). \u003ci\u003eGAIA: a benchmark for General AI Assistants\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.12983\"\u003e10.48550/ARXIV.2311.12983\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2311.12983"},"https://doi.org/10.48550/arxiv.2406.04692":{"label":"https://doi.org/10.48550/arxiv.2406.04692","enumerator":"19","doi":"10.48550/ARXIV.2406.04692","html":"Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., \u0026 Zou, J. (2024). \u003ci\u003eMixture-of-Agents Enhances Large Language Model Capabilities\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.04692\"\u003e10.48550/ARXIV.2406.04692\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2406.04692"},"https://doi.org/10.48550/arxiv.2311.05884":{"label":"https://doi.org/10.48550/arxiv.2311.05884","enumerator":"20","doi":"10.48550/ARXIV.2311.05884","html":"Gui, H., Wang, R., Yin, K., Jin, L., Kula, M., Xu, T., Hong, L., \u0026 Chi, E. H. (2023). \u003ci\u003eHiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2311.05884\"\u003e10.48550/ARXIV.2311.05884\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2311.05884"},"https://doi.org/10.48550/arxiv.2307.11760":{"label":"https://doi.org/10.48550/arxiv.2307.11760","enumerator":"21","doi":"10.48550/ARXIV.2307.11760","html":"Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., \u0026 Xie, X. (2023). \u003ci\u003eLarge Language Models Understand and Can be Enhanced by Emotional Stimuli\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.11760\"\u003e10.48550/ARXIV.2307.11760\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2307.11760"},"https://doi.org/10.48550/arxiv.2402.09171":{"label":"https://doi.org/10.48550/arxiv.2402.09171","enumerator":"22","doi":"10.48550/ARXIV.2402.09171","html":"Alshahwan, N., Chheda, J., Finegenova, A., Gokkaya, B., Harman, M., Harper, I., Marginean, A., Sengupta, S., \u0026 Wang, E. (2024). \u003ci\u003eAutomated Unit Test Improvement using Large Language Models at Meta\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.09171\"\u003e10.48550/ARXIV.2402.09171\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2402.09171"},"https://doi.org/10.48550/arxiv.2404.01413":{"label":"https://doi.org/10.48550/arxiv.2404.01413","enumerator":"23","doi":"10.48550/ARXIV.2404.01413","html":"Gerstgrasser, M., Schaeffer, R., Dey, A., Rafailov, R., Sleight, H., Hughes, J., Korbak, T., Agrawal, R., Pai, D., Gromov, A., Roberts, D. A., Yang, D., Donoho, D. L., \u0026 Koyejo, S. (2024). \u003ci\u003eIs Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.01413\"\u003e10.48550/ARXIV.2404.01413\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2404.01413"},"https://doi.org/10.48550/arxiv.2102.04518":{"label":"https://doi.org/10.48550/arxiv.2102.04518","enumerator":"24","doi":"10.48550/ARXIV.2102.04518","html":"Agostinelli, F., Shmakov, A., McAleer, S., Fox, R., \u0026 Baldi, P. (2021). \u003ci\u003eA* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2102.04518\"\u003e10.48550/ARXIV.2102.04518\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2102.04518"},"https://doi.org/10.48550/arxiv.2402.06196":{"label":"https://doi.org/10.48550/arxiv.2402.06196","enumerator":"25","doi":"10.48550/ARXIV.2402.06196","html":"Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., \u0026 Gao, J. (2024). \u003ci\u003eLarge Language Models: A Survey\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.06196\"\u003e10.48550/ARXIV.2402.06196\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2402.06196"},"https://doi.org/10.48550/arxiv.2402.14433":{"label":"https://doi.org/10.48550/arxiv.2402.14433","enumerator":"26","doi":"10.48550/ARXIV.2402.14433","html":"von Rütte, D., Anagnostidis, S., Bachmann, G., \u0026 Hofmann, T. (2024). \u003ci\u003eA Language Model’s Guide Through Latent Space\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2402.14433\"\u003e10.48550/ARXIV.2402.14433\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2402.14433"},"https://doi.org/10.48550/arxiv.2205.05124":{"label":"https://doi.org/10.48550/arxiv.2205.05124","enumerator":"27","doi":"10.48550/ARXIV.2205.05124","html":"Subramani, N., Suresh, N., \u0026 Peters, M. E. (2022). \u003ci\u003eExtracting Latent Steering Vectors from Pretrained Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2205.05124\"\u003e10.48550/ARXIV.2205.05124\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2205.05124"},"https://doi.org/10.48550/arxiv.1910.10683":{"label":"https://doi.org/10.48550/arxiv.1910.10683","enumerator":"28","doi":"10.48550/ARXIV.1910.10683","html":"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., \u0026 Liu, P. J. (2019). \u003ci\u003eExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1910.10683\"\u003e10.48550/ARXIV.1910.10683\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1910.10683"},"https://doi.org/10.48550/arxiv.2308.10248":{"label":"https://doi.org/10.48550/arxiv.2308.10248","enumerator":"29","doi":"10.48550/ARXIV.2308.10248","html":"Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez, J. J., Mini, U., \u0026 MacDiarmid, M. (2023). \u003ci\u003eSteering Language Models With Activation Engineering\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2308.10248\"\u003e10.48550/ARXIV.2308.10248\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2308.10248"},"https://doi.org/10.48550/arxiv.2107.03374":{"label":"https://doi.org/10.48550/arxiv.2107.03374","enumerator":"30","doi":"10.48550/ARXIV.2107.03374","html":"Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). \u003ci\u003eEvaluating Large Language Models Trained on Code\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2107.03374\"\u003e10.48550/ARXIV.2107.03374\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2107.03374"},"https://doi.org/10.48550/arxiv.2406.02543":{"label":"https://doi.org/10.48550/arxiv.2406.02543","enumerator":"31","doi":"10.48550/ARXIV.2406.02543","html":"Yadkori, Y. A., Kuzborskij, I., György, A., \u0026 Szepesvári, C. (2024). \u003ci\u003eTo Believe or Not to Believe Your LLM\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.02543\"\u003e10.48550/ARXIV.2406.02543\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2406.02543"},"https://doi.org/10.48550/arxiv.2404.14619":{"label":"https://doi.org/10.48550/arxiv.2404.14619","enumerator":"32","doi":"10.48550/ARXIV.2404.14619","html":"Mehta, S., Sekhavat, M. H., Cao, Q., Horton, M., Jin, Y., Sun, C., Mirzadeh, I., Najibi, M., Belenko, D., Zatloukal, P., \u0026 Rastegari, M. (2024). \u003ci\u003eOpenELM: An Efficient Language Model Family with Open Training and Inference Framework\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2404.14619\"\u003e10.48550/ARXIV.2404.14619\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2404.14619"},"https://doi.org/10.48550/arxiv.1706.03741":{"label":"https://doi.org/10.48550/arxiv.1706.03741","enumerator":"33","doi":"10.48550/ARXIV.1706.03741","html":"Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., \u0026 Amodei, D. (2017). \u003ci\u003eDeep reinforcement learning from human preferences\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1706.03741\"\u003e10.48550/ARXIV.1706.03741\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1706.03741"},"https://doi.org/10.48550/arxiv.2307.08925":{"label":"https://doi.org/10.48550/arxiv.2307.08925","enumerator":"34","doi":"10.48550/ARXIV.2307.08925","html":"Chen, C., Feng, X., Li, Y., Lyu, L., Zhou, J., Zheng, X., \u0026 Yin, J. (2023). \u003ci\u003eIntegration of Large Language Models and Federated Learning\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.08925\"\u003e10.48550/ARXIV.2307.08925\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2307.08925"},"https://doi.org/10.48550/arxiv.2212.02508":{"label":"https://doi.org/10.48550/arxiv.2212.02508","enumerator":"35","doi":"10.48550/ARXIV.2212.02508","html":"Li, Y., Yuan, R., Zhang, G., Ma, Y., Lin, C., Chen, X., Ragni, A., Yin, H., Hu, Z., He, H., Benetos, E., Gyenge, N., Liu, R., \u0026 Fu, J. (2022). \u003ci\u003eMAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2212.02508\"\u003e10.48550/ARXIV.2212.02508\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2212.02508"},"https://doi.org/10.48550/arxiv.2302.13971":{"label":"https://doi.org/10.48550/arxiv.2302.13971","enumerator":"36","doi":"10.48550/ARXIV.2302.13971","html":"Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., \u0026 Lample, G. (2023). \u003ci\u003eLLaMA: Open and Efficient Foundation Language Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2302.13971\"\u003e10.48550/ARXIV.2302.13971\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2302.13971"},"https://doi.org/10.48550/arxiv.1607.06450":{"label":"https://doi.org/10.48550/arxiv.1607.06450","enumerator":"37","doi":"10.48550/ARXIV.1607.06450","html":"Ba, J. L., Kiros, J. R., \u0026 Hinton, G. E. (2016). \u003ci\u003eLayer Normalization\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1607.06450\"\u003e10.48550/ARXIV.1607.06450\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1607.06450"},"https://doi.org/10.48550/arxiv.1706.03762":{"label":"https://doi.org/10.48550/arxiv.1706.03762","enumerator":"38","doi":"10.48550/ARXIV.1706.03762","html":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \u0026 Polosukhin, I. (2017). \u003ci\u003eAttention Is All You Need\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1706.03762\"\u003e10.48550/ARXIV.1706.03762\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1706.03762"},"https://doi.org/10.48550/arxiv.2406.09412":{"label":"https://doi.org/10.48550/arxiv.2406.09412","enumerator":"39","doi":"10.48550/ARXIV.2406.09412","html":"Zhang, Y., Li, H., Liu, J., \u0026 Yue, X. (2024). \u003ci\u003eExplore the Limits of Omni-modal Pretraining at Scale\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2406.09412\"\u003e10.48550/ARXIV.2406.09412\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2406.09412"},"https://doi.org/10.48550/arxiv.1606.08415":{"label":"https://doi.org/10.48550/arxiv.1606.08415","enumerator":"40","doi":"10.48550/ARXIV.1606.08415","html":"Hendrycks, D., \u0026 Gimpel, K. (2016). \u003ci\u003eGaussian Error Linear Units (GELUs)\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1606.08415\"\u003e10.48550/ARXIV.1606.08415\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1606.08415"},"https://doi.org/10.48550/arxiv.2401.09796":{"label":"https://doi.org/10.48550/arxiv.2401.09796","enumerator":"41","doi":"10.48550/ARXIV.2401.09796","html":"Huang, W., Wang, Y., Cheng, A., Zhou, A., Yu, C., \u0026 Wang, L. (2024). \u003ci\u003eA Fast, Performant, Secure Distributed Training Framework For Large Language Model\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2401.09796\"\u003e10.48550/ARXIV.2401.09796\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2401.09796"},"https://doi.org/10.48550/arxiv.1910.01108":{"label":"https://doi.org/10.48550/arxiv.1910.01108","enumerator":"42","doi":"10.48550/ARXIV.1910.01108","html":"Sanh, V., Debut, L., Chaumond, J., \u0026 Wolf, T. (2019). \u003ci\u003eDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.1910.01108\"\u003e10.48550/ARXIV.1910.01108\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.1910.01108"},"https://doi.org/10.48550/arxiv.2203.02155":{"label":"https://doi.org/10.48550/arxiv.2203.02155","enumerator":"43","doi":"10.48550/ARXIV.2203.02155","html":"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., \u0026 Lowe, R. (2022). \u003ci\u003eTraining language models to follow instructions with human feedback\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2203.02155\"\u003e10.48550/ARXIV.2203.02155\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2203.02155"},"https://doi.org/10.48550/arxiv.2307.09288":{"label":"https://doi.org/10.48550/arxiv.2307.09288","enumerator":"44","doi":"10.48550/ARXIV.2307.09288","html":"Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., … Scialom, T. (2023). \u003ci\u003eLlama 2: Open Foundation and Fine-Tuned Chat Models\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2307.09288\"\u003e10.48550/ARXIV.2307.09288\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2307.09288"},"https://doi.org/10.48550/arxiv.2104.09864":{"label":"https://doi.org/10.48550/arxiv.2104.09864","enumerator":"45","doi":"10.48550/ARXIV.2104.09864","html":"Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., \u0026 Liu, Y. (2021). \u003ci\u003eRoFormer: Enhanced Transformer with Rotary Position Embedding\u003c/i\u003e. arXiv. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.48550/ARXIV.2104.09864\"\u003e10.48550/ARXIV.2104.09864\u003c/a\u003e","url":"https://doi.org/10.48550/ARXIV.2104.09864"},"Hsiao_2024":{"label":"Hsiao_2024","enumerator":"46","doi":"10.1111/tops.12737","html":"Hsiao, J. H. (2024). Understanding Human Cognition Through Computational Modeling. \u003ci\u003eTopics in Cognitive Science\u003c/i\u003e, \u003ci\u003e16\u003c/i\u003e(3), 349–376. \u003ca target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1111/tops.12737\"\u003e10.1111/tops.12737\u003c/a\u003e","url":"https://doi.org/10.1111/tops.12737"}}}},"footer":{"navigation":{"prev":{"title":"The Transformer","url":"/glossary/transformer-1","group":"ML Notes"},"next":{"title":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","url":"/paper-notes/perplexity-based-data-pruning","group":"ML Notes"}}},"domain":"http://localhost:3002"},"project":{"title":"ML Notes","github":"https://github.com/joshcarp/ml-notes","id":"af717f67-a8e5-4337-ac20-caf05da70397","thebe":{"binder":{"url":"https://mybinder.org/","provider":"github","repo":"https://github.com/joshcarp/ml-notes","ref":"HEAD"}},"toc":[{"file":"intro.md"},{"children":[{"file":"projects/cognition.to.md"},{"file":"projects/the-interactive-transformer/interactive-transformer.ipynb","title":"The Interactive Transformer"}],"file":"projects/index.md"},{"children":[{"file":"glossary/dropout.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"},{"file":"glossary/dropout.md"},{"file":"glossary/feed_forward_network.md"},{"file":"glossary/fine-tuning.md"},{"file":"glossary/lora.md"},{"file":"glossary/model_collapse.md"},{"file":"glossary/poly_semanticity.md"},{"file":"glossary/pre-training.md"},{"file":"glossary/rag.md"},{"file":"glossary/reinforcement-learning.md"},{"file":"glossary/residual_stream.md"},{"file":"glossary/sparsedense-reward.md"},{"file":"glossary/transformer.md"}],"file":"glossary/index.md"},{"children":[{"file":"paper-notes/perplexity-based-data-pruning.md"}],"file":"paper-notes/index.md","title":"Paper Notes"}],"exports":[],"bibliography":[],"index":"intro","pages":[{"slug":"projects.index","title":"Projects","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"projects.cognition-to","title":"CognitionTO Community","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.index","title":"Glossary","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"glossary.dropout","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.dropout-1","title":"Dropout","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.feed-forward-network","title":"Feed-Forward Network","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.fine-tuning-1","title":"Fine-Tuning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.lora","title":"LoRA (Low-Rank Adaptation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.model-collapse","title":"Model Collapse","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.poly-semanticity","title":"Poly-semanticity","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.pre-training-1","title":"Pre-Training","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.rag","title":"RAG (Retrieval-Augmented Generation)","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.reinforcement-learning-1","title":"Reinforcement Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.residual-stream","title":"Residual Stream","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.sparsedense-reward-1","title":"Sparse/Dense Reward","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"glossary.transformer-1","title":"The Transformer","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2},{"slug":"paper-notes.index","title":"Papers","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"paper-notes.perplexity-based-data-pruning","title":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":2}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":true,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/build/manifest-A92797E9.js";
import * as route0 from "/build/root-HROFNPGU.js";
import * as route1 from "/build/routes/$-WNZNXUO2.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/build/entry.client-UNPC4GT3.js");</script></body></html>